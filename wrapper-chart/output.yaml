---
# Source: my-openebs/charts/openebs/charts/cstor/templates/priority-class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: release-name-cstor-csi-controller-critical
value: 9e+08
globalDefault: false
description: "This priority class should be used for the CStor CSI driver controller deployment only."
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/priority-class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: release-name-cstor-csi-node-critical
value: 9.00001e+08
globalDefault: false
description: "This priority class should be used for the CStor CSI driver node deployment only."
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/priority-class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: release-name-jiva-csi-controller-critical
value: 9e+08
globalDefault: false
description: "This priority class should be used for the OpenEBS CSI driver controller deployment only."
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/priority-class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: release-name-jiva-csi-node-critical
value: 9.00001e+08
globalDefault: false
description: "This priority class should be used for the OpenEBS CSI driver node deployment only."
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/priority-class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: release-name-lvm-localpv-csi-controller-critical
value: 900000000
globalDefault: false
description: "This priority class should be used for the CStor CSI driver controller deployment only."
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/priority-class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: release-name-lvm-localpv-csi-node-critical
value: 900001000
globalDefault: false
description: "This priority class should be used for the CStor CSI driver node deployment only."
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/priority-class/priority-class.yaml
apiVersion: scheduling.k8s.io/v1
description: Used for critical pods that must run in the cluster, which can be moved to another node if necessary.
kind: PriorityClass
metadata:
  name: release-name-cluster-critical
preemptionPolicy: PreemptLowerPriority
value: 1000000000
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/priority-class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: release-name-zfs-csi-controller-critical
value: 900000000
globalDefault: false
description: "This priority class should be used for the CStor CSI driver controller deployment only."
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/priority-class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: release-name-zfs-csi-node-critical
value: 900001000
globalDefault: false
description: "This priority class should be used for the CStor CSI driver node deployment only."
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/fluent-bit/templates/podsecuritypolicy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-fluent-bit-loki
  labels:
    app: fluent-bit-loki
    chart: fluent-bit-2.3.1
    heritage: Helm
    release: release-name
spec:
  privileged: false
  allowPrivilegeEscalation: false
  volumes:
    - 'secret'
    - 'configMap'
    - 'hostPath'
    - 'projected'
    - 'downwardAPI'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
  readOnlyRootFilesystem: true
  requiredDropCapabilities:
    - ALL
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/podsecuritypolicy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-grafana
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default'
    seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'
    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    # Default set from Docker, with DAC_OVERRIDE and CHOWN
      - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'csi'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/tests/test-podsecuritypolicy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: release-name-grafana-test
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
spec:
  allowPrivilegeEscalation: true
  privileged: false
  hostNetwork: false
  hostIPC: false
  hostPID: false
  fsGroup:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  runAsUser:
    rule: RunAsAny
  volumes:
  - configMap
  - downwardAPI
  - emptyDir
  - projected
  - csi
  - secret
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/etcd/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: release-name-etcd
  namespace: "prod"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.6.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  minAvailable: 51%
  selector:
    matchLabels:
      app.kubernetes.io/name: etcd
      app.kubernetes.io/instance: release-name
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/logstash/templates/poddisruptionbudget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "release-name-logstash-pdb"
  labels:
    app: "release-name-logstash"
    chart: "logstash"
    heritage: "Helm"
    release: "release-name"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "release-name-logstash"
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/nats/templates/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: release-name-nats
  namespace: prod
  labels:
    helm.sh/chart: nats-0.19.14
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.9.17"
    app.kubernetes.io/managed-by: Helm
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: nats
      app.kubernetes.io/instance: release-name
---
# Source: my-openebs/charts/openebs/charts/cstor/charts/openebs-ndm/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-ndm
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-controller-rbac.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: openebs-cstor-csi-controller-sa
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-controller"
    release: release-name
    component: "openebs-cstor-csi-controller"
    openebs.io/component-name: "openebs-cstor-csi-controller"
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-node-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-cstor-csi-node-sa
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-node"
    release: release-name
    component: "openebs-cstor-csi-node"
    openebs.io/component-name: "openebs-cstor-csi-node"
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-cstor-operator
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
---
# Source: my-openebs/charts/openebs/charts/jiva/charts/localpv-provisioner/charts/openebs-ndm/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-ndm
---
# Source: my-openebs/charts/openebs/charts/jiva/charts/localpv-provisioner/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-localpv-provisioner
  labels:
    chart: localpv-provisioner-3.4.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    app: localpv-provisioner
    release: release-name
    component: "localpv-provisioner"
    openebs.io/component-name: openebs-localpv-provisioner
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-controller-rbac.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: openebs-jiva-csi-controller-sa
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-controller"
    release: release-name
    component: "openebs-jiva-csi-controller"
    openebs.io/component-name: "openebs-jiva-csi-controller"
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-node-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-jiva-csi-node-sa
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-node"
    release: release-name
    component: "openebs-jiva-csi-node"
    openebs.io/component-name: "openebs-jiva-csi-node"
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/jiva-operator-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-jiva-operator
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
---
# Source: my-openebs/charts/openebs/charts/localpv-provisioner/charts/openebs-ndm/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-ndm
---
# Source: my-openebs/charts/openebs/charts/localpv-provisioner/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-localpv-provisioner
  labels:
    chart: localpv-provisioner-3.4.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    app: localpv-provisioner
    release: release-name
    component: "localpv-provisioner"
    openebs.io/component-name: openebs-localpv-provisioner
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/rbac.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: openebs-lvm-controller-sa
  namespace: prod
  labels:
    chart: lvm-localpv-1.3.0
    heritage: Helm
    openebs.io/version: "1.3.0"
    role: "openebs-lvm"
    app: "openebs-lvm-controller"
    release: release-name
    component: "openebs-lvm-controller"
    openebs.io/component-name: "openebs-lvm-controller"
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-lvm-node-sa
  namespace: prod
  labels:
    chart: lvm-localpv-1.3.0
    heritage: Helm
    openebs.io/version: "1.3.0"
    role: "openebs-lvm"
    name: "openebs-lvm-node"
    release: release-name
    openebs.io/component-name: "openebs-lvm-node"
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/jaeger-operator/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-jaeger-operator
  namespace: prod
  labels:
    
    app.kubernetes.io/name: jaeger-operator
    app.kubernetes.io/instance: release-name
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/localpv-provisioner/charts/openebs-ndm/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-ndm
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/localpv-provisioner/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-localpv-provisioner
  labels:
    chart: localpv-provisioner-3.4.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    app: localpv-provisioner
    release: release-name
    component: "localpv-provisioner"
    openebs.io/component-name: openebs-localpv-provisioner
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/filebeat/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-filebeat
  annotations:
  labels:
    app: "release-name-filebeat"
    chart: "filebeat-7.17.1"
    heritage: "Helm"
    release: "release-name"
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/fluent-bit/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: fluent-bit-loki
    chart: fluent-bit-2.3.1
    heritage: Helm
    release: release-name
  name: release-name-fluent-bit-loki
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
  name: release-name-grafana
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/tests/test-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
  name: release-name-grafana-test
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/loki/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app: loki
    chart: loki-2.11.0
    heritage: Helm
    release: release-name
  annotations:
    {}
  name: release-name-loki
  namespace: prod
automountServiceAccountToken: true
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/charts/kube-state-metrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-4.4.3
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.3.0"
  name: release-name-kube-state-metrics
  namespace: prod
imagePullSecrets:
  []
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/alertmanager/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-alertmanager
  namespace: prod
  annotations:
    {}
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/node-exporter/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: "node-exporter"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-node-exporter
  namespace: prod
  annotations:
    {}
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/pushgateway/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: "pushgateway"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-pushgateway
  namespace: prod
  annotations:
    {}
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/server/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: "server"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-server
  namespace: prod
  annotations:
    {}
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/promtail/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-promtail
  namespace: prod
  labels:
    helm.sh/chart: promtail-3.11.0
    app.kubernetes.io/name: promtail
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.4.2"
    app.kubernetes.io/managed-by: Helm
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/nats/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-nats
  namespace: prod
  labels:
    helm.sh/chart: nats-0.19.14
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.9.17"
    app.kubernetes.io/managed-by: Helm
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/rbac/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-service-account
  namespace: prod
  labels:
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
---
# Source: my-openebs/charts/openebs/charts/nfs-provisioner/templates/serviceaccount.yaml
# Create Service Account for nfs-provisioner.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-nfs-provisioner
  namespace: prod
  labels:
    chart: nfs-provisioner-0.10.0
    heritage: Helm
    openebs.io/version: "0.10.0"
    app: nfs-provisioner
    release: release-name
    component: nfs-provisioner
    openebs.io/component-name: openebs-nfs-provisioner
---
# Source: my-openebs/charts/openebs/charts/openebs-ndm/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-ndm
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/rbac.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: openebs-zfs-controller-sa
  namespace: prod
  labels:
    chart: zfs-localpv-2.3.1
    heritage: Helm
    openebs.io/version: "2.3.1"
    role: "openebs-zfs"
    app: "openebs-zfs-controller"
    release: release-name
    component: "openebs-zfs-controller"
    openebs.io/component-name: "openebs-zfs-controller"
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: openebs-zfs-node-sa
  namespace: prod
  labels:
    chart: zfs-localpv-2.3.1
    heritage: Helm
    openebs.io/version: "2.3.1"
    role: "openebs-zfs"
    name: "openebs-zfs-node"
    release: release-name
    openebs.io/component-name: "openebs-zfs-node"
---
# Source: my-openebs/charts/openebs/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-openebs
  labels:
    app: openebs
    chart: openebs-3.9.0
    release: release-name
    heritage: Helm
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/etcd/templates/token-secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-etcd-jwt-token
  namespace: "prod"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.6.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  jwt-token.pem: "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"
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-grafana
  namespace: prod
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  admin-user: "YWRtaW4="
  admin-password: "Q3pZR242YUN2RmV6WlJqZU1MeGFURmhxNzJzeFM2T0Q1STZxY1BuQw=="
  ldap-toml: ""
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/loki/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-loki
  namespace: prod
  labels:
    app: loki
    chart: loki-2.11.0
    release: release-name
    heritage: Helm
data:
  loki.yaml: YXV0aF9lbmFibGVkOiBmYWxzZQpjaHVua19zdG9yZV9jb25maWc6CiAgbWF4X2xvb2tfYmFja19wZXJpb2Q6IDBzCmNvbXBhY3RvcjoKICBjb21wYWN0aW9uX2ludGVydmFsOiAyMG0KICByZXRlbnRpb25fZGVsZXRlX2RlbGF5OiAxaAogIHJldGVudGlvbl9kZWxldGVfd29ya2VyX2NvdW50OiA1MAogIHJldGVudGlvbl9lbmFibGVkOiB0cnVlCiAgc2hhcmVkX3N0b3JlOiBmaWxlc3lzdGVtCiAgd29ya2luZ19kaXJlY3Rvcnk6IC9kYXRhL2xva2kvYm9sdGRiLXNoaXBwZXItY29tcGFjdG9yCmluZ2VzdGVyOgogIGNodW5rX2Jsb2NrX3NpemU6IDI2MjE0NAogIGNodW5rX2lkbGVfcGVyaW9kOiAzbQogIGNodW5rX3JldGFpbl9wZXJpb2Q6IDFtCiAgbGlmZWN5Y2xlcjoKICAgIHJpbmc6CiAgICAgIGt2c3RvcmU6CiAgICAgICAgc3RvcmU6IGlubWVtb3J5CiAgICAgIHJlcGxpY2F0aW9uX2ZhY3RvcjogMQogIG1heF90cmFuc2Zlcl9yZXRyaWVzOiAwCiAgd2FsOgogICAgZGlyOiAvZGF0YS9sb2tpL3dhbApsaW1pdHNfY29uZmlnOgogIGVuZm9yY2VfbWV0cmljX25hbWU6IGZhbHNlCiAgcmVqZWN0X29sZF9zYW1wbGVzOiB0cnVlCiAgcmVqZWN0X29sZF9zYW1wbGVzX21heF9hZ2U6IDE2OGgKICByZXRlbnRpb25fcGVyaW9kOiAxNjhoCnNjaGVtYV9jb25maWc6CiAgY29uZmlnczoKICAtIGZyb206ICIyMDIwLTEwLTI0IgogICAgaW5kZXg6CiAgICAgIHBlcmlvZDogMjRoCiAgICAgIHByZWZpeDogaW5kZXhfCiAgICBvYmplY3Rfc3RvcmU6IGZpbGVzeXN0ZW0KICAgIHNjaGVtYTogdjExCiAgICBzdG9yZTogYm9sdGRiLXNoaXBwZXIKc2VydmVyOgogIGh0dHBfbGlzdGVuX3BvcnQ6IDMxMDAKc3RvcmFnZV9jb25maWc6CiAgYm9sdGRiX3NoaXBwZXI6CiAgICBhY3RpdmVfaW5kZXhfZGlyZWN0b3J5OiAvZGF0YS9sb2tpL2JvbHRkYi1zaGlwcGVyLWFjdGl2ZQogICAgY2FjaGVfbG9jYXRpb246IC9kYXRhL2xva2kvYm9sdGRiLXNoaXBwZXItY2FjaGUKICAgIGNhY2hlX3R0bDogMjRoCiAgICBzaGFyZWRfc3RvcmU6IGZpbGVzeXN0ZW0KICBmaWxlc3lzdGVtOgogICAgZGlyZWN0b3J5OiAvZGF0YS9sb2tpL2NodW5rcwp0YWJsZV9tYW5hZ2VyOgogIHJldGVudGlvbl9kZWxldGVzX2VuYWJsZWQ6IGZhbHNlCiAgcmV0ZW50aW9uX3BlcmlvZDogMHM=
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/promtail/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-promtail
  namespace: prod
  labels:
    helm.sh/chart: promtail-3.11.0
    app.kubernetes.io/name: promtail
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.4.2"
    app.kubernetes.io/managed-by: Helm
stringData:
  promtail.yaml: |
    server:
      log_level: info
      http_listen_port: 3101
    
    client:
      url: http://release-name-loki:3100/loki/api/v1/push
      
    
    positions:
      filename: /run/promtail/positions.yaml
    
    scrape_configs:
      - job_name: release-name-pods-name
        pipeline_stages:
          - docker: {}
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels:
          - __meta_kubernetes_pod_node_name
          target_label: hostname
          action: replace
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - action: keep
          source_labels:
          - __meta_kubernetes_pod_label_openebs_io_logging
          regex: true
          target_label: release-name_component
        - action: replace
          replacement: $1
          separator: /
          source_labels:
          - __meta_kubernetes_namespace
          target_label: job
        - action: replace
          source_labels:
          - __meta_kubernetes_pod_name
          target_label: pod
        - action: replace
          source_labels:
          - __meta_kubernetes_pod_container_name
          target_label: container
        - replacement: /var/log/pods/*$1/*.log
          separator: /
          source_labels:
          - __meta_kubernetes_pod_uid
          - __meta_kubernetes_pod_container_name
          target_label: __path__
---
# Source: my-openebs/charts/openebs/charts/cstor/charts/openebs-ndm/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-openebs-ndm-config
data:
  # node-disk-manager-config contains config of available probes and filters.
  # Probes and Filters will initialize with default values if config for that
  # filter or probe are not present in configmap

  # udev-probe is default or primary probe it should be enabled to run ndm
  # filterconfigs contains configs of filters. To provide a group of include
  # and exclude values add it as , separated string
  node-disk-manager.config: |
    probeconfigs:
      - key: udev-probe
        name: udev probe
        state: true
      - key: seachest-probe
        name: seachest probe
        state: false
      - key: smart-probe
        name: smart probe
        state: true
    filterconfigs:
      - key: os-disk-exclude-filter
        name: os disk exclude filter
        state: true
        exclude: "/,/etc/hosts,/boot"
      - key: vendor-filter
        name: vendor filter
        state: true
        include: ""
        exclude: "CLOUDBYT,OpenEBS"
      - key: path-filter
        name: path filter
        state: true
        include: ""
        exclude: "loop,fd0,sr0,/dev/ram,/dev/dm-,/dev/md,/dev/rbd,/dev/zd"
    metaconfigs:
      - key: node-labels
        name: node labels
        pattern: ""
      - key: device-labels
        name: device labels
        type: ""
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-iscsiadm-config.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: openebs-cstor-csi-iscsiadm
data:
  iscsiadm: |
    #!/bin/sh
    if [ -x /host/sbin/iscsiadm ]; then
      chroot /host /sbin/iscsiadm "$@"
    elif [ -x /host/usr/local/sbin/iscsiadm ]; then
      chroot /host /usr/local/sbin/iscsiadm "$@"
    elif [ -x /host/bin/iscsiadm ]; then
      chroot /host /bin/iscsiadm "$@"
    elif [ -x /host/usr/local/bin/iscsiadm ]; then
      chroot /host /usr/local/bin/iscsiadm "$@"
    else
      chroot /host iscsiadm "$@"
    fi
---
# Source: my-openebs/charts/openebs/charts/jiva/charts/localpv-provisioner/charts/openebs-ndm/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-openebs-ndm-config
data:
  # node-disk-manager-config contains config of available probes and filters.
  # Probes and Filters will initialize with default values if config for that
  # filter or probe are not present in configmap

  # udev-probe is default or primary probe it should be enabled to run ndm
  # filterconfigs contains configs of filters. To provide a group of include
  # and exclude values add it as , separated string
  node-disk-manager.config: |
    probeconfigs:
      - key: udev-probe
        name: udev probe
        state: true
      - key: seachest-probe
        name: seachest probe
        state: false
      - key: smart-probe
        name: smart probe
        state: true
    filterconfigs:
      - key: os-disk-exclude-filter
        name: os disk exclude filter
        state: true
        exclude: "/,/etc/hosts,/boot"
      - key: vendor-filter
        name: vendor filter
        state: true
        include: ""
        exclude: "CLOUDBYT,OpenEBS"
      - key: path-filter
        name: path filter
        state: true
        include: ""
        exclude: "loop,fd0,sr0,/dev/ram,/dev/dm-,/dev/md,/dev/rbd,/dev/zd"
    metaconfigs:
      - key: node-labels
        name: node labels
        pattern: ""
      - key: device-labels
        name: device labels
        type: ""
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-iscsiadm-config.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: openebs-jiva-csi-iscsiadm
data:
  iscsiadm: |
    #!/bin/sh
    if [ -x /host/sbin/iscsiadm ]; then
      chroot /host /sbin/iscsiadm "$@"
    elif [ -x /host/usr/local/sbin/iscsiadm ]; then
      chroot /host /usr/local/sbin/iscsiadm "$@"
    elif [ -x /host/bin/iscsiadm ]; then
      chroot /host /bin/iscsiadm "$@"
    elif [ -x /host/usr/local/bin/iscsiadm ]; then
      chroot /host /usr/local/bin/iscsiadm "$@"
    else
      chroot /host iscsiadm "$@"
    fi
---
# Source: my-openebs/charts/openebs/charts/localpv-provisioner/charts/openebs-ndm/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-openebs-ndm-config
data:
  # node-disk-manager-config contains config of available probes and filters.
  # Probes and Filters will initialize with default values if config for that
  # filter or probe are not present in configmap

  # udev-probe is default or primary probe it should be enabled to run ndm
  # filterconfigs contains configs of filters. To provide a group of include
  # and exclude values add it as , separated string
  node-disk-manager.config: |
    probeconfigs:
      - key: udev-probe
        name: udev probe
        state: true
      - key: seachest-probe
        name: seachest probe
        state: false
      - key: smart-probe
        name: smart probe
        state: true
    filterconfigs:
      - key: os-disk-exclude-filter
        name: os disk exclude filter
        state: true
        exclude: "/,/etc/hosts,/boot"
      - key: vendor-filter
        name: vendor filter
        state: true
        include: ""
        exclude: "CLOUDBYT,OpenEBS"
      - key: path-filter
        name: path filter
        state: true
        include: ""
        exclude: "loop,fd0,sr0,/dev/ram,/dev/dm-,/dev/md,/dev/rbd,/dev/zd"
    metaconfigs:
      - key: node-labels
        name: node labels
        pattern: ""
      - key: device-labels
        name: device labels
        type: ""
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/localpv-provisioner/charts/openebs-ndm/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-openebs-ndm-config
data:
  # node-disk-manager-config contains config of available probes and filters.
  # Probes and Filters will initialize with default values if config for that
  # filter or probe are not present in configmap

  # udev-probe is default or primary probe it should be enabled to run ndm
  # filterconfigs contains configs of filters. To provide a group of include
  # and exclude values add it as , separated string
  node-disk-manager.config: |
    probeconfigs:
      - key: udev-probe
        name: udev probe
        state: true
      - key: seachest-probe
        name: seachest probe
        state: false
      - key: smart-probe
        name: smart probe
        state: true
    filterconfigs:
      - key: os-disk-exclude-filter
        name: os disk exclude filter
        state: true
        exclude: "/,/etc/hosts,/boot"
      - key: vendor-filter
        name: vendor filter
        state: true
        include: ""
        exclude: "CLOUDBYT,OpenEBS"
      - key: path-filter
        name: path filter
        state: true
        include: ""
        exclude: "loop,fd0,sr0,/dev/ram,/dev/dm-,/dev/md,/dev/rbd,/dev/zd"
    metaconfigs:
      - key: node-labels
        name: node labels
        pattern: ""
      - key: device-labels
        name: device labels
        type: ""
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/filebeat/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-filebeat-config
  labels:
    app: "release-name-filebeat"
    chart: "filebeat-7.17.1"
    heritage: "Helm"
    release: "release-name"
data:
  filebeat.yml: |
    # logging.level: debug
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*.log
      processors:
      - add_kubernetes_metadata:
          host: ${NODE_NAME}
          matchers:
          - logs_path:
              logs_path: "/var/log/containers/"
    output.logstash:
      hosts: ["logstash-loki:5044"]
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/filebeat/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-filebeat-daemonset-config
  labels:
    app: "release-name-filebeat"
    chart: "filebeat-7.17.1"
    heritage: "Helm"
    release: "release-name"
data:
  filebeat.yml: |
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*.log
      processors:
      - add_kubernetes_metadata:
          host: ${NODE_NAME}
          matchers:
          - logs_path:
              logs_path: "/var/log/containers/"
    
    output.elasticsearch:
      host: '${NODE_NAME}'
      hosts: '${ELASTICSEARCH_HOSTS:elasticsearch-master:9200}'
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/fluent-bit/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-fluent-bit-loki
  namespace: prod
  labels:
    app: fluent-bit-loki
    chart: fluent-bit-2.3.1
    release: release-name
    heritage: Helm
data:
  fluent-bit.conf: |-
    [SERVICE]
        HTTP_Server    On
        HTTP_Listen    0.0.0.0
        HTTP_PORT      2020
        Flush          1
        Daemon         Off
        Log_Level      warn
        Parsers_File   parsers.conf
    [INPUT]
        Name              tail
        Tag               kube.*
        Path              /var/log/containers/*.log
        Parser            docker
        DB                /run/fluent-bit/flb_kube.db
        Mem_Buf_Limit     5MB
        Buffer_Chunk_size 32k
        Buffer_Max_size   32k
    [FILTER]
        Name           kubernetes
        Match          kube.*
        Kube_URL       https://kubernetes.default.svc:443
        Merge_Log On
        K8S-Logging.Exclude Off
        K8S-Logging.Parser Off
    [Output]
        Name grafana-loki
        Match *
        Url http://release-name-loki:3100/api/prom/push
        TenantID ""
        BatchWait 1
        BatchSize 1048576
        Labels {job="fluent-bit"}
        RemoveKeys kubernetes,stream
        AutoKubernetesLabels false
        LabelMapPath /fluent-bit/etc/labelmap.json
        LineFormat json
        LogLevel warn
  parsers.conf: |-
    [PARSER]
        Name        docker
        Format      json
        Time_Key    time
        Time_Format %Y-%m-%dT%H:%M:%S.%L

  labelmap.json: |-
    {
      "kubernetes": {
        "container_name": "container",
        "host": "node",
        "labels": {
          "app": "app",
          "release": "release"
        },
        "namespace_name": "namespace",
        "pod_name": "instance"
      },
      "stream": "stream"
    }
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-grafana
  namespace: prod
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
data:
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/tests/test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-grafana-test
  namespace: prod
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
data:
  run.sh: |-
    @test "Test Health" {
      url="http://release-name-grafana/api/health"

      code=$(wget --server-response --spider --timeout 10 --tries 1 ${url} 2>&1 | awk '/^  HTTP/{print $2}')
      [ "$code" == "200" ]
    }
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/alertmanager/cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-alertmanager
  namespace: prod
data:
  alertmanager.yml: |
    global: {}
    receivers:
    - name: default-receiver
    route:
      group_interval: 5m
      group_wait: 10s
      receiver: default-receiver
      repeat_interval: 3h
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/server/cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    component: "server"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-server
  namespace: prod
data:
  alerting_rules.yml: |
    {}
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 1m
      scrape_interval: 1m
      scrape_timeout: 10s
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    - job_name: prometheus
      static_configs:
      - targets:
        - localhost:9090
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-apiservers
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: default;kubernetes;https
        source_labels:
        - __meta_kubernetes_namespace
        - __meta_kubernetes_service_name
        - __meta_kubernetes_endpoint_port_name
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes-cadvisor
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - job_name: kubernetes-service-endpoints
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape
      - action: drop
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: service
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
    - job_name: kubernetes-service-endpoints-slow
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: service
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: node
      scrape_interval: 5m
      scrape_timeout: 30s
    - honor_labels: true
      job_name: prometheus-pushgateway
      kubernetes_sd_configs:
      - role: service
      relabel_configs:
      - action: keep
        regex: pushgateway
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
    - job_name: kubernetes-services
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module:
        - http_2xx
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
      - source_labels:
        - __address__
        target_label: __param_target
      - replacement: blackbox
        target_label: __address__
      - source_labels:
        - __param_target
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: service
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      - action: drop
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - action: drop
        regex: Pending|Succeeded|Failed|Completed
        source_labels:
        - __meta_kubernetes_pod_phase
    - job_name: kubernetes-pods-slow
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
        replacement: __param_$1
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - action: drop
        regex: Pending|Succeeded|Failed|Completed
        source_labels:
        - __meta_kubernetes_pod_phase
      scrape_interval: 5m
      scrape_timeout: 30s
    alerting:
      alertmanagers:
      - kubernetes_sd_configs:
          - role: pod
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace]
          regex: prod
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_app]
          regex: prometheus
          action: keep
        - source_labels: [__meta_kubernetes_pod_label_component]
          regex: alertmanager
          action: keep
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_probe]
          regex: .*
          action: keep
        - source_labels: [__meta_kubernetes_pod_container_port_number]
          regex: "9093"
          action: keep
  recording_rules.yml: |
    {}
  rules: |
    {}
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/templates/datasources.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-loki-stack
  namespace: prod
  labels:
    app: loki-stack
    chart: loki-stack-2.6.4
    release: release-name
    heritage: Helm
    grafana_datasource: "1"
data:
  loki-stack-datasource.yaml: |-
    apiVersion: 1
    datasources:
    - name: Loki
      type: loki
      access: proxy
      url: http://release-name-loki:3100
      version: 1
      isDefault: true
      jsonData:
        maxLines: 1000
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/templates/tests/loki-test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-loki-stack-test
  labels:
    app: loki-stack
    chart: loki-stack-2.6.4
    release: release-name
    heritage: Helm
data:
  test.sh: |
    #!/usr/bin/env bash

    LOKI_URI="http://${LOKI_SERVICE}:${LOKI_PORT}"

    function setup() {
      apk add -u curl jq
      until (curl -s ${LOKI_URI}/api/prom/label/app/values | jq -e '.values[] | select(. == "loki")'); do
        sleep 1
      done
    }

    @test "Has labels" {
      curl -s ${LOKI_URI}/api/prom/label | \
      jq -e '.values[] | select(. == "app")'
    }

    @test "Query log entry" {
      curl -sG ${LOKI_URI}/api/prom/query?limit=10 --data-urlencode 'query={app="loki"}' | \
      jq -e '.streams[].entries | length >= 1'
    }

    @test "Push log entry legacy" {
      local timestamp=$(date -Iseconds -u | sed 's/UTC/.000000000+00:00/')
      local data=$(jq -n --arg timestamp "${timestamp}" '{"streams": [{"labels": "{app=\"loki-test\"}", "entries": [{"ts": $timestamp, "line": "foobar"}]}]}')

      curl -s -X POST -H "Content-Type: application/json" ${LOKI_URI}/api/prom/push -d "${data}"

      curl -sG ${LOKI_URI}/api/prom/query?limit=1 --data-urlencode 'query={app="loki-test"}' | \
      jq -e '.streams[].entries[].line == "foobar"'
    }

    @test "Push log entry" {
      local timestamp=$(date +%s000000000)
      local data=$(jq -n --arg timestamp "${timestamp}" '{"streams": [{"stream": {"app": "loki-test"}, "values": [[$timestamp, "foobar"]]}]}')

      curl -s -X POST -H "Content-Type: application/json" ${LOKI_URI}/loki/api/v1/push -d "${data}"

      curl -sG ${LOKI_URI}/api/prom/query?limit=1 --data-urlencode 'query={app="loki-test"}' | \
      jq -e '.streams[].entries[].line == "foobar"'
    }
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/nats/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-nats-config
  namespace: prod
  labels:
    helm.sh/chart: nats-0.19.14
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.9.17"
    app.kubernetes.io/managed-by: Helm
data:
  nats.conf: |
    # NATS Clients Port
    port: 4222

    # PID file shared with configuration reloader.
    pid_file: "/var/run/nats/nats.pid"

    ###############
    #             #
    # Monitoring  #
    #             #
    ###############
    http: 8222
    server_name:$POD_NAME
    ###################################
    #                                 #
    # NATS JetStream                  #
    #                                 #
    ###################################
    jetstream {
      max_mem: 5Mi
    }
    ###################################
    #                                 #
    # NATS Full Mesh Clustering Setup #
    #                                 #
    ###################################
    cluster {
      port: 6222
      name: nats

      routes = [
        nats://release-name-nats-0.release-name-nats.prod.svc.cluster.local:6222,nats://release-name-nats-1.release-name-nats.prod.svc.cluster.local:6222,nats://release-name-nats-2.release-name-nats.prod.svc.cluster.local:6222,
        
      ]
      cluster_advertise: $CLUSTER_ADVERTISE

      connect_retries: 120
    }
    lame_duck_grace_period: 10s
    lame_duck_duration: 30s
---
# Source: my-openebs/charts/openebs/charts/openebs-ndm/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-openebs-ndm-config
data:
  # node-disk-manager-config contains config of available probes and filters.
  # Probes and Filters will initialize with default values if config for that
  # filter or probe are not present in configmap

  # udev-probe is default or primary probe it should be enabled to run ndm
  # filterconfigs contains configs of filters. To provide a group of include
  # and exclude values add it as , separated string
  node-disk-manager.config: |
    probeconfigs:
      - key: udev-probe
        name: udev probe
        state: true
      - key: seachest-probe
        name: seachest probe
        state: false
      - key: smart-probe
        name: smart probe
        state: true
    filterconfigs:
      - key: os-disk-exclude-filter
        name: os disk exclude filter
        state: true
        exclude: "/,/etc/hosts,/boot"
      - key: vendor-filter
        name: vendor filter
        state: true
        include: ""
        exclude: "CLOUDBYT,OpenEBS"
      - key: path-filter
        name: path filter
        state: true
        include: ""
        exclude: "loop,fd0,sr0,/dev/ram,/dev/dm-,/dev/md,/dev/rbd,/dev/zd"
    metaconfigs:
      - key: node-labels
        name: node labels
        pattern: ""
      - key: device-labels
        name: device labels
        type: ""
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: openebs-zfspv-bin
  namespace: prod # should be the same namespace where it is getting mounted
  labels:
    chart: zfs-localpv-2.3.1
    heritage: Helm
    openebs.io/version: "2.3.1"
    role: "openebs-zfs"
    name: "openebs-zfs-node"
    release: release-name
    openebs.io/component-name: "openebs-zfs-node"
data:
  zfs: |
    #!/bin/sh
    if [ -x /host/sbin/zfs ]; then
      chroot /host /sbin/zfs "$@"
    elif [ -x /host/usr/sbin/zfs ]; then
      chroot /host /usr/sbin/zfs "$@"
    else
      chroot /host "zfs" "$@"
    fi
---
# Source: my-openebs/charts/openebs/templates/ndm/cm-node-disk-manager.yaml
# This is the node-disk-manager related config.
# It can be used to customize the disks probes and filters
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-openebs-ndm-config
  labels:
    app: openebs
    chart: openebs-3.9.0
    release: release-name
    heritage: Helm
    component: ndm-config
    openebs.io/component-name: ndm-config
data:
  # udev-probe is default or primary probe which should be enabled to run ndm
  # filterconfigs contains configs of filters - in the form of include
  # and exclude comma separated strings
  node-disk-manager.config: |
    probeconfigs:
      - key: udev-probe
        name: udev probe
        state: true
      - key: seachest-probe
        name: seachest probe
        state: false
      - key: smart-probe
        name: smart probe
        state: true
    filterconfigs:
      - key: os-disk-exclude-filter
        name: os disk exclude filter
        state: true
        exclude: "/,/etc/hosts,/boot"
      - key: vendor-filter
        name: vendor filter
        state: true
        include: ""
        exclude: "CLOUDBYT,OpenEBS"
      - key: path-filter
        name: path filter
        state: true
        include: ""
        exclude: "/dev/loop,/dev/fd0,/dev/sr0,/dev/ram,/dev/dm-,/dev/md,/dev/rbd,/dev/zd"
---
# Source: my-openebs/charts/openebs/charts/jiva/charts/localpv-provisioner/templates/hostpath-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: openebs-hostpath
  annotations:
    openebs.io/cas-type: local
    cas.openebs.io/config: |
      - name: StorageType
        value: "hostpath"
      - name: BasePath
        value: "/var/openebs/local"
provisioner: openebs.io/local
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/default-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: openebs-jiva-csi-default
  annotations:
provisioner: jiva.csi.openebs.io
volumeBindingMode: Immediate
allowVolumeExpansion: true
reclaimPolicy: Delete
parameters:
  cas-type: "jiva"
  policy: openebs-jiva-default-policy
---
# Source: my-openebs/charts/openebs/charts/localpv-provisioner/templates/device-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: openebs-device
  annotations:
    openebs.io/cas-type: local
    cas.openebs.io/config: |
      - name: StorageType
        value: "device"
      - name: FSType
        value: "ext4"
provisioner: openebs.io/local
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
---
# Source: my-openebs/charts/openebs/charts/localpv-provisioner/templates/hostpath-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: openebs-hostpath
  annotations:
    openebs.io/cas-type: local
    cas.openebs.io/config: |
      - name: StorageType
        value: "hostpath"
      - name: BasePath
        value: "/var/openebs/local"
provisioner: openebs.io/local
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/etcd/storage/localpv-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    cas.openebs.io/config: |
      - name: StorageType
        value: "hostpath"
      - name: BasePath
        value: "/var/local/localpv-hostpath/release-name/etcd"
    openebs.io/cas-type: local
  name: mayastor-etcd-localpv
provisioner: openebs.io/local
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/loki-stack/storage/localpv-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    cas.openebs.io/config: |
      - name: StorageType
        value: "hostpath"
      - name: BasePath
        value: "/var/local/localpv-hostpath/release-name/loki"
    openebs.io/cas-type: local
  name: mayastor-loki-localpv
provisioner: openebs.io/local
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/storageclass.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: release-name-single-replica
parameters:
  repl: '1'
  protocol: 'nvmf'
  ioTimeout: '60'
provisioner: io.openebs.csi-mayastor
---
# Source: my-openebs/charts/openebs/charts/nfs-provisioner/templates/kernel-nfs-storageclass.yaml
# Storage classes for OpenEBS NFS Dynamic PV
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: openebs-kernel-nfs
  annotations:
    openebs.io/cas-type: nfsrwx
    cas.openebs.io/config: |
      - name: NFSServerType
        value: kernel
provisioner: openebs.io/nfsrwx
reclaimPolicy: Delete
---
# Source: my-openebs/charts/openebs/templates/localprovisioner/device-class.yaml
# The second operand in the AND operation can be removed
# when enableDeviceClass is deprecated.
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: openebs-device
  annotations:
    openebs.io/cas-type: local
    cas.openebs.io/config: |
      - name: StorageType
        value: "device"
      - name: FSType
        value: "ext4"
provisioner: openebs.io/local
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
---
# Source: my-openebs/charts/openebs/templates/localprovisioner/hostpath-class.yaml
# The second operand in the AND operation can be removed
# when enableHostpathClass is deprecated.
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: openebs-hostpath
  annotations:
    openebs.io/cas-type: local
    cas.openebs.io/config: |
      - name: StorageType
        value: "hostpath"
      - name: BasePath
        value: "/var/openebs/local"
provisioner: openebs.io/local
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/alertmanager/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-alertmanager
  namespace: prod
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "2Gi"
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/server/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    component: "server"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-server
  namespace: prod
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "8Gi"
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/volumesnapshotclasses-crd.yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.11.3
    api-approved.kubernetes.io: "https://github.com/kubernetes-csi/external-snapshotter/pull/814"
  creationTimestamp: null
  name: volumesnapshotclasses.snapshot.storage.k8s.io
spec:
  group: snapshot.storage.k8s.io
  names:
    kind: VolumeSnapshotClass
    listKind: VolumeSnapshotClassList
    plural: volumesnapshotclasses
    shortNames:
      - vsclass
      - vsclasses
    singular: volumesnapshotclass
  scope: Cluster
  versions:
    - additionalPrinterColumns:
        - jsonPath: .driver
          name: Driver
          type: string
        - description: Determines whether a VolumeSnapshotContent created through the
            VolumeSnapshotClass should be deleted when its bound VolumeSnapshot is deleted.
          jsonPath: .deletionPolicy
          name: DeletionPolicy
          type: string
        - jsonPath: .metadata.creationTimestamp
          name: Age
          type: date
      name: v1
      schema:
        openAPIV3Schema:
          description: VolumeSnapshotClass specifies parameters that a underlying storage
            system uses when creating a volume snapshot. A specific VolumeSnapshotClass
            is used by specifying its name in a VolumeSnapshot object. VolumeSnapshotClasses
            are non-namespaced
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
              of an object. Servers should convert recognized schemas to the latest
              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            deletionPolicy:
              description: deletionPolicy determines whether a VolumeSnapshotContent
                created through the VolumeSnapshotClass should be deleted when its bound
                VolumeSnapshot is deleted. Supported values are "Retain" and "Delete".
                "Retain" means that the VolumeSnapshotContent and its physical snapshot
                on underlying storage system are kept. "Delete" means that the VolumeSnapshotContent
                and its physical snapshot on underlying storage system are deleted.
                Required.
              enum:
                - Delete
                - Retain
              type: string
            driver:
              description: driver is the name of the storage driver that handles this
                VolumeSnapshotClass. Required.
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource this
              object represents. Servers may infer this from the endpoint the client
              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            parameters:
              additionalProperties:
                type: string
              description: parameters is a key-value map with storage driver specific
                parameters for creating snapshots. These values are opaque to Kubernetes.
              type: object
          required:
            - deletionPolicy
            - driver
          type: object
      served: true
      storage: true
      subresources: {}
    - additionalPrinterColumns:
        - jsonPath: .driver
          name: Driver
          type: string
        - description: Determines whether a VolumeSnapshotContent created through the VolumeSnapshotClass should be deleted when its bound VolumeSnapshot is deleted.
          jsonPath: .deletionPolicy
          name: DeletionPolicy
          type: string
        - jsonPath: .metadata.creationTimestamp
          name: Age
          type: date
      name: v1beta1
      # This indicates the v1beta1 version of the custom resource is deprecated.
      # API requests to this version receive a warning in the server response.
      deprecated: true
      # This overrides the default warning returned to clients making v1beta1 API requests.
      deprecationWarning: "snapshot.storage.k8s.io/v1beta1 VolumeSnapshotClass is deprecated; use snapshot.storage.k8s.io/v1 VolumeSnapshotClass"
      schema:
        openAPIV3Schema:
          description: VolumeSnapshotClass specifies parameters that a underlying storage system uses when creating a volume snapshot. A specific VolumeSnapshotClass is used by specifying its name in a VolumeSnapshot object. VolumeSnapshotClasses are non-namespaced
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            deletionPolicy:
              description: deletionPolicy determines whether a VolumeSnapshotContent created through the VolumeSnapshotClass should be deleted when its bound VolumeSnapshot is deleted. Supported values are "Retain" and "Delete". "Retain" means that the VolumeSnapshotContent and its physical snapshot on underlying storage system are kept. "Delete" means that the VolumeSnapshotContent and its physical snapshot on underlying storage system are deleted. Required.
              enum:
                - Delete
                - Retain
              type: string
            driver:
              description: driver is the name of the storage driver that handles this VolumeSnapshotClass. Required.
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            parameters:
              additionalProperties:
                type: string
              description: parameters is a key-value map with storage driver specific parameters for creating snapshots. These values are opaque to Kubernetes.
              type: object
          required:
            - deletionPolicy
            - driver
          type: object
      served: false
      storage: false
      subresources: {}
status:
  acceptedNames:
    kind: ""
    plural: ""
  conditions: []
  storedVersions: []
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/volumesnapshotcontents-crd.yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.11.3
    api-approved.kubernetes.io: "https://github.com/kubernetes-csi/external-snapshotter/pull/814"
  creationTimestamp: null
  name: volumesnapshotcontents.snapshot.storage.k8s.io
spec:
  group: snapshot.storage.k8s.io
  names:
    kind: VolumeSnapshotContent
    listKind: VolumeSnapshotContentList
    plural: volumesnapshotcontents
    shortNames:
      - vsc
      - vscs
    singular: volumesnapshotcontent
  scope: Cluster
  versions:
    - additionalPrinterColumns:
        - description: Indicates if the snapshot is ready to be used to restore a volume.
          jsonPath: .status.readyToUse
          name: ReadyToUse
          type: boolean
        - description: Represents the complete size of the snapshot in bytes
          jsonPath: .status.restoreSize
          name: RestoreSize
          type: integer
        - description: Determines whether this VolumeSnapshotContent and its physical
            snapshot on the underlying storage system should be deleted when its bound
            VolumeSnapshot is deleted.
          jsonPath: .spec.deletionPolicy
          name: DeletionPolicy
          type: string
        - description: Name of the CSI driver used to create the physical snapshot on
            the underlying storage system.
          jsonPath: .spec.driver
          name: Driver
          type: string
        - description: Name of the VolumeSnapshotClass to which this snapshot belongs.
          jsonPath: .spec.volumeSnapshotClassName
          name: VolumeSnapshotClass
          type: string
        - description: Name of the VolumeSnapshot object to which this VolumeSnapshotContent
            object is bound.
          jsonPath: .spec.volumeSnapshotRef.name
          name: VolumeSnapshot
          type: string
        - description: Namespace of the VolumeSnapshot object to which this VolumeSnapshotContent object is bound.
          jsonPath: .spec.volumeSnapshotRef.namespace
          name: VolumeSnapshotNamespace
          type: string
        - jsonPath: .metadata.creationTimestamp
          name: Age
          type: date
      name: v1
      schema:
        openAPIV3Schema:
          description: VolumeSnapshotContent represents the actual "on-disk" snapshot
            object in the underlying storage system
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
                of an object. Servers should convert recognized schemas to the latest
                internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource this
                object represents. Servers may infer this from the endpoint the client
                submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            spec:
              description: spec defines properties of a VolumeSnapshotContent created
                by the underlying storage system. Required.
              properties:
                deletionPolicy:
                  description: deletionPolicy determines whether this VolumeSnapshotContent
                    and its physical snapshot on the underlying storage system should
                    be deleted when its bound VolumeSnapshot is deleted. Supported values
                    are "Retain" and "Delete". "Retain" means that the VolumeSnapshotContent
                    and its physical snapshot on underlying storage system are kept.
                    "Delete" means that the VolumeSnapshotContent and its physical snapshot
                    on underlying storage system are deleted. For dynamically provisioned
                    snapshots, this field will automatically be filled in by the CSI
                    snapshotter sidecar with the "DeletionPolicy" field defined in the
                    corresponding VolumeSnapshotClass. For pre-existing snapshots, users
                    MUST specify this field when creating the VolumeSnapshotContent
                    object. Required.
                  enum:
                    - Delete
                    - Retain
                  type: string
                driver:
                  description: driver is the name of the CSI driver used to create the
                    physical snapshot on the underlying storage system. This MUST be
                    the same as the name returned by the CSI GetPluginName() call for
                    that driver. Required.
                  type: string
                source:
                  description: source specifies whether the snapshot is (or should be)
                    dynamically provisioned or already exists, and just requires a Kubernetes
                    object representation. This field is immutable after creation. Required.
                  properties:
                    snapshotHandle:
                      description: snapshotHandle specifies the CSI "snapshot_id" of
                        a pre-existing snapshot on the underlying storage system for
                        which a Kubernetes object representation was (or should be)
                        created. This field is immutable.
                      type: string
                    volumeHandle:
                      description: volumeHandle specifies the CSI "volume_id" of the
                        volume from which a snapshot should be dynamically taken from.
                        This field is immutable.
                      type: string
                  type: object
                  oneOf:
                    - required: [ "snapshotHandle" ]
                    - required: [ "volumeHandle" ]
                sourceVolumeMode:
                  description: SourceVolumeMode is the mode of the volume whose snapshot
                    is taken. Can be either Filesystem or Block. If not specified,
                    it indicates the source volume's mode is unknown. This field is
                    immutable. This field is an alpha field.
                  type: string
                volumeSnapshotClassName:
                  description: name of the VolumeSnapshotClass from which this snapshot
                    was (or will be) created. Note that after provisioning, the VolumeSnapshotClass
                    may be deleted or recreated with different set of values, and as
                    such, should not be referenced post-snapshot creation.
                  type: string
                volumeSnapshotRef:
                  description: volumeSnapshotRef specifies the VolumeSnapshot object
                    to which this VolumeSnapshotContent object is bound. VolumeSnapshot.Spec.VolumeSnapshotContentName
                    field must reference to this VolumeSnapshotContent's name for the
                    bidirectional binding to be valid. For a pre-existing VolumeSnapshotContent
                    object, name and namespace of the VolumeSnapshot object MUST be
                    provided for binding to happen. This field is immutable after creation.
                    Required.
                  properties:
                    apiVersion:
                      description: API version of the referent.
                      type: string
                    fieldPath:
                      description: 'If referring to a piece of an object instead of
                        an entire object, this string should contain a valid JSON/Go
                        field access statement, such as desiredState.manifest.containers[2].
                        For example, if the object reference is to a container within
                        a pod, this would take on a value like: "spec.containers{name}"
                        (where "name" refers to the name of the container that triggered
                        the event) or if no container name is specified "spec.containers[2]"
                        (container with index 2 in this pod). This syntax is chosen
                        only to have some well-defined way of referencing a part of
                        an object. TODO: this design is not final and this field is
                        subject to change in the future.'
                      type: string
                    kind:
                      description: 'Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
                      type: string
                    name:
                      description: 'Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names'
                      type: string
                    namespace:
                      description: 'Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/'
                      type: string
                    resourceVersion:
                      description: 'Specific resourceVersion to which this reference
                        is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency'
                      type: string
                    uid:
                      description: 'UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids'
                      type: string
                  type: object
                  x-kubernetes-map-type: atomic
              required:
                - deletionPolicy
                - driver
                - source
                - volumeSnapshotRef
              type: object
            status:
              description: status represents the current information of a snapshot.
              properties:
                creationTime:
                  description: creationTime is the timestamp when the point-in-time
                    snapshot is taken by the underlying storage system. In dynamic snapshot
                    creation case, this field will be filled in by the CSI snapshotter
                    sidecar with the "creation_time" value returned from CSI "CreateSnapshot"
                    gRPC call. For a pre-existing snapshot, this field will be filled
                    with the "creation_time" value returned from the CSI "ListSnapshots"
                    gRPC call if the driver supports it. If not specified, it indicates
                    the creation time is unknown. The format of this field is a Unix
                    nanoseconds time encoded as an int64. On Unix, the command `date
                    +%s%N` returns the current time in nanoseconds since 1970-01-01
                    00:00:00 UTC.
                  format: int64
                  type: integer
                error:
                  description: error is the last observed error during snapshot creation,
                    if any. Upon success after retry, this error field will be cleared.
                  properties:
                    message:
                      description: 'message is a string detailing the encountered error
                        during snapshot creation if specified. NOTE: message may be
                        logged, and it should not contain sensitive information.'
                      type: string
                    time:
                      description: time is the timestamp when the error was encountered.
                      format: date-time
                      type: string
                  type: object
                readyToUse:
                  description: readyToUse indicates if a snapshot is ready to be used
                    to restore a volume. In dynamic snapshot creation case, this field
                    will be filled in by the CSI snapshotter sidecar with the "ready_to_use"
                    value returned from CSI "CreateSnapshot" gRPC call. For a pre-existing
                    snapshot, this field will be filled with the "ready_to_use" value
                    returned from the CSI "ListSnapshots" gRPC call if the driver supports
                    it, otherwise, this field will be set to "True". If not specified,
                    it means the readiness of a snapshot is unknown.
                  type: boolean
                restoreSize:
                  description: restoreSize represents the complete size of the snapshot
                    in bytes. In dynamic snapshot creation case, this field will be
                    filled in by the CSI snapshotter sidecar with the "size_bytes" value
                    returned from CSI "CreateSnapshot" gRPC call. For a pre-existing
                    snapshot, this field will be filled with the "size_bytes" value
                    returned from the CSI "ListSnapshots" gRPC call if the driver supports
                    it. When restoring a volume from this snapshot, the size of the
                    volume MUST NOT be smaller than the restoreSize if it is specified,
                    otherwise the restoration will fail. If not specified, it indicates
                    that the size is unknown.
                  format: int64
                  minimum: 0
                  type: integer
                snapshotHandle:
                  description: snapshotHandle is the CSI "snapshot_id" of a snapshot
                    on the underlying storage system. If not specified, it indicates
                    that dynamic snapshot creation has either failed or it is still
                    in progress.
                  type: string
                volumeGroupSnapshotContentName:
                  description: VolumeGroupSnapshotContentName is the name of the VolumeGroupSnapshotContent
                    of which this VolumeSnapshotContent is a part of.
                  type: string
              type: object
          required:
            - spec
          type: object
      served: true
      storage: true
      subresources:
        status: { }
    - additionalPrinterColumns:
        - description: Indicates if the snapshot is ready to be used to restore a volume.
          jsonPath: .status.readyToUse
          name: ReadyToUse
          type: boolean
        - description: Represents the complete size of the snapshot in bytes
          jsonPath: .status.restoreSize
          name: RestoreSize
          type: integer
        - description: Determines whether this VolumeSnapshotContent and its physical snapshot on the underlying storage system should be deleted when its bound VolumeSnapshot is deleted.
          jsonPath: .spec.deletionPolicy
          name: DeletionPolicy
          type: string
        - description: Name of the CSI driver used to create the physical snapshot on the underlying storage system.
          jsonPath: .spec.driver
          name: Driver
          type: string
        - description: Name of the VolumeSnapshotClass to which this snapshot belongs.
          jsonPath: .spec.volumeSnapshotClassName
          name: VolumeSnapshotClass
          type: string
        - description: Name of the VolumeSnapshot object to which this VolumeSnapshotContent object is bound.
          jsonPath: .spec.volumeSnapshotRef.name
          name: VolumeSnapshot
          type: string
        - description: Namespace of the VolumeSnapshot object to which this VolumeSnapshotContent object is bound.
          jsonPath: .spec.volumeSnapshotRef.namespace
          name: VolumeSnapshotNamespace
          type: string
        - jsonPath: .metadata.creationTimestamp
          name: Age
          type: date
      name: v1beta1
      # This indicates the v1beta1 version of the custom resource is deprecated.
      # API requests to this version receive a warning in the server response.
      deprecated: true
      # This overrides the default warning returned to clients making v1beta1 API requests.
      deprecationWarning: "snapshot.storage.k8s.io/v1beta1 VolumeSnapshotContent is deprecated; use snapshot.storage.k8s.io/v1 VolumeSnapshotContent"
      schema:
        openAPIV3Schema:
          description: VolumeSnapshotContent represents the actual "on-disk" snapshot object in the underlying storage system
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            spec:
              description: spec defines properties of a VolumeSnapshotContent created by the underlying storage system. Required.
              properties:
                deletionPolicy:
                  description: deletionPolicy determines whether this VolumeSnapshotContent and its physical snapshot on the underlying storage system should be deleted when its bound VolumeSnapshot is deleted. Supported values are "Retain" and "Delete". "Retain" means that the VolumeSnapshotContent and its physical snapshot on underlying storage system are kept. "Delete" means that the VolumeSnapshotContent and its physical snapshot on underlying storage system are deleted. For dynamically provisioned snapshots, this field will automatically be filled in by the CSI snapshotter sidecar with the "DeletionPolicy" field defined in the corresponding VolumeSnapshotClass. For pre-existing snapshots, users MUST specify this field when creating the  VolumeSnapshotContent object. Required.
                  enum:
                    - Delete
                    - Retain
                  type: string
                driver:
                  description: driver is the name of the CSI driver used to create the physical snapshot on the underlying storage system. This MUST be the same as the name returned by the CSI GetPluginName() call for that driver. Required.
                  type: string
                source:
                  description: source specifies whether the snapshot is (or should be) dynamically provisioned or already exists, and just requires a Kubernetes object representation. This field is immutable after creation. Required.
                  properties:
                    snapshotHandle:
                      description: snapshotHandle specifies the CSI "snapshot_id" of a pre-existing snapshot on the underlying storage system for which a Kubernetes object representation was (or should be) created. This field is immutable.
                      type: string
                    volumeHandle:
                      description: volumeHandle specifies the CSI "volume_id" of the volume from which a snapshot should be dynamically taken from. This field is immutable.
                      type: string
                  type: object
                volumeSnapshotClassName:
                  description: name of the VolumeSnapshotClass from which this snapshot was (or will be) created. Note that after provisioning, the VolumeSnapshotClass may be deleted or recreated with different set of values, and as such, should not be referenced post-snapshot creation.
                  type: string
                volumeSnapshotRef:
                  description: volumeSnapshotRef specifies the VolumeSnapshot object to which this VolumeSnapshotContent object is bound. VolumeSnapshot.Spec.VolumeSnapshotContentName field must reference to this VolumeSnapshotContent's name for the bidirectional binding to be valid. For a pre-existing VolumeSnapshotContent object, name and namespace of the VolumeSnapshot object MUST be provided for binding to happen. This field is immutable after creation. Required.
                  properties:
                    apiVersion:
                      description: API version of the referent.
                      type: string
                    fieldPath:
                      description: 'If referring to a piece of an object instead of an entire object, this string should contain a valid JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object reference is to a container within a pod, this would take on a value like: "spec.containers{name}" (where "name" refers to the name of the container that triggered the event) or if no container name is specified "spec.containers[2]" (container with index 2 in this pod). This syntax is chosen only to have some well-defined way of referencing a part of an object. TODO: this design is not final and this field is subject to change in the future.'
                      type: string
                    kind:
                      description: 'Kind of the referent. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
                      type: string
                    name:
                      description: 'Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names'
                      type: string
                    namespace:
                      description: 'Namespace of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/'
                      type: string
                    resourceVersion:
                      description: 'Specific resourceVersion to which this reference is made, if any. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency'
                      type: string
                    uid:
                      description: 'UID of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids'
                      type: string
                  type: object
              required:
                - deletionPolicy
                - driver
                - source
                - volumeSnapshotRef
              type: object
            status:
              description: status represents the current information of a snapshot.
              properties:
                creationTime:
                  description: creationTime is the timestamp when the point-in-time snapshot is taken by the underlying storage system. In dynamic snapshot creation case, this field will be filled in by the CSI snapshotter sidecar with the "creation_time" value returned from CSI "CreateSnapshot" gRPC call. For a pre-existing snapshot, this field will be filled with the "creation_time" value returned from the CSI "ListSnapshots" gRPC call if the driver supports it. If not specified, it indicates the creation time is unknown. The format of this field is a Unix nanoseconds time encoded as an int64. On Unix, the command `date +%s%N` returns the current time in nanoseconds since 1970-01-01 00:00:00 UTC.
                  format: int64
                  type: integer
                error:
                  description: error is the last observed error during snapshot creation, if any. Upon success after retry, this error field will be cleared.
                  properties:
                    message:
                      description: 'message is a string detailing the encountered error during snapshot creation if specified. NOTE: message may be logged, and it should not contain sensitive information.'
                      type: string
                    time:
                      description: time is the timestamp when the error was encountered.
                      format: date-time
                      type: string
                  type: object
                readyToUse:
                  description: readyToUse indicates if a snapshot is ready to be used to restore a volume. In dynamic snapshot creation case, this field will be filled in by the CSI snapshotter sidecar with the "ready_to_use" value returned from CSI "CreateSnapshot" gRPC call. For a pre-existing snapshot, this field will be filled with the "ready_to_use" value returned from the CSI "ListSnapshots" gRPC call if the driver supports it, otherwise, this field will be set to "True". If not specified, it means the readiness of a snapshot is unknown.
                  type: boolean
                restoreSize:
                  description: restoreSize represents the complete size of the snapshot in bytes. In dynamic snapshot creation case, this field will be filled in by the CSI snapshotter sidecar with the "size_bytes" value returned from CSI "CreateSnapshot" gRPC call. For a pre-existing snapshot, this field will be filled with the "size_bytes" value returned from the CSI "ListSnapshots" gRPC call if the driver supports it. When restoring a volume from this snapshot, the size of the volume MUST NOT be smaller than the restoreSize if it is specified, otherwise the restoration will fail. If not specified, it indicates that the size is unknown.
                  format: int64
                  minimum: 0
                  type: integer
                snapshotHandle:
                  description: snapshotHandle is the CSI "snapshot_id" of a snapshot on the underlying storage system. If not specified, it indicates that dynamic snapshot creation has either failed or it is still in progress.
                  type: string
              type: object
          required:
            - spec
          type: object
      served: false
      storage: false
      subresources:
        status: { }
status:
  acceptedNames:
    kind: ""
    plural: ""
  conditions: [ ]
  storedVersions: [ ]
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/volumesnapshots-crd.yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.11.3
    api-approved.kubernetes.io: "https://github.com/kubernetes-csi/external-snapshotter/pull/814"
  creationTimestamp: null
  name: volumesnapshots.snapshot.storage.k8s.io
spec:
  group: snapshot.storage.k8s.io
  names:
    kind: VolumeSnapshot
    listKind: VolumeSnapshotList
    plural: volumesnapshots
    shortNames:
      - vs
    singular: volumesnapshot
  scope: Namespaced
  versions:
    - additionalPrinterColumns:
        - description: Indicates if the snapshot is ready to be used to restore a volume.
          jsonPath: .status.readyToUse
          name: ReadyToUse
          type: boolean
        - description: If a new snapshot needs to be created, this contains the name of
            the source PVC from which this snapshot was (or will be) created.
          jsonPath: .spec.source.persistentVolumeClaimName
          name: SourcePVC
          type: string
        - description: If a snapshot already exists, this contains the name of the existing
            VolumeSnapshotContent object representing the existing snapshot.
          jsonPath: .spec.source.volumeSnapshotContentName
          name: SourceSnapshotContent
          type: string
        - description: Represents the minimum size of volume required to rehydrate from
            this snapshot.
          jsonPath: .status.restoreSize
          name: RestoreSize
          type: string
        - description: The name of the VolumeSnapshotClass requested by the VolumeSnapshot.
          jsonPath: .spec.volumeSnapshotClassName
          name: SnapshotClass
          type: string
        - description: Name of the VolumeSnapshotContent object to which the VolumeSnapshot
            object intends to bind to. Please note that verification of binding actually
            requires checking both VolumeSnapshot and VolumeSnapshotContent to ensure
            both are pointing at each other. Binding MUST be verified prior to usage of
            this object.
          jsonPath: .status.boundVolumeSnapshotContentName
          name: SnapshotContent
          type: string
        - description: Timestamp when the point-in-time snapshot was taken by the underlying
            storage system.
          jsonPath: .status.creationTime
          name: CreationTime
          type: date
        - jsonPath: .metadata.creationTimestamp
          name: Age
          type: date
      name: v1
      schema:
        openAPIV3Schema:
          description: VolumeSnapshot is a user's request for either creating a point-in-time
            snapshot of a persistent volume, or binding to a pre-existing snapshot.
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation
              of an object. Servers should convert recognized schemas to the latest
              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource this
              object represents. Servers may infer this from the endpoint the client
              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            spec:
              description: 'spec defines the desired characteristics of a snapshot requested
              by a user. More info: https://kubernetes.io/docs/concepts/storage/volume-snapshots#volumesnapshots
              Required.'
              properties:
                source:
                  description: source specifies where a snapshot will be created from.
                    This field is immutable after creation. Required.
                  properties:
                    persistentVolumeClaimName:
                      description: persistentVolumeClaimName specifies the name of the
                        PersistentVolumeClaim object representing the volume from which
                        a snapshot should be created. This PVC is assumed to be in the
                        same namespace as the VolumeSnapshot object. This field should
                        be set if the snapshot does not exists, and needs to be created.
                        This field is immutable.
                      type: string
                    volumeSnapshotContentName:
                      description: volumeSnapshotContentName specifies the name of a
                        pre-existing VolumeSnapshotContent object representing an existing
                        volume snapshot. This field should be set if the snapshot already
                        exists and only needs a representation in Kubernetes. This field
                        is immutable.
                      type: string
                  type: object
                  oneOf:
                    - required: ["persistentVolumeClaimName"]
                    - required: ["volumeSnapshotContentName"]
                volumeSnapshotClassName:
                  description: 'VolumeSnapshotClassName is the name of the VolumeSnapshotClass
                  requested by the VolumeSnapshot. VolumeSnapshotClassName may be
                  left nil to indicate that the default SnapshotClass should be used.
                  A given cluster may have multiple default Volume SnapshotClasses:
                  one default per CSI Driver. If a VolumeSnapshot does not specify
                  a SnapshotClass, VolumeSnapshotSource will be checked to figure
                  out what the associated CSI Driver is, and the default VolumeSnapshotClass
                  associated with that CSI Driver will be used. If more than one VolumeSnapshotClass
                  exist for a given CSI Driver and more than one have been marked
                  as default, CreateSnapshot will fail and generate an event. Empty
                  string is not allowed for this field.'
                  type: string
              required:
                - source
              type: object
            status:
              description: status represents the current information of a snapshot.
                Consumers must verify binding between VolumeSnapshot and VolumeSnapshotContent
                objects is successful (by validating that both VolumeSnapshot and VolumeSnapshotContent
                point at each other) before using this object.
              properties:
                boundVolumeSnapshotContentName:
                  description: 'boundVolumeSnapshotContentName is the name of the VolumeSnapshotContent
                  object to which this VolumeSnapshot object intends to bind to. If
                  not specified, it indicates that the VolumeSnapshot object has not
                  been successfully bound to a VolumeSnapshotContent object yet. NOTE:
                  To avoid possible security issues, consumers must verify binding
                  between VolumeSnapshot and VolumeSnapshotContent objects is successful
                  (by validating that both VolumeSnapshot and VolumeSnapshotContent
                  point at each other) before using this object.'
                  type: string
                creationTime:
                  description: creationTime is the timestamp when the point-in-time
                    snapshot is taken by the underlying storage system. In dynamic snapshot
                    creation case, this field will be filled in by the snapshot controller
                    with the "creation_time" value returned from CSI "CreateSnapshot"
                    gRPC call. For a pre-existing snapshot, this field will be filled
                    with the "creation_time" value returned from the CSI "ListSnapshots"
                    gRPC call if the driver supports it. If not specified, it may indicate
                    that the creation time of the snapshot is unknown.
                  format: date-time
                  type: string
                error:
                  description: error is the last observed error during snapshot creation,
                    if any. This field could be helpful to upper level controllers(i.e.,
                    application controller) to decide whether they should continue on
                    waiting for the snapshot to be created based on the type of error
                    reported. The snapshot controller will keep retrying when an error
                    occurs during the snapshot creation. Upon success, this error field
                    will be cleared.
                  properties:
                    message:
                      description: 'message is a string detailing the encountered error
                      during snapshot creation if specified. NOTE: message may be
                      logged, and it should not contain sensitive information.'
                      type: string
                    time:
                      description: time is the timestamp when the error was encountered.
                      format: date-time
                      type: string
                  type: object
                readyToUse:
                  description: readyToUse indicates if the snapshot is ready to be used
                    to restore a volume. In dynamic snapshot creation case, this field
                    will be filled in by the snapshot controller with the "ready_to_use"
                    value returned from CSI "CreateSnapshot" gRPC call. For a pre-existing
                    snapshot, this field will be filled with the "ready_to_use" value
                    returned from the CSI "ListSnapshots" gRPC call if the driver supports
                    it, otherwise, this field will be set to "True". If not specified,
                    it means the readiness of a snapshot is unknown.
                  type: boolean
                restoreSize:
                  type: string
                  description: restoreSize represents the minimum size of volume required
                    to create a volume from this snapshot. In dynamic snapshot creation
                    case, this field will be filled in by the snapshot controller with
                    the "size_bytes" value returned from CSI "CreateSnapshot" gRPC call.
                    For a pre-existing snapshot, this field will be filled with the
                    "size_bytes" value returned from the CSI "ListSnapshots" gRPC call
                    if the driver supports it. When restoring a volume from this snapshot,
                    the size of the volume MUST NOT be smaller than the restoreSize
                    if it is specified, otherwise the restoration will fail. If not
                    specified, it indicates that the size is unknown.
                  pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
                  x-kubernetes-int-or-string: true
                volumeGroupSnapshotName:
                  description: VolumeGroupSnapshotName is the name of the VolumeGroupSnapshot
                    of which this VolumeSnapshot is a part of.
                  type: string
              type: object
          required:
            - spec
          type: object
      served: true
      storage: true
      subresources:
        status: {}
    - additionalPrinterColumns:
        - description: Indicates if the snapshot is ready to be used to restore a volume.
          jsonPath: .status.readyToUse
          name: ReadyToUse
          type: boolean
        - description: If a new snapshot needs to be created, this contains the name of the source PVC from which this snapshot was (or will be) created.
          jsonPath: .spec.source.persistentVolumeClaimName
          name: SourcePVC
          type: string
        - description: If a snapshot already exists, this contains the name of the existing VolumeSnapshotContent object representing the existing snapshot.
          jsonPath: .spec.source.volumeSnapshotContentName
          name: SourceSnapshotContent
          type: string
        - description: Represents the minimum size of volume required to rehydrate from this snapshot.
          jsonPath: .status.restoreSize
          name: RestoreSize
          type: string
        - description: The name of the VolumeSnapshotClass requested by the VolumeSnapshot.
          jsonPath: .spec.volumeSnapshotClassName
          name: SnapshotClass
          type: string
        - description: Name of the VolumeSnapshotContent object to which the VolumeSnapshot object intends to bind to. Please note that verification of binding actually requires checking both VolumeSnapshot and VolumeSnapshotContent to ensure both are pointing at each other. Binding MUST be verified prior to usage of this object.
          jsonPath: .status.boundVolumeSnapshotContentName
          name: SnapshotContent
          type: string
        - description: Timestamp when the point-in-time snapshot was taken by the underlying storage system.
          jsonPath: .status.creationTime
          name: CreationTime
          type: date
        - jsonPath: .metadata.creationTimestamp
          name: Age
          type: date
      name: v1beta1
      # This indicates the v1beta1 version of the custom resource is deprecated.
      # API requests to this version receive a warning in the server response.
      deprecated: true
      # This overrides the default warning returned to clients making v1beta1 API requests.
      deprecationWarning: "snapshot.storage.k8s.io/v1beta1 VolumeSnapshot is deprecated; use snapshot.storage.k8s.io/v1 VolumeSnapshot"
      schema:
        openAPIV3Schema:
          description: VolumeSnapshot is a user's request for either creating a point-in-time snapshot of a persistent volume, or binding to a pre-existing snapshot.
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            spec:
              description: 'spec defines the desired characteristics of a snapshot requested by a user. More info: https://kubernetes.io/docs/concepts/storage/volume-snapshots#volumesnapshots Required.'
              properties:
                source:
                  description: source specifies where a snapshot will be created from. This field is immutable after creation. Required.
                  properties:
                    persistentVolumeClaimName:
                      description: persistentVolumeClaimName specifies the name of the PersistentVolumeClaim object representing the volume from which a snapshot should be created. This PVC is assumed to be in the same namespace as the VolumeSnapshot object. This field should be set if the snapshot does not exists, and needs to be created. This field is immutable.
                      type: string
                    volumeSnapshotContentName:
                      description: volumeSnapshotContentName specifies the name of a pre-existing VolumeSnapshotContent object representing an existing volume snapshot. This field should be set if the snapshot already exists and only needs a representation in Kubernetes. This field is immutable.
                      type: string
                  type: object
                volumeSnapshotClassName:
                  description: 'VolumeSnapshotClassName is the name of the VolumeSnapshotClass requested by the VolumeSnapshot. VolumeSnapshotClassName may be left nil to indicate that the default SnapshotClass should be used. A given cluster may have multiple default Volume SnapshotClasses: one default per CSI Driver. If a VolumeSnapshot does not specify a SnapshotClass, VolumeSnapshotSource will be checked to figure out what the associated CSI Driver is, and the default VolumeSnapshotClass associated with that CSI Driver will be used. If more than one VolumeSnapshotClass exist for a given CSI Driver and more than one have been marked as default, CreateSnapshot will fail and generate an event. Empty string is not allowed for this field.'
                  type: string
              required:
                - source
              type: object
            status:
              description: status represents the current information of a snapshot. Consumers must verify binding between VolumeSnapshot and VolumeSnapshotContent objects is successful (by validating that both VolumeSnapshot and VolumeSnapshotContent point at each other) before using this object.
              properties:
                boundVolumeSnapshotContentName:
                  description: 'boundVolumeSnapshotContentName is the name of the VolumeSnapshotContent object to which this VolumeSnapshot object intends to bind to. If not specified, it indicates that the VolumeSnapshot object has not been successfully bound to a VolumeSnapshotContent object yet. NOTE: To avoid possible security issues, consumers must verify binding between VolumeSnapshot and VolumeSnapshotContent objects is successful (by validating that both VolumeSnapshot and VolumeSnapshotContent point at each other) before using this object.'
                  type: string
                creationTime:
                  description: creationTime is the timestamp when the point-in-time snapshot is taken by the underlying storage system. In dynamic snapshot creation case, this field will be filled in by the snapshot controller with the "creation_time" value returned from CSI "CreateSnapshot" gRPC call. For a pre-existing snapshot, this field will be filled with the "creation_time" value returned from the CSI "ListSnapshots" gRPC call if the driver supports it. If not specified, it may indicate that the creation time of the snapshot is unknown.
                  format: date-time
                  type: string
                error:
                  description: error is the last observed error during snapshot creation, if any. This field could be helpful to upper level controllers(i.e., application controller) to decide whether they should continue on waiting for the snapshot to be created based on the type of error reported. The snapshot controller will keep retrying when an error occurs during the snapshot creation. Upon success, this error field will be cleared.
                  properties:
                    message:
                      description: 'message is a string detailing the encountered error during snapshot creation if specified. NOTE: message may be logged, and it should not contain sensitive information.'
                      type: string
                    time:
                      description: time is the timestamp when the error was encountered.
                      format: date-time
                      type: string
                  type: object
                readyToUse:
                  description: readyToUse indicates if the snapshot is ready to be used to restore a volume. In dynamic snapshot creation case, this field will be filled in by the snapshot controller with the "ready_to_use" value returned from CSI "CreateSnapshot" gRPC call. For a pre-existing snapshot, this field will be filled with the "ready_to_use" value returned from the CSI "ListSnapshots" gRPC call if the driver supports it, otherwise, this field will be set to "True". If not specified, it means the readiness of a snapshot is unknown.
                  type: boolean
                restoreSize:
                  type: string
                  description: restoreSize represents the minimum size of volume required to create a volume from this snapshot. In dynamic snapshot creation case, this field will be filled in by the snapshot controller with the "size_bytes" value returned from CSI "CreateSnapshot" gRPC call. For a pre-existing snapshot, this field will be filled with the "size_bytes" value returned from the CSI "ListSnapshots" gRPC call if the driver supports it. When restoring a volume from this snapshot, the size of the volume MUST NOT be smaller than the restoreSize if it is specified, otherwise the restoration will fail. If not specified, it indicates that the size is unknown.
                  pattern: ^(\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))(([KMGTPE]i)|[numkMGTPE]|([eE](\+|-)?(([0-9]+(\.[0-9]*)?)|(\.[0-9]+))))?$
                  x-kubernetes-int-or-string: true
              type: object
          required:
            - spec
          type: object
      served: false
      storage: false
      subresources:
        status: {}
status:
  acceptedNames:
    kind: ""
    plural: ""
  conditions: []
  storedVersions: []
---
# Source: my-openebs/charts/openebs/charts/cstor/charts/openebs-ndm/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-openebs-ndm
rules:
  - apiGroups: ["*"]
    resources: ["nodes", "pods", "events", "configmaps", "jobs"]
    verbs:
      - '*'
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs:
      - '*'
  - apiGroups:
      - openebs.io
    resources:
      - blockdevices
      - blockdeviceclaims
    verbs:
      - '*'
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-controller-rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-csi-snapshotter-role
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-controller"
    release: release-name
    component: "openebs-cstor-csi-controller"
    openebs.io/component-name: "openebs-cstor-csi-controller"
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["list", "watch", "create", "update", "patch"]
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotcontents"]
    verbs: ["create", "get", "list", "watch", "update", "delete", "patch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotcontents/status"]
    verbs: ["update"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshots"]
    verbs: ["get", "list", "watch", "update", "patch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshots/status"]
    verbs: ["update"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "watch", "list", "delete", "update", "create"]
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs: ["create", "list", "watch", "delete", "get", "update"]
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-controller-rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-csi-provisioner-role
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-controller"
    release: release-name
    component: "openebs-cstor-csi-controller"
    openebs.io/component-name: "openebs-cstor-csi-controller"
rules:
  - apiGroups: [""]
    resources: ["secrets","namespaces"]
    verbs: ["get", "list"]
  - apiGroups: [ "" ]
    resources: [ "pods" ]
    verbs: [ "get", "list", "watch" ]
  - apiGroups: [""]
    resources: ["persistentvolumes", "services"]
    verbs: ["get", "list", "watch", "create", "delete", "update", "patch"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims/status"]
    verbs: ["update", "patch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["list", "watch", "create", "update", "patch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshots"]
    verbs: ["get", "list"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["volumeattachments"]
    verbs: ["get", "list", "watch", "create", "delete", "update", "patch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotcontents"]
    verbs: ["get", "list"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["*"]
  - apiGroups: ["*"]
    resources: ["cstorvolumeattachments", "cstorvolumes","cstorvolumeconfigs"]
    verbs: ["*"]
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-controller-rbac.yaml
############################## CSI- Attacher #######################
# Attacher must be able to work with PVs, nodes and VolumeAttachments
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-csi-attacher-role
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-controller"
    release: release-name
    component: "openebs-cstor-csi-controller"
    openebs.io/component-name: "openebs-cstor-csi-controller"
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["csi.storage.k8s.io"]
    resources: ["csinodeinfos"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["volumeattachments", "csinodes"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: [ "storage.k8s.io" ]
    resources: [ "volumeattachments/status" ]
    verbs: [ "patch" ]
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-controller-rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-csi-cluster-registrar-role
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-controller"
    release: release-name
    component: "openebs-cstor-csi-controller"
    openebs.io/component-name: "openebs-cstor-csi-controller"
rules:
  - apiGroups: ["csi.storage.k8s.io"]
    resources: ["csidrivers"]
    verbs: ["create", "delete"]
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-node-rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-csi-registrar-role
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-node"
    release: release-name
    component: "openebs-cstor-csi-node"
    openebs.io/component-name: "openebs-cstor-csi-node"
rules:
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
  - apiGroups: [""]
    resources: ["persistentvolumes", "nodes", "services"]
    verbs: ["get", "list", "patch"]
  - apiGroups: ["*"]
    resources: ["cstorvolumeattachments", "cstorvolumes","cstorvolumeconfigs"]
    verbs: ["get", "list", "watch", "create", "update", "delete", "patch"]
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-operator
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
rules:
  - apiGroups: ["*"]
    resources: ["nodes", "nodes/proxy"]
    verbs: ["*"]
  - apiGroups: ["*"]
    resources: ["namespaces", "services", "pods", "deployments", "deployments/finalizers", "replicationcontrollers", "replicasets", "events", "endpoints", "configmaps", "secrets", "jobs", "cronjobs"]
    verbs: ["*"]
  - apiGroups: ["*"]
    resources: ["statefulsets", "daemonsets"]
    verbs: ["*"]
  - apiGroups: ["*"]
    resources: ["resourcequotas", "limitranges"]
    verbs: ["list", "watch"]
  - apiGroups: ["*"]
    resources: ["certificatesigningrequests"]
    verbs: ["list", "watch"]
  - apiGroups: ["*"]
    resources: ["storageclasses", "persistentvolumeclaims", "persistentvolumes"]
    verbs: ["*"]
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs: [ "get", "list", "create", "update", "delete", "patch"]
  - apiGroups: ["openebs.io"]
    resources: ["*"]
    verbs: ["*" ]
  - apiGroups: ["cstor.openebs.io"]
    resources: ["*"]
    verbs: ["*" ]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "watch", "list", "delete", "update", "create"]
  - apiGroups: ["admissionregistration.k8s.io"]
    resources: ["validatingwebhookconfigurations", "mutatingwebhookconfigurations"]
    verbs: ["get", "create", "list", "delete", "update", "patch"]
  - nonResourceURLs: ["/metrics"]
    verbs: ["get"]
  - apiGroups: ["*"]
    resources: ["upgradetasks","migrationtasks"]
    verbs: ["*"]
  - apiGroups: ["*"]
    resources: ["poddisruptionbudgets"]
    verbs: ["get", "list", "create", "delete", "watch"]
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/rbac.yaml
# Define Role that allows operations required for migration of snapshots
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-migration
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
rules:
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotclasses"]
    verbs: ["get", "list"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotcontents"]
    verbs: ["create", "get", "list"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshots"]
    verbs: ["create", "get", "list"]
---
# Source: my-openebs/charts/openebs/charts/jiva/charts/localpv-provisioner/charts/openebs-ndm/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-openebs-ndm
rules:
  - apiGroups: ["*"]
    resources: ["nodes", "pods", "events", "configmaps", "jobs"]
    verbs:
      - '*'
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs:
      - '*'
  - apiGroups:
      - openebs.io
    resources:
      - blockdevices
      - blockdeviceclaims
    verbs:
      - '*'
---
# Source: my-openebs/charts/openebs/charts/jiva/charts/localpv-provisioner/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-localpv-provisioner
  labels:
    chart: localpv-provisioner-3.4.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    app: localpv-provisioner
    release: release-name
    component: "localpv-provisioner"
    openebs.io/component-name: openebs-localpv-provisioner
rules:
- apiGroups: ["*"]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["*"]
  resources: ["namespaces", "pods", "events", "endpoints"]
  verbs: ["*"]
- apiGroups: ["*"]
  resources: ["resourcequotas", "limitranges"]
  verbs: ["list", "watch"]
- apiGroups: ["*"]
  resources: ["storageclasses", "persistentvolumeclaims", "persistentvolumes"]
  verbs: ["*"]
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: [ "get", "list", "create", "update", "delete", "patch"]
- apiGroups: ["openebs.io"]
  resources: [ "*"]
  verbs: ["*" ]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-controller-rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-jiva-csi-snapshotter-role
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-controller"
    release: release-name
    component: "openebs-jiva-csi-controller"
    openebs.io/component-name: "openebs-jiva-csi-controller"
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["list", "watch", "create", "update", "patch"]
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotcontents"]
    verbs: ["create", "get", "list", "watch", "update", "delete"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotcontents/status"]
    verbs: ["update"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshots"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshots/status"]
    verbs: ["update"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "watch", "list", "delete", "update", "create"]
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs: ["create", "list", "watch", "delete", "get", "update"]
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-controller-rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-jiva-csi-provisioner-role
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-controller"
    release: release-name
    component: "openebs-jiva-csi-controller"
    openebs.io/component-name: "openebs-jiva-csi-controller"
rules:
  - apiGroups: [""]
    resources: ["secrets","namespaces"]
    verbs: ["get", "list"]
  - apiGroups: [ "" ]
    resources: [ "pods" ]
    verbs: [ "get", "list", "watch" ]
  - apiGroups: [""]
    resources: ["persistentvolumes", "services"]
    verbs: ["get", "list", "watch", "create", "delete", "update", "patch"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims/status"]
    verbs: ["update", "patch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["list", "watch", "create", "update", "patch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshots"]
    verbs: ["get", "list"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["volumeattachments"]
    verbs: ["get", "list", "watch", "create", "delete", "update", "patch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotcontents"]
    verbs: ["get", "list"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["*"]
  - apiGroups: ["*"]
    resources: ["jivavolumeattachments", "jivavolumes","jivavolumeconfigs"]
    verbs: ["*"]
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-controller-rbac.yaml
############################## CSI- Attacher #######################
# Attacher must be able to work with PVs, nodes and VolumeAttachments
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-jiva-csi-attacher-role
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-controller"
    release: release-name
    component: "openebs-jiva-csi-controller"
    openebs.io/component-name: "openebs-jiva-csi-controller"
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["csi.storage.k8s.io"]
    resources: ["csinodeinfos"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["volumeattachments", "csinodes"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: [ "storage.k8s.io" ]
    resources: [ "volumeattachments/status" ]
    verbs: [ "patch" ]
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-controller-rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-jiva-csi-cluster-registrar-role
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-controller"
    release: release-name
    component: "openebs-jiva-csi-controller"
    openebs.io/component-name: "openebs-jiva-csi-controller"
rules:
  - apiGroups: ["csi.storage.k8s.io"]
    resources: ["csidrivers"]
    verbs: ["create", "delete"]
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-node-rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-jiva-csi-registrar-role
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-node"
    release: release-name
    component: "openebs-jiva-csi-node"
    openebs.io/component-name: "openebs-jiva-csi-node"
rules:
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
  - apiGroups: [""]
    resources: ["persistentvolumes", "nodes", "services"]
    verbs: ["get", "list", "patch"]
  - apiGroups: ["*"]
    resources: ["jivavolumes"]
    verbs: ["get", "list", "watch", "create", "update", "delete", "patch"]
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/jiva-operator-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: jiva-operator
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - services
  - services/finalizers
  - endpoints
  - persistentvolumes
  - persistentvolumeclaims
  - events
  - configmaps
  - secrets
  verbs:
  - '*'
- apiGroups:
  - apps
  resources:
  - deployments
  - daemonsets
  - replicasets
  - statefulsets
  verbs:
  - '*'
- apiGroups:
  - monitoring.coreos.com
  resources:
  - servicemonitors
  verbs:
  - get
  - create
- apiGroups:
  - apps
  resourceNames:
  - jiva-operator
  resources:
  - deployments/finalizers
  verbs:
  - update
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - apps
  resources:
  - replicasets
  verbs:
  - get
- apiGroups:
  - policy
  resources:
  - poddisruptionbudgets
  verbs:
  - '*'
- apiGroups:
  - openebs.io
  resources:
  - '*'
  verbs:
  - '*'
---
# Source: my-openebs/charts/openebs/charts/localpv-provisioner/charts/openebs-ndm/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-openebs-ndm
rules:
  - apiGroups: ["*"]
    resources: ["nodes", "pods", "events", "configmaps", "jobs"]
    verbs:
      - '*'
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs:
      - '*'
  - apiGroups:
      - openebs.io
    resources:
      - blockdevices
      - blockdeviceclaims
    verbs:
      - '*'
---
# Source: my-openebs/charts/openebs/charts/localpv-provisioner/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-localpv-provisioner
  labels:
    chart: localpv-provisioner-3.4.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    app: localpv-provisioner
    release: release-name
    component: "localpv-provisioner"
    openebs.io/component-name: openebs-localpv-provisioner
rules:
- apiGroups: ["*"]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["*"]
  resources: ["namespaces", "pods", "events", "endpoints"]
  verbs: ["*"]
- apiGroups: ["*"]
  resources: ["resourcequotas", "limitranges"]
  verbs: ["list", "watch"]
- apiGroups: ["*"]
  resources: ["storageclasses", "persistentvolumeclaims", "persistentvolumes"]
  verbs: ["*"]
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: [ "get", "list", "create", "update", "delete", "patch"]
- apiGroups: ["openebs.io"]
  resources: [ "*"]
  verbs: ["*" ]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-lvm-provisioner-role
  labels:
    chart: lvm-localpv-1.3.0
    heritage: Helm
    openebs.io/version: "1.3.0"
    role: "openebs-lvm"
    app: "openebs-lvm-controller"
    release: release-name
    component: "openebs-lvm-controller"
    openebs.io/component-name: "openebs-lvm-controller"
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list"]
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes", "services"]
    verbs: ["get", "list", "watch", "create", "delete", "update", "patch"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims/status"]
    verbs: ["update", "patch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [ "storage.k8s.io" ]
    resources: [ "csistoragecapacities"]
    verbs: ["*"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["list", "watch", "create", "update", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "watch", "list", "delete", "update", "create"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch", "update", "patch"]
  - apiGroups: ["local.openebs.io"]
    resources: ["lvmvolumes", "lvmsnapshots", "lvmnodes"]
    verbs: ["*"]
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-lvm-snapshotter-role
  labels:
    chart: lvm-localpv-1.3.0
    heritage: Helm
    openebs.io/version: "1.3.0"
    role: "openebs-lvm"
    app: "openebs-lvm-controller"
    release: release-name
    component: "openebs-lvm-controller"
    openebs.io/component-name: "openebs-lvm-controller"
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["list", "watch", "create", "update", "patch"]
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotcontents"]
    verbs: ["create", "get", "list", "watch", "update", "delete", "patch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshots"]
    verbs: ["get", "list", "watch", "update", "patch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotcontents/status"]
    verbs: ["update"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshots/status"]
    verbs: ["update"]
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs: ["create", "list", "watch", "delete"]
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-lvm-driver-registrar-role
  labels:
    chart: lvm-localpv-1.3.0
    heritage: Helm
    openebs.io/version: "1.3.0"
    role: "openebs-lvm"
    name: "openebs-lvm-node"
    release: release-name
    openebs.io/component-name: "openebs-lvm-node"
rules:
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
  - apiGroups: [""]
    resources: ["persistentvolumes", "nodes", "services"]
    verbs: ["get", "list"]
  - apiGroups: ["local.openebs.io"]
    resources: ["lvmvolumes", "lvmsnapshots", "lvmnodes"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/jaeger-operator/templates/role.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-jaeger-operator
  namespace: prod
  labels:
    
    app.kubernetes.io/name: jaeger-operator
    app.kubernetes.io/instance: release-name
rules:
## our own custom resources
- apiGroups:
  - jaegertracing.io
  resources:
  - '*'
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch

## for the operator's own deployment
- apiGroups:
  - apps
  resourceNames:
  - jaeger-operator
  resources:
  - deployments/finalizers
  verbs:
  - update

## regular things the operator manages for an instance, as the result of processing CRs
- apiGroups:
  - ""
  resources:
  - configmaps
  - persistentvolumeclaims
  - pods
  - secrets
  - serviceaccounts
  - services
  - services/finalizers
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - apps
  resources:
  - deployments
  - daemonsets
  - replicasets
  - statefulsets
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
# Ingress for kubernetes 1.14 or higher
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - batch
  resources:
  - jobs
  - cronjobs
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - route.openshift.io
  resources:
  - routes
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - console.openshift.io
  resources:
  - consolelinks
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - autoscaling
  resources:
  - horizontalpodautoscalers
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch

## needed if you want the operator to create service monitors for the Jaeger instances
- apiGroups:
  - monitoring.coreos.com
  resources:
  - servicemonitors
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch

## for the Elasticsearch auto-provisioning
- apiGroups:
  - logging.openshift.io
  resources:
  - elasticsearches
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch

## for the Kafka auto-provisioning
- apiGroups:
  - kafka.strimzi.io
  resources:
  - kafkas
  - kafkausers
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch

## Extra permissions
## This is an extra set of permissions that the Jaeger Operator might make use of if granted

## needed if support for injecting sidecars based on namespace annotation is required
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - 'get'
  - 'list'
  - 'watch'

## needed if support for injecting sidecars based on deployment annotation is required, across all namespaces
- apiGroups:
  - apps
  resources:
  - deployments
  verbs:
  - get
  - list
  - patch
  - update
  - watch

## needed only when .Spec.Ingress.Openshift.DelegateUrls is used
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - clusterrolebindings
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/localpv-provisioner/charts/openebs-ndm/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-openebs-ndm
rules:
  - apiGroups: ["*"]
    resources: ["nodes", "pods", "events", "configmaps", "jobs"]
    verbs:
      - '*'
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs:
      - '*'
  - apiGroups:
      - openebs.io
    resources:
      - blockdevices
      - blockdeviceclaims
    verbs:
      - '*'
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/localpv-provisioner/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-localpv-provisioner
  labels:
    chart: localpv-provisioner-3.4.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    app: localpv-provisioner
    release: release-name
    component: "localpv-provisioner"
    openebs.io/component-name: openebs-localpv-provisioner
rules:
- apiGroups: ["*"]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["*"]
  resources: ["namespaces", "pods", "events", "endpoints"]
  verbs: ["*"]
- apiGroups: ["*"]
  resources: ["resourcequotas", "limitranges"]
  verbs: ["list", "watch"]
- apiGroups: ["*"]
  resources: ["storageclasses", "persistentvolumeclaims", "persistentvolumes"]
  verbs: ["*"]
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: [ "get", "list", "create", "update", "delete", "patch"]
- apiGroups: ["openebs.io"]
  resources: [ "*"]
  verbs: ["*" ]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/filebeat/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-filebeat-cluster-role
  labels:
    app: "release-name-filebeat"
    chart: "filebeat-7.17.1"
    heritage: "Helm"
    release: "release-name"
rules: 
  - apiGroups:
    - ""
    resources:
    - namespaces
    - nodes
    - pods
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - apps
    resources:
    - replicasets
    verbs:
    - get
    - list
    - watch
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/fluent-bit/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    app: fluent-bit-loki
    chart: fluent-bit-2.3.1
    release: release-name
    heritage: Helm
  name: release-name-fluent-bit-loki-clusterrole
rules:
- apiGroups: [""] # "" indicates the core API group
  resources:
  - namespaces
  - pods
  verbs: ["get", "watch", "list"]
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
  name: release-name-grafana-clusterrole
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["configmaps", "secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/charts/kube-state-metrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-4.4.3
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.3.0"
  name: release-name-kube-state-metrics
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/alertmanager/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-alertmanager
rules:
  []
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/pushgateway/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    component: "pushgateway"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-pushgateway
rules:
  []
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/server/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    component: "server"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-server
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - nodes/metrics
      - services
      - endpoints
      - pods
      - ingresses
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
      - ingresses
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/promtail/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-promtail
  labels:
    helm.sh/chart: promtail-3.11.0
    app.kubernetes.io/name: promtail
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.4.2"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - services
      - endpoints
      - pods
    verbs:
      - get
      - watch
      - list
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/rbac/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-cluster-role
  labels:
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
rules:
  # must create mayastor crd if it doesn't exist, replace if exist,
  # merge schema to existing CRD.
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: ["create", "get", "update", "list", "patch", "replace"]
  # must update stored_version in status to include new schema only.
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions/status"]
  verbs: ["get", "update", "patch"]
  # must read mayastorpools info. This is needed to handle upgrades from v1.
- apiGroups: [ "openebs.io" ]
  resources: [ "mayastorpools" ]
  verbs: ["get", "list", "patch", "delete", "deletecollection"]
  # must read diskpool info
- apiGroups: ["openebs.io"]
  resources: ["diskpools"]
  verbs: ["get", "list", "watch", "update", "replace", "patch", "create"]
  # must update diskpool status
- apiGroups: ["openebs.io"]
  resources: ["diskpools/status"]
  verbs: ["update", "patch"]
  # must read cm info
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create", "get", "update", "patch"]
  # must get deployments info
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list"]
  # external provisioner & attacher
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "list", "watch", "update", "create", "delete", "patch"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]

  # external provisioner
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list", "watch", "update"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["events"]
  verbs: ["list", "watch", "create", "update", "patch"]

  # external snapshotter and snapshot-controller
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshotclasses"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshotcontents"]
  verbs: ["create","get", "list", "watch", "update", "patch", "delete"]
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshotcontents/status"]
  verbs: ["update", "patch"]
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshots"]
  verbs: ["get", "list", "watch", "update", "patch", "delete"]
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshots/status"]
  verbs: ["update", "patch"]

- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]

  # external attacher
- apiGroups: ["storage.k8s.io"]
  resources: ["volumeattachments"]
  verbs: ["get", "list", "watch", "update", "patch"]
- apiGroups: ["storage.k8s.io"]
  resources: ["volumeattachments/status"]
  verbs: ["patch"]
  # CSI nodes must be listed
- apiGroups: ["storage.k8s.io"]
  resources: ["csinodes"]
  verbs: ["get", "list", "watch"]
  # get kube-system namespace to retrieve Uid
- apiGroups: [""]
  resources: ["namespaces"]
  verbs: ["get"]
---
# Source: my-openebs/charts/openebs/charts/nfs-provisioner/templates/clusterrole.yaml
# Define Role that allows operations on K8s pods/deployments
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-nfs-provisioner
  labels:
    chart: nfs-provisioner-0.10.0
    heritage: Helm
    openebs.io/version: "0.10.0"
    app: nfs-provisioner
    release: release-name
    component: nfs-provisioner
    openebs.io/component-name: openebs-nfs-provisioner
rules:
  - apiGroups: ["*"]
    resources: ["nodes", "nodes/proxy"]
    verbs: ["*"]
  - apiGroups: ["*"]
    resources: ["namespaces", "services", "pods", "pods/exec", "deployments", "deployments/finalizers", "replicationcontrollers", "replicasets", "events", "endpoints", "configmaps", "secrets", "jobs", "cronjobs"]
    verbs: ["*"]
  - apiGroups: ["*"]
    resources: ["statefulsets", "daemonsets"]
    verbs: ["*"]
  - apiGroups: ["*"]
    resources: ["resourcequotas", "limitranges"]
    verbs: ["list", "watch"]
  - apiGroups: ["*"]
    resources: ["ingresses", "horizontalpodautoscalers", "verticalpodautoscalers", "poddisruptionbudgets", "certificatesigningrequests"]
    verbs: ["list", "watch"]
  - apiGroups: ["*"]
    resources: ["storageclasses", "persistentvolumeclaims", "persistentvolumes"]
    verbs: ["*"]
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs: [ "get", "list", "create", "update", "delete", "patch"]
  - apiGroups: ["openebs.io"]
    resources: [ "*"]
    verbs: ["*"]
  - nonResourceURLs: ["/metrics"]
    verbs: ["get"]
---
# Source: my-openebs/charts/openebs/charts/openebs-ndm/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-openebs-ndm
rules:
  - apiGroups: ["*"]
    resources: ["nodes", "pods", "events", "configmaps", "jobs"]
    verbs:
      - '*'
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs:
      - '*'
  - apiGroups:
      - openebs.io
    resources:
      - blockdevices
      - blockdeviceclaims
    verbs:
      - '*'
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-zfs-provisioner-role
  labels:
    chart: zfs-localpv-2.3.1
    heritage: Helm
    openebs.io/version: "2.3.1"
    role: "openebs-zfs"
    app: "openebs-zfs-controller"
    release: release-name
    component: "openebs-zfs-controller"
    openebs.io/component-name: "openebs-zfs-controller"
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list"]
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: ["*"]
  - apiGroups: [""]
    resources: ["persistentvolumes", "services"]
    verbs: ["get", "list", "watch", "create", "delete", "update", "patch"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims/status"]
    verbs: ["update", "patch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [ "storage.k8s.io" ]
    resources: [ "csistoragecapacities"]
    verbs: ["*"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["list", "watch", "create", "update", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["get", "watch", "list", "delete", "update", "create"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch", "update", "patch"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch", "update", "patch"]
  - apiGroups: ["*"]
    resources: ["zfsvolumes", "zfssnapshots", "zfsbackups", "zfsrestores", "zfsnodes"]
    verbs: ["*"]
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-zfs-snapshotter-role
  labels:
    chart: zfs-localpv-2.3.1
    heritage: Helm
    openebs.io/version: "2.3.1"
    role: "openebs-zfs"
    app: "openebs-zfs-controller"
    release: release-name
    component: "openebs-zfs-controller"
    openebs.io/component-name: "openebs-zfs-controller"
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["list", "watch", "create", "update", "patch"]
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotcontents"]
    verbs: ["create", "get", "list", "watch", "update", "delete", "patch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshots"]
    verbs: ["get", "list", "watch", "update", "patch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotcontents/status"]
    verbs: ["update"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshots/status"]
    verbs: ["update"]
  - apiGroups: ["apiextensions.k8s.io"]
    resources: ["customresourcedefinitions"]
    verbs: ["create", "list", "watch", "delete"]
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/rbac.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-zfs-driver-registrar-role
  labels:
    chart: zfs-localpv-2.3.1
    heritage: Helm
    openebs.io/version: "2.3.1"
    role: "openebs-zfs"
    name: "openebs-zfs-node"
    release: release-name
    openebs.io/component-name: "openebs-zfs-node"
rules:
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
  - apiGroups: [""]
    resources: ["persistentvolumes", "nodes", "services"]
    verbs: ["get", "list"]
  - apiGroups: ["*"]
    resources: ["zfsvolumes", "zfssnapshots", "zfsbackups", "zfsrestores", "zfsnodes"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
# Source: my-openebs/charts/openebs/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-openebs
  labels:
    app: openebs
    chart: openebs-3.9.0
    release: release-name
    heritage: Helm
rules:
- apiGroups: ["*"]
  resources: ["nodes", "nodes/proxy"]
  verbs: ["*"]
- apiGroups: ["*"]
  resources: ["namespaces", "services", "pods", "pods/exec", "deployments", "deployments/finalizers", "replicationcontrollers", "replicasets", "events", "endpoints", "configmaps", "secrets",  "jobs", "cronjobs" ]
  verbs: ["*"]
- apiGroups: ["*"]
  resources: ["statefulsets", "daemonsets"]
  verbs: ["*"]
- apiGroups: ["*"]
  resources: ["resourcequotas", "limitranges"]
  verbs: ["list", "watch"]
- apiGroups: ["*"]
  resources: ["ingresses", "horizontalpodautoscalers", "verticalpodautoscalers", "poddisruptionbudgets", "certificatesigningrequests"]
  verbs: ["list", "watch"]
- apiGroups: ["*"]
  resources: ["storageclasses", "persistentvolumeclaims", "persistentvolumes"]
  verbs: ["*"]
- apiGroups: ["volumesnapshot.external-storage.k8s.io"]
  resources: ["volumesnapshots", "volumesnapshotdatas"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: [ "get", "list", "create", "update", "delete", "patch"]
- apiGroups: ["openebs.io"]
  resources: [ "*"]
  verbs: ["*" ]
- apiGroups: ["cstor.openebs.io"]
  resources: [ "*"]
  verbs: ["*" ]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["get", "watch", "list", "delete", "update", "create"]
- apiGroups: ["admissionregistration.k8s.io"]
  resources: ["validatingwebhookconfigurations", "mutatingwebhookconfigurations"]
  verbs: ["get", "create", "list", "delete", "update", "patch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
# Source: my-openebs/charts/openebs/charts/cstor/charts/openebs-ndm/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-openebs-ndm
subjects:
  - kind: ServiceAccount
    name: openebs-ndm
    namespace: prod
  - kind: User
    name: system:serviceaccount:default:default
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: release-name-openebs-ndm
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-controller-rbac.yaml
# cstor csi roles and bindings
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-csi-snapshotter-binding
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-controller"
    release: release-name
    component: "openebs-cstor-csi-controller"
    openebs.io/component-name: "openebs-cstor-csi-controller"
subjects:
  - kind: ServiceAccount
    name: openebs-cstor-csi-controller-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-cstor-csi-snapshotter-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-controller-rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-csi-provisioner-binding
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-controller"
    release: release-name
    component: "openebs-cstor-csi-controller"
    openebs.io/component-name: "openebs-cstor-csi-controller"
subjects:
  - kind: ServiceAccount
    name: openebs-cstor-csi-controller-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-cstor-csi-provisioner-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-controller-rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-csi-attacher-binding
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-controller"
    release: release-name
    component: "openebs-cstor-csi-controller"
    openebs.io/component-name: "openebs-cstor-csi-controller"
subjects:
  - kind: ServiceAccount
    name: openebs-cstor-csi-controller-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-cstor-csi-attacher-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-controller-rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-csi-cluster-registrar-binding
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-controller"
    release: release-name
    component: "openebs-cstor-csi-controller"
    openebs.io/component-name: "openebs-cstor-csi-controller"
subjects:
  - kind: ServiceAccount
    name: openebs-cstor-csi-controller-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-cstor-csi-cluster-registrar-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-node-rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-csi-registrar-binding
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-node"
    release: release-name
    component: "openebs-cstor-csi-node"
    openebs.io/component-name: "openebs-cstor-csi-node"
subjects:
  - kind: ServiceAccount
    name: openebs-cstor-csi-node-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-cstor-csi-registrar-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: openebs-cstor-operator
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: openebs-cstor-operator
subjects:
- kind: ServiceAccount
  name: openebs-cstor-operator
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-cstor-migration
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
subjects:
- kind: ServiceAccount
  name: openebs-cstor-operator
  namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-cstor-migration
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/jiva/charts/localpv-provisioner/charts/openebs-ndm/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-openebs-ndm
subjects:
  - kind: ServiceAccount
    name: openebs-ndm
    namespace: prod
  - kind: User
    name: system:serviceaccount:default:default
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: release-name-openebs-ndm
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/jiva/charts/localpv-provisioner/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-localpv-provisioner
  labels:
    chart: localpv-provisioner-3.4.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    app: localpv-provisioner
    release: release-name
    component: "localpv-provisioner"
    openebs.io/component-name: openebs-localpv-provisioner
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-localpv-provisioner
subjects:
- kind: ServiceAccount
  name: release-name-localpv-provisioner
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-controller-rbac.yaml
# jiva csi roles and bindings
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-jiva-csi-snapshotter-binding
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-controller"
    release: release-name
    component: "openebs-jiva-csi-controller"
    openebs.io/component-name: "openebs-jiva-csi-controller"
subjects:
  - kind: ServiceAccount
    name: openebs-jiva-csi-controller-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-jiva-csi-snapshotter-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-controller-rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-jiva-csi-provisioner-binding
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-controller"
    release: release-name
    component: "openebs-jiva-csi-controller"
    openebs.io/component-name: "openebs-jiva-csi-controller"
subjects:
  - kind: ServiceAccount
    name: openebs-jiva-csi-controller-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-jiva-csi-provisioner-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-controller-rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-jiva-csi-attacher-binding
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-controller"
    release: release-name
    component: "openebs-jiva-csi-controller"
    openebs.io/component-name: "openebs-jiva-csi-controller"
subjects:
  - kind: ServiceAccount
    name: openebs-jiva-csi-controller-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-jiva-csi-attacher-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-controller-rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-jiva-csi-cluster-registrar-binding
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-controller"
    release: release-name
    component: "openebs-jiva-csi-controller"
    openebs.io/component-name: "openebs-jiva-csi-controller"
subjects:
  - kind: ServiceAccount
    name: openebs-jiva-csi-controller-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-jiva-csi-cluster-registrar-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-node-rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-jiva-csi-registrar-binding
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-node"
    release: release-name
    component: "openebs-jiva-csi-node"
    openebs.io/component-name: "openebs-jiva-csi-node"
subjects:
  - kind: ServiceAccount
    name: openebs-jiva-csi-node-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-jiva-csi-registrar-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/jiva-operator-rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-jiva-operator
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
subjects:
- kind: ServiceAccount
  name: openebs-jiva-operator
  namespace: prod
roleRef:
  kind: ClusterRole
  name: jiva-operator
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/localpv-provisioner/charts/openebs-ndm/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-openebs-ndm
subjects:
  - kind: ServiceAccount
    name: openebs-ndm
    namespace: prod
  - kind: User
    name: system:serviceaccount:default:default
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: release-name-openebs-ndm
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/localpv-provisioner/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-localpv-provisioner
  labels:
    chart: localpv-provisioner-3.4.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    app: localpv-provisioner
    release: release-name
    component: "localpv-provisioner"
    openebs.io/component-name: openebs-localpv-provisioner
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-localpv-provisioner
subjects:
- kind: ServiceAccount
  name: release-name-localpv-provisioner
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-lvm-provisioner-binding
  labels:
    chart: lvm-localpv-1.3.0
    heritage: Helm
    openebs.io/version: "1.3.0"
    role: "openebs-lvm"
    app: "openebs-lvm-controller"
    release: release-name
    component: "openebs-lvm-controller"
    openebs.io/component-name: "openebs-lvm-controller"
subjects:
  - kind: ServiceAccount
    name: openebs-lvm-controller-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-lvm-provisioner-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-lvm-snapshotter-binding
  labels:
    chart: lvm-localpv-1.3.0
    heritage: Helm
    openebs.io/version: "1.3.0"
    role: "openebs-lvm"
    app: "openebs-lvm-controller"
    release: release-name
    component: "openebs-lvm-controller"
    openebs.io/component-name: "openebs-lvm-controller"
subjects:
  - kind: ServiceAccount
    name: openebs-lvm-controller-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-lvm-snapshotter-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-lvm-driver-registrar-binding
  labels:
    chart: lvm-localpv-1.3.0
    heritage: Helm
    openebs.io/version: "1.3.0"
    role: "openebs-lvm"
    name: "openebs-lvm-node"
    release: release-name
    openebs.io/component-name: "openebs-lvm-node"
subjects:
  - kind: ServiceAccount
    name: openebs-lvm-node-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-lvm-driver-registrar-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/jaeger-operator/templates/role-binding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-jaeger-operator
  namespace: prod
  labels:
    
    app.kubernetes.io/name: jaeger-operator
    app.kubernetes.io/instance: release-name
subjects:
- kind: ServiceAccount
  namespace: prod
  name: release-name-jaeger-operator
roleRef:
  kind: ClusterRole
  name: release-name-jaeger-operator
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/localpv-provisioner/charts/openebs-ndm/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-openebs-ndm
subjects:
  - kind: ServiceAccount
    name: openebs-ndm
    namespace: prod
  - kind: User
    name: system:serviceaccount:default:default
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: release-name-openebs-ndm
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/localpv-provisioner/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-localpv-provisioner
  labels:
    chart: localpv-provisioner-3.4.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    app: localpv-provisioner
    release: release-name
    component: "localpv-provisioner"
    openebs.io/component-name: openebs-localpv-provisioner
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-localpv-provisioner
subjects:
- kind: ServiceAccount
  name: release-name-localpv-provisioner
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/filebeat/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-filebeat-cluster-role-binding
  labels:
    app: "release-name-filebeat"
    chart: "filebeat-7.17.1"
    heritage: "Helm"
    release: "release-name"
roleRef:
  kind: ClusterRole
  name: release-name-filebeat-cluster-role
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: release-name-filebeat
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/fluent-bit/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-fluent-bit-loki-clusterrolebinding
  labels:
    app: fluent-bit-loki
    chart: fluent-bit-2.3.1
    release: release-name
    heritage: Helm
subjects:
  - kind: ServiceAccount
    name: release-name-fluent-bit-loki
    namespace: prod
roleRef:
  kind: ClusterRole
  name: release-name-fluent-bit-loki-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-grafana-clusterrolebinding
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: release-name-grafana
    namespace: prod
roleRef:
  kind: ClusterRole
  name: release-name-grafana-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/charts/kube-state-metrics/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: kube-state-metrics-4.4.3
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.3.0"
  name: release-name-kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-kube-state-metrics
subjects:
- kind: ServiceAccount
  name: release-name-kube-state-metrics
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/alertmanager/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-alertmanager
subjects:
  - kind: ServiceAccount
    name: release-name-prometheus-alertmanager
    namespace: prod
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-prometheus-alertmanager
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/pushgateway/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    component: "pushgateway"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-pushgateway
subjects:
  - kind: ServiceAccount
    name: release-name-prometheus-pushgateway
    namespace: prod
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-prometheus-pushgateway
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/server/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    component: "server"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-server
subjects:
  - kind: ServiceAccount
    name: release-name-prometheus-server
    namespace: prod
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-prometheus-server
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/promtail/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-promtail
  labels:
    helm.sh/chart: promtail-3.11.0
    app.kubernetes.io/name: promtail
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.4.2"
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: release-name-promtail
    namespace: prod
roleRef:
  kind: ClusterRole
  name: release-name-promtail
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/rbac/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-cluster-role-binding
  labels:
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
subjects:
- kind: ServiceAccount
  name: release-name-service-account
  namespace: prod
roleRef:
  kind: ClusterRole
  name: release-name-cluster-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/nfs-provisioner/templates/clusterrolebinding.yaml
# Bind the Service Account with the Role Privileges.
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-nfs-provisioner
  labels:
    chart: nfs-provisioner-0.10.0
    heritage: Helm
    openebs.io/version: "0.10.0"
    app: nfs-provisioner
    release: release-name
    component: nfs-provisioner
    openebs.io/component-name: openebs-nfs-provisioner
roleRef:
  kind: ClusterRole
  name: release-name-nfs-provisioner
  apiGroup: rbac.authorization.k8s.io
subjects:
  - kind: ServiceAccount
    name: release-name-nfs-provisioner
    namespace: prod
---
# Source: my-openebs/charts/openebs/charts/openebs-ndm/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-name-openebs-ndm
subjects:
  - kind: ServiceAccount
    name: openebs-ndm
    namespace: prod
  - kind: User
    name: system:serviceaccount:default:default
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: release-name-openebs-ndm
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-zfs-provisioner-binding
  labels:
    chart: zfs-localpv-2.3.1
    heritage: Helm
    openebs.io/version: "2.3.1"
    role: "openebs-zfs"
    app: "openebs-zfs-controller"
    release: release-name
    component: "openebs-zfs-controller"
    openebs.io/component-name: "openebs-zfs-controller"
subjects:
  - kind: ServiceAccount
    name: openebs-zfs-controller-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-zfs-provisioner-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-zfs-snapshotter-binding
  labels:
    chart: zfs-localpv-2.3.1
    heritage: Helm
    openebs.io/version: "2.3.1"
    role: "openebs-zfs"
    app: "openebs-zfs-controller"
    release: release-name
    component: "openebs-zfs-controller"
    openebs.io/component-name: "openebs-zfs-controller"
subjects:
  - kind: ServiceAccount
    name: openebs-zfs-controller-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-zfs-snapshotter-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/rbac.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: openebs-zfs-driver-registrar-binding
  labels:
    chart: zfs-localpv-2.3.1
    heritage: Helm
    openebs.io/version: "2.3.1"
    role: "openebs-zfs"
    name: "openebs-zfs-node"
    release: release-name
    openebs.io/component-name: "openebs-zfs-node"
subjects:
  - kind: ServiceAccount
    name: openebs-zfs-node-sa
    namespace: prod
roleRef:
  kind: ClusterRole
  name: openebs-zfs-driver-registrar-role
  apiGroup: rbac.authorization.k8s.io
---
# Source: my-openebs/charts/openebs/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-name-openebs
  labels:
    app: openebs
    chart: openebs-3.9.0
    release: release-name
    heritage: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-openebs
subjects:
- kind: ServiceAccount
  name: release-name-openebs
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/filebeat/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-filebeat-role
  labels:
    app: "release-name-filebeat"
rules:
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs: ["get", "create", "update"]
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/fluent-bit/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-fluent-bit-loki
  namespace: prod
  labels:
    app: fluent-bit-loki
    chart: fluent-bit-2.3.1
    heritage: Helm
    release: release-name
rules:
- apiGroups:      ['extensions']
  resources:      ['podsecuritypolicies']
  verbs:          ['use']
  resourceNames:  [release-name-fluent-bit-loki]
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-grafana
  namespace: prod
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:      ['extensions']
  resources:      ['podsecuritypolicies']
  verbs:          ['use']
  resourceNames:  [release-name-grafana]
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/tests/test-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-grafana-test
  namespace: prod
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:      ['policy']
  resources:      ['podsecuritypolicies']
  verbs:          ['use']
  resourceNames:  [release-name-grafana-test]
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/loki/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: release-name-loki
  namespace: prod
  labels:
    app: loki
    chart: loki-2.11.0
    heritage: Helm
    release: release-name
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/filebeat/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-filebeat-role-binding
  labels:
    app: "release-name-filebeat"
    chart: "filebeat-7.17.1"
    heritage: "Helm"
    release: "release-name"
roleRef:
  kind: Role
  name: release-name-filebeat-role
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: release-name-filebeat
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/fluent-bit/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-fluent-bit-loki
  namespace: prod
  labels:
    app: fluent-bit-loki
    chart: fluent-bit-2.3.1
    heritage: Helm
    release: release-name
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-fluent-bit-loki
subjects:
- kind: ServiceAccount
  name: release-name-fluent-bit-loki
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-grafana
  namespace: prod
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-grafana
subjects:
- kind: ServiceAccount
  name: release-name-grafana
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/tests/test-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-grafana-test
  namespace: prod
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-grafana-test
subjects:
- kind: ServiceAccount
  name: release-name-grafana-test
  namespace: prod
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/loki/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-loki
  namespace: prod
  labels:
    app: loki
    chart: loki-2.11.0
    heritage: Helm
    release: release-name
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-loki
subjects:
- kind: ServiceAccount
  name: release-name-loki
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/cvc-operator-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-cstor-cvc-operator-svc
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "cvc-operator"
    release: release-name
    component: "cvc-operator"
    openebs.io/component-name: "cvc-operator-svc"
spec:
  ports:
    - name: api
      port: 5757
      protocol: TCP
      targetPort: 5757
  selector:
    name: cvc-operator
  sessionAffinity: None
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/lvm-node-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-lvm-localpv-node-service
  labels:
    chart: lvm-localpv-1.3.0
    heritage: Helm
    openebs.io/version: "1.3.0"
    role: "openebs-lvm"
    name: "openebs-lvm-node"
    release: release-name
    openebs.io/component-name: "openebs-lvm-node"
spec:
  clusterIP: None
  ports:
    - name: metrics
      port: 9500
      targetPort: 9500
  selector:
    app: openebs-lvm-node
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/etcd/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-etcd-headless
  namespace: "prod"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.6.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: client
      port: 2379
      targetPort: client
    - name: peer
      port: 2380
      targetPort: peer
  selector:
    app.kubernetes.io/name: etcd
    app.kubernetes.io/instance: release-name
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/etcd/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-etcd
  namespace: "prod"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.6.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: "client"
      port: 2379
      targetPort: client
      nodePort: null
    - name: "peer"
      port: 2380
      targetPort: peer
      nodePort: null
  selector:
    app.kubernetes.io/name: etcd
    app.kubernetes.io/instance: release-name
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/jaeger-operator/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-jaeger-operator-metrics
  namespace: prod
  labels:
    
    app.kubernetes.io/name: jaeger-operator
    app.kubernetes.io/instance: release-name
spec:
  ports:
  - name: metrics
    port: 8383
    protocol: TCP
    targetPort: 8383
  selector:
    app.kubernetes.io/name: jaeger-operator
    app.kubernetes.io/instance: release-name
  type: ClusterIP
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-grafana
  namespace: prod
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000

  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/logstash/templates/service-headless.yaml
kind: Service
apiVersion: v1
metadata:
  name: "release-name-logstash-headless"
  labels:
    app: "release-name-logstash"
    chart: "logstash"
    heritage: "Helm"
    release: "release-name"
spec:
  clusterIP: None
  selector:
    app: "release-name-logstash"
  ports:
    - name: http
      port: 9600
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/loki/templates/service-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-loki-headless
  namespace: prod
  labels:
    app: loki
    chart: loki-2.11.0
    release: release-name
    heritage: Helm
    variant: headless
spec:
  clusterIP: None
  ports:
    - port: 3100
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
  selector:
    app: loki
    release: release-name
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/loki/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-loki
  namespace: prod
  labels:
    app: loki
    chart: loki-2.11.0
    release: release-name
    heritage: Helm
  annotations:
    {}
spec:
  type: ClusterIP
  ports:
    - port: 3100
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
  selector:
    app: loki
    release: release-name
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/charts/kube-state-metrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kube-state-metrics
  namespace: prod
  labels:    
    helm.sh/chart: kube-state-metrics-4.4.3
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.3.0"
  annotations:
    prometheus.io/scrape: 'true'
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  
  selector:    
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/alertmanager/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-alertmanager
  namespace: prod
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9093
  selector:
    component: "alertmanager"
    app: prometheus
    release: release-name
  sessionAffinity: None
  type: "ClusterIP"
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/node-exporter/svc.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: "true"
  labels:
    component: "node-exporter"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-node-exporter
  namespace: prod
spec:
  clusterIP: None
  ports:
    - name: metrics
      port: 9100
      protocol: TCP
      targetPort: 9100
  selector:
    component: "node-exporter"
    app: prometheus
    release: release-name
  type: "ClusterIP"
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/pushgateway/service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/probe: pushgateway
  labels:
    component: "pushgateway"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-pushgateway
  namespace: prod
spec:
  ports:
    - name: http
      port: 9091
      protocol: TCP
      targetPort: 9091
  selector:
    component: "pushgateway"
    app: prometheus
    release: release-name
  type: "ClusterIP"
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/server/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    component: "server"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-server
  namespace: prod
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9090
  selector:
    component: "server"
    app: prometheus
    release: release-name
  sessionAffinity: None
  type: "ClusterIP"
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/nats/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-nats
  namespace: prod
  labels:
    helm.sh/chart: nats-0.19.14
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.9.17"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
  - name: client
    port: 4222
    appProtocol: tcp
  - name: cluster
    port: 6222
    appProtocol: tcp
  - name: monitor
    port: 8222
    appProtocol: http
  - name: metrics
    port: 7777
    appProtocol: http
  - name: leafnodes
    port: 7422
    appProtocol: tcp
  - name: gateways
    port: 7522
    appProtocol: tcp
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/agents/core/agent-core-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-agent-core
  labels:
    app: agent-core
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
spec:
  selector:
    app: agent-core
    openebs.io/release: release-name
  ports:
    - name: grpc
      port: 50051
    - name: ha-cluster
      port: 50052
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/apis/api-rest-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-api-rest
  labels:
    app: api-rest
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
spec:
  type: ClusterIP
  selector:
    app: api-rest
    openebs.io/release: release-name
  ports:
    - port: 8080
      name: https
      targetPort: 8080
      protocol: TCP
    - port: 8081
      name: http
      targetPort: 8081
      protocol: TCP
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/metrics/metrics-exporter-pool-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-metrics-exporter-pool
  labels:
    app: metrics-exporter-pool
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
spec:
  ports:
    - name: metrics
      port: 9502
      targetPort: 9502
      protocol: TCP
  selector:
    app: io-engine
    openebs.io/release: release-name
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/obs/stats-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-obs-callhome-stats
  labels:
    app: obs-callhome
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
spec:
  ports:
    - port: 9090
      name: https
      targetPort: 9090
      protocol: TCP
    - port: 9091
      name: http
      targetPort: 9091
      protocol: TCP
  selector:
    app: obs-callhome
    openebs.io/release: release-name
---
# Source: my-openebs/charts/openebs/charts/cstor/charts/openebs-ndm/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-openebs-ndm
  labels:
    chart: openebs-ndm-2.1.0
    heritage: Helm
    openebs.io/version: "2.1.0"
    app: openebs-ndm
    release: release-name
    component: "ndm"
    openebs.io/component-name: "ndm"
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: openebs-ndm
      release: release-name
      component: "ndm"
  template:
    metadata:
      labels:
        chart: openebs-ndm-2.1.0
        heritage: Helm
        openebs.io/version: "2.1.0"
        app: openebs-ndm
        release: release-name
        component: "ndm"
        openebs.io/component-name: "ndm"
        name: openebs-ndm
    spec:
      serviceAccountName: openebs-ndm
      containers:
      - name: openebs-ndm
        image: "openebs/node-disk-manager:2.1.0"
        args:
          - -v=4
          - --feature-gates=GPTBasedUUID
        imagePullPolicy: IfNotPresent
        resources:
            {}
        securityContext:
          privileged: true
        env:
        # namespace in which NDM is installed will be passed to NDM Daemonset
        # as environment variable
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        # pass hostname as env variable using downward API to the NDM container
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # specify the directory where the sparse files need to be created.
        # if not specified, then sparse files will not be created.
        - name: SPARSE_FILE_DIR
          value: "/var/openebs/sparse"
        # Size(bytes) of the sparse file to be created.
        - name: SPARSE_FILE_SIZE
          value: "10737418240"
        # Specify the number of sparse files to be created
        - name: SPARSE_FILE_COUNT
          value: "0"
        # Process name used for matching is limited to the 15 characters
        # present in the pgrep output.
        # So fullname can be used here with pgrep (cmd is < 15 chars).
        livenessProbe:
          exec:
            command:
            - pgrep
            - "ndm"
          initialDelaySeconds: 30
          periodSeconds: 60
        volumeMounts:
        - name: config
          mountPath: /host/node-disk-manager.config
          subPath: node-disk-manager.config
          readOnly: true
        - name: udev
          mountPath: /run/udev
        - name: procmount
          mountPath: /host/proc
          readOnly: true
        - name: devmount
          mountPath: /dev
        - name: basepath
          mountPath: /var/openebs/ndm
        - name: sparsepath
          mountPath: /var/openebs/sparse
      volumes:
      - name: config
        configMap:
          name: release-name-openebs-ndm-config
      - name: udev
        hostPath:
          path: /run/udev
          type: Directory
      # mount /proc (to access mount file of process 1 of host) inside container
      # to read mount-point of disks and partitions
      - name: procmount
        hostPath:
          path: /proc
          type: Directory
      - name: devmount
      # the /dev directory is mounted so that we have access to the devices that
      # are connected at runtime of the pod.
        hostPath:
          path: /dev
          type: Directory
      - name: basepath
        hostPath:
          path: "/var/openebs/ndm"
          type: DirectoryOrCreate
      - name: sparsepath
        hostPath:
          path: /var/openebs/sparse
      # By default the node-disk-manager will be run on all kubernetes nodes
      # If you would like to limit this to only some nodes, say the nodes
      # that have storage attached, you could label those node and use
      # nodeSelector.
      #
      # e.g. label the storage nodes with - "openebs.io/nodegroup"="storage-node"
      # kubectl label node <node-name> "openebs.io/nodegroup"="storage-node"
      #nodeSelector:
      #  "openebs.io/nodegroup": "storage-node"
      hostNetwork: true
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-node.yaml
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: release-name-cstor-csi-node
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-node"
    release: release-name
    component: "openebs-cstor-csi-node"
    openebs.io/component-name: "openebs-cstor-csi-node"
spec:
  selector:
    matchLabels:
      name: "openebs-cstor-csi-node"
      release: release-name
      component: "openebs-cstor-csi-node"
  template:
    metadata:
      labels:
        chart: cstor-3.5.0
        heritage: Helm
        openebs.io/version: "3.5.0"
        name: "openebs-cstor-csi-node"
        release: release-name
        component: "openebs-cstor-csi-node"
        openebs.io/component-name: "openebs-cstor-csi-node"
    spec:
      priorityClassName: release-name-cstor-csi-node-critical
      serviceAccountName: openebs-cstor-csi-node-sa
      hostNetwork: true
      containers:
        - name: csi-node-driver-registrar
          image: "registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0"
          imagePullPolicy: IfNotPresent
          resources:
            {}
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
            - "--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)"
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh", "-c", "rm -rf /registration/cstor.csi.openebs.io /registration/cstor.csi.openebs.io-reg.sock"]
          env:
            - name: ADDRESS
              value: /plugin/csi.sock
            - name: DRIVER_REG_SOCK_PATH
              value: /var/lib/kubelet/plugins/cstor.csi.openebs.io/csi.sock
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: NODE_DRIVER
              value: openebs-cstor-csi
          volumeMounts:
            - name: plugin-dir
              mountPath: /plugin
            - name: registration-dir
              mountPath: /registration
        - name: cstor-csi-plugin
          securityContext:
            privileged: true
            allowPrivilegeEscalation: true
          image: "openebs/cstor-csi-driver:3.5.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--nodeid=$(OPENEBS_NODE_ID)"
            - "--endpoint=$(OPENEBS_CSI_ENDPOINT)"
            - "--url=$(OPENEBS_CSI_API_URL)"
            - "--plugin=$(OPENEBS_NODE_DRIVER)"
          env:
            - name: OPENEBS_NODE_ID
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: OPENEBS_CSI_ENDPOINT
              value: unix:///plugin/csi.sock
            - name: OPENEBS_NODE_DRIVER
              value: node
            - name: OPENEBS_CSI_API_URL
              value: https://openebs.io
              # OpenEBS namespace where the openebs cstor operator components
              # has been installed
            - name: OPENEBS_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
              # Enable/Disable auto-remount feature, when volumes
              # recovers from the read-only state
            - name: REMOUNT
              value: "true"
          volumeMounts:
            - name: plugin-dir
              mountPath: /plugin
            - name: device-dir
              mountPath: /dev
            - name: pods-mount-dir
              mountPath: /var/lib/kubelet/
              # needed so that any mounts setup inside this container are
              # propagated back to the host machine.
              mountPropagation: "Bidirectional"
            - name: host-root
              mountPath: /host
              mountPropagation: "HostToContainer"
            - name: chroot-iscsiadm
              mountPath: /sbin/iscsiadm
              subPath: iscsiadm
      volumes:
        - name: device-dir
          hostPath:
            path: /dev
            type: Directory
        - name: registration-dir
          hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: DirectoryOrCreate
        - name: plugin-dir
          hostPath:
            path: /var/lib/kubelet/plugins/cstor.csi.openebs.io/
            type: DirectoryOrCreate
        - name: pods-mount-dir
          hostPath:
            path: /var/lib/kubelet/
            type: Directory
        - name: chroot-iscsiadm
          configMap:
            defaultMode: 0555
            name: openebs-cstor-csi-iscsiadm
        - name: host-root
          hostPath:
            path: /
            type: Directory
---
# Source: my-openebs/charts/openebs/charts/jiva/charts/localpv-provisioner/charts/openebs-ndm/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-openebs-ndm
  labels:
    chart: openebs-ndm-2.1.0
    heritage: Helm
    openebs.io/version: "2.1.0"
    app: openebs-ndm
    release: release-name
    component: "ndm"
    openebs.io/component-name: "ndm"
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: openebs-ndm
      release: release-name
      component: "ndm"
  template:
    metadata:
      labels:
        chart: openebs-ndm-2.1.0
        heritage: Helm
        openebs.io/version: "2.1.0"
        app: openebs-ndm
        release: release-name
        component: "ndm"
        openebs.io/component-name: "ndm"
        name: openebs-ndm
    spec:
      serviceAccountName: openebs-ndm
      containers:
      - name: openebs-ndm
        image: "openebs/node-disk-manager:2.1.0"
        args:
          - -v=4
          - --feature-gates=GPTBasedUUID
        imagePullPolicy: IfNotPresent
        resources:
            {}
        securityContext:
          privileged: true
        env:
        # namespace in which NDM is installed will be passed to NDM Daemonset
        # as environment variable
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        # pass hostname as env variable using downward API to the NDM container
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # specify the directory where the sparse files need to be created.
        # if not specified, then sparse files will not be created.
        - name: SPARSE_FILE_DIR
          value: "/var/openebs/sparse"
        # Size(bytes) of the sparse file to be created.
        - name: SPARSE_FILE_SIZE
          value: "10737418240"
        # Specify the number of sparse files to be created
        - name: SPARSE_FILE_COUNT
          value: "0"
        # Process name used for matching is limited to the 15 characters
        # present in the pgrep output.
        # So fullname can be used here with pgrep (cmd is < 15 chars).
        livenessProbe:
          exec:
            command:
            - pgrep
            - "ndm"
          initialDelaySeconds: 30
          periodSeconds: 60
        volumeMounts:
        - name: config
          mountPath: /host/node-disk-manager.config
          subPath: node-disk-manager.config
          readOnly: true
        - name: udev
          mountPath: /run/udev
        - name: procmount
          mountPath: /host/proc
          readOnly: true
        - name: devmount
          mountPath: /dev
        - name: basepath
          mountPath: /var/openebs/ndm
        - name: sparsepath
          mountPath: /var/openebs/sparse
      volumes:
      - name: config
        configMap:
          name: release-name-openebs-ndm-config
      - name: udev
        hostPath:
          path: /run/udev
          type: Directory
      # mount /proc (to access mount file of process 1 of host) inside container
      # to read mount-point of disks and partitions
      - name: procmount
        hostPath:
          path: /proc
          type: Directory
      - name: devmount
      # the /dev directory is mounted so that we have access to the devices that
      # are connected at runtime of the pod.
        hostPath:
          path: /dev
          type: Directory
      - name: basepath
        hostPath:
          path: "/var/openebs/ndm"
          type: DirectoryOrCreate
      - name: sparsepath
        hostPath:
          path: /var/openebs/sparse
      # By default the node-disk-manager will be run on all kubernetes nodes
      # If you would like to limit this to only some nodes, say the nodes
      # that have storage attached, you could label those node and use
      # nodeSelector.
      #
      # e.g. label the storage nodes with - "openebs.io/nodegroup"="storage-node"
      # kubectl label node <node-name> "openebs.io/nodegroup"="storage-node"
      #nodeSelector:
      #  "openebs.io/nodegroup": "storage-node"
      hostNetwork: true
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-node.yaml
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: release-name-jiva-csi-node
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-node"
    release: release-name
    component: "openebs-jiva-csi-node"
    openebs.io/component-name: "openebs-jiva-csi-node"
spec:
  selector:
    matchLabels:
      name: "openebs-jiva-csi-node"
      release: release-name
      component: "openebs-jiva-csi-node"
  template:
    metadata:
      labels:
        chart: jiva-3.5.1
        heritage: Helm
        openebs.io/version: "3.4.0"
        name: "openebs-jiva-csi-node"
        release: release-name
        component: "openebs-jiva-csi-node"
        openebs.io/component-name: "openebs-jiva-csi-node"
    spec:
      priorityClassName: release-name-jiva-csi-node-critical
      serviceAccountName: openebs-jiva-csi-node-sa
      hostNetwork: true
      containers:
        - name: csi-node-driver-registrar
          image: "registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0"
          imagePullPolicy: IfNotPresent
          resources:
            {}
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
            - "--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)"
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh", "-c", "rm -rf /registration/jiva.csi.openebs.io /registration/jiva.csi.openebs.io-reg.sock"]
          env:
            - name: ADDRESS
              value: /plugin/csi.sock
            - name: DRIVER_REG_SOCK_PATH
              value: /var/lib/kubelet/plugins/jiva.csi.openebs.io/csi.sock
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: NODE_DRIVER
              value: openebs-jiva-csi
          volumeMounts:
            - name: plugin-dir
              mountPath: /plugin
            - name: registration-dir
              mountPath: /registration
        - name: jiva-csi-plugin
          securityContext:
            privileged: true
            allowPrivilegeEscalation: true
          image: "openebs/jiva-csi:3.4.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--name=jiva.csi.openebs.io"
            - "--nodeid=$(OPENEBS_NODE_ID)"
            - "--endpoint=$(OPENEBS_CSI_ENDPOINT)"
            - "--plugin=$(OPENEBS_NODE_DRIVER)"
            # enableiscsidebug is used to enable debug logs for iscsi operations
            - "--enableiscsidebug=true"
            # logging level for klog library used in k8s packages
            #- "--v=5"
            # retrycount is the max number of retries per nodeStaging rpc
            # request on a timeout of 5 sec
            # This count has been set to 20 for sanity test cases as it takes
            # time in minikube
            - "--retrycount=20"
            # metricsBindAddress is the TCP address that the controller should bind to
            # for serving prometheus metrics. By default the address is set to localhost:9505.
            # The address can be configured to any desired address.
            # Remove the flag to disable prometheus metrics.
            - "--metricsBindAddress=:9505"
          env:
            - name: OPENEBS_NODE_ID
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: OPENEBS_CSI_ENDPOINT
              value: unix:///plugin/csi.sock
            - name: OPENEBS_NODE_DRIVER
              value: node
            - name: OPENEBS_CSI_API_URL
              value: https://openebs.io
              # OpenEBS namespace where the openebs jiva operator components
              # has been installed
            - name: OPENEBS_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
              # Enable/Disable auto-remount feature, when volumes
              # recovers form the read-only state
            - name: REMOUNT
              value: "true"
          volumeMounts:
            - name: plugin-dir
              mountPath: /plugin
            - name: device-dir
              mountPath: /dev
            - name: pods-mount-dir
              mountPath: /var/lib/kubelet/
              # needed so that any mounts setup inside this container are
              # propagated back to the host machine.
              mountPropagation: "Bidirectional"
            - name: host-root
              mountPath: /host
              mountPropagation: "HostToContainer"
            - name: chroot-iscsiadm
              mountPath: /sbin/iscsiadm
              subPath: iscsiadm
        - name: liveness-probe
          image: "registry.k8s.io/sig-storage/livenessprobe:v2.10.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--csi-address=/plugin/csi.sock"
          volumeMounts:
          - mountPath: /plugin
            name: plugin-dir
      volumes:
        - name: device-dir
          hostPath:
            path: /dev
            type: Directory
        - name: registration-dir
          hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: DirectoryOrCreate
        - name: plugin-dir
          hostPath:
            path: /var/lib/kubelet/plugins/jiva.csi.openebs.io/
            type: DirectoryOrCreate
        - name: pods-mount-dir
          hostPath:
            path: /var/lib/kubelet/
            type: Directory
        - name: chroot-iscsiadm
          configMap:
            defaultMode: 0555
            name: openebs-jiva-csi-iscsiadm
        - name: host-root
          hostPath:
            path: /
            type: Directory
---
# Source: my-openebs/charts/openebs/charts/localpv-provisioner/charts/openebs-ndm/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-openebs-ndm
  labels:
    chart: openebs-ndm-2.1.0
    heritage: Helm
    openebs.io/version: "2.1.0"
    app: openebs-ndm
    release: release-name
    component: "ndm"
    openebs.io/component-name: "ndm"
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: openebs-ndm
      release: release-name
      component: "ndm"
  template:
    metadata:
      labels:
        chart: openebs-ndm-2.1.0
        heritage: Helm
        openebs.io/version: "2.1.0"
        app: openebs-ndm
        release: release-name
        component: "ndm"
        openebs.io/component-name: "ndm"
        name: openebs-ndm
    spec:
      serviceAccountName: openebs-ndm
      containers:
      - name: openebs-ndm
        image: "openebs/node-disk-manager:2.1.0"
        args:
          - -v=4
          - --feature-gates=GPTBasedUUID
        imagePullPolicy: IfNotPresent
        resources:
            {}
        securityContext:
          privileged: true
        env:
        # namespace in which NDM is installed will be passed to NDM Daemonset
        # as environment variable
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        # pass hostname as env variable using downward API to the NDM container
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # specify the directory where the sparse files need to be created.
        # if not specified, then sparse files will not be created.
        - name: SPARSE_FILE_DIR
          value: "/var/openebs/sparse"
        # Size(bytes) of the sparse file to be created.
        - name: SPARSE_FILE_SIZE
          value: "10737418240"
        # Specify the number of sparse files to be created
        - name: SPARSE_FILE_COUNT
          value: "0"
        # Process name used for matching is limited to the 15 characters
        # present in the pgrep output.
        # So fullname can be used here with pgrep (cmd is < 15 chars).
        livenessProbe:
          exec:
            command:
            - pgrep
            - "ndm"
          initialDelaySeconds: 30
          periodSeconds: 60
        volumeMounts:
        - name: config
          mountPath: /host/node-disk-manager.config
          subPath: node-disk-manager.config
          readOnly: true
        - name: udev
          mountPath: /run/udev
        - name: procmount
          mountPath: /host/proc
          readOnly: true
        - name: devmount
          mountPath: /dev
        - name: basepath
          mountPath: /var/openebs/ndm
        - name: sparsepath
          mountPath: /var/openebs/sparse
      volumes:
      - name: config
        configMap:
          name: release-name-openebs-ndm-config
      - name: udev
        hostPath:
          path: /run/udev
          type: Directory
      # mount /proc (to access mount file of process 1 of host) inside container
      # to read mount-point of disks and partitions
      - name: procmount
        hostPath:
          path: /proc
          type: Directory
      - name: devmount
      # the /dev directory is mounted so that we have access to the devices that
      # are connected at runtime of the pod.
        hostPath:
          path: /dev
          type: Directory
      - name: basepath
        hostPath:
          path: "/var/openebs/ndm"
          type: DirectoryOrCreate
      - name: sparsepath
        hostPath:
          path: /var/openebs/sparse
      # By default the node-disk-manager will be run on all kubernetes nodes
      # If you would like to limit this to only some nodes, say the nodes
      # that have storage attached, you could label those node and use
      # nodeSelector.
      #
      # e.g. label the storage nodes with - "openebs.io/nodegroup"="storage-node"
      # kubectl label node <node-name> "openebs.io/nodegroup"="storage-node"
      #nodeSelector:
      #  "openebs.io/nodegroup": "storage-node"
      hostNetwork: true
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/lvm-node.yaml
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: release-name-lvm-localpv-node
  labels:
    chart: lvm-localpv-1.3.0
    heritage: Helm
    openebs.io/version: "1.3.0"
    role: "openebs-lvm"
    name: "openebs-lvm-node"
    release: release-name
    openebs.io/component-name: "openebs-lvm-node"
spec:
  selector:
    matchLabels:
      name: "openebs-lvm-node"
      release: release-name
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 100%
    type: RollingUpdate
  template:
    metadata:
      labels:
        chart: lvm-localpv-1.3.0
        heritage: Helm
        openebs.io/version: "1.3.0"
        role: "openebs-lvm"
        name: "openebs-lvm-node"
        release: release-name
        openebs.io/component-name: "openebs-lvm-node"
        
        app: openebs-lvm-node
    spec:
      priorityClassName: release-name-lvm-localpv-csi-node-critical
      serviceAccountName: openebs-lvm-node-sa
      hostNetwork: true
      containers:
        - name: csi-node-driver-registrar
          image: "registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
            - "--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)"
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh", "-c", "rm -rf /registration/lvm-localpv /registration/lvm-localpv-reg.sock"]
          env:
            - name: ADDRESS
              value: /plugin/csi.sock
            - name: DRIVER_REG_SOCK_PATH
              value: /var/lib/kubelet/plugins/lvm-localpv/csi.sock
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: NODE_DRIVER
              value: openebs-lvm
          volumeMounts:
            - name: plugin-dir
              mountPath: /plugin
            - name: registration-dir
              mountPath: /registration
          resources:
            {}
        - name: openebs-lvm-plugin
          securityContext:
            privileged: true
            allowPrivilegeEscalation: true
          image: "openebs/lvm-driver:1.3.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--nodeid=$(OPENEBS_NODE_ID)"
            - "--endpoint=$(OPENEBS_CSI_ENDPOINT)"
            - "--plugin=$(OPENEBS_NODE_DRIVER)"
            - "--kube-api-qps=0"
            - "--kube-api-burst=0"
            - "--listen-address=$(METRICS_LISTEN_ADDRESS)"
          env:
            - name: OPENEBS_NODE_ID
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: OPENEBS_CSI_ENDPOINT
              value: unix:///plugin/csi.sock
            - name: OPENEBS_NODE_DRIVER
              value: agent
            - name: LVM_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: METRICS_LISTEN_ADDRESS
              value: :9500
            - name: ALLOWED_TOPOLOGIES
              value: kubernetes.io/hostname,
          volumeMounts:
            - name: plugin-dir
              mountPath: /plugin
            - name: device-dir
              mountPath: /dev
            - name: pods-mount-dir
              mountPath: /var/lib/kubelet/
              # needed so that any mounts setup inside this container are
              # propagated back to the host machine.
              mountPropagation: "Bidirectional"
          resources:
            {}
      volumes:
        - name: device-dir
          hostPath:
            path: /dev
            type: Directory
        - name: registration-dir
          hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: DirectoryOrCreate
        - name: plugin-dir
          hostPath:
            path: /var/lib/kubelet/plugins/lvm-localpv/
            type: DirectoryOrCreate
        - name: pods-mount-dir
          hostPath:
            path: /var/lib/kubelet/
            type: Directory
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/localpv-provisioner/charts/openebs-ndm/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-openebs-ndm
  labels:
    chart: openebs-ndm-2.1.0
    heritage: Helm
    openebs.io/version: "2.1.0"
    app: openebs-ndm
    release: release-name
    component: "ndm"
    openebs.io/component-name: "ndm"
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: openebs-ndm
      release: release-name
      component: "ndm"
  template:
    metadata:
      labels:
        chart: openebs-ndm-2.1.0
        heritage: Helm
        openebs.io/version: "2.1.0"
        app: openebs-ndm
        release: release-name
        component: "ndm"
        openebs.io/component-name: "ndm"
        name: openebs-ndm
    spec:
      serviceAccountName: openebs-ndm
      containers:
      - name: openebs-ndm
        image: "openebs/node-disk-manager:2.1.0"
        args:
          - -v=4
          - --feature-gates=GPTBasedUUID
        imagePullPolicy: IfNotPresent
        resources:
            {}
        securityContext:
          privileged: true
        env:
        # namespace in which NDM is installed will be passed to NDM Daemonset
        # as environment variable
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        # pass hostname as env variable using downward API to the NDM container
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # specify the directory where the sparse files need to be created.
        # if not specified, then sparse files will not be created.
        - name: SPARSE_FILE_DIR
          value: "/var/openebs/sparse"
        # Size(bytes) of the sparse file to be created.
        - name: SPARSE_FILE_SIZE
          value: "10737418240"
        # Specify the number of sparse files to be created
        - name: SPARSE_FILE_COUNT
          value: "0"
        # Process name used for matching is limited to the 15 characters
        # present in the pgrep output.
        # So fullname can be used here with pgrep (cmd is < 15 chars).
        livenessProbe:
          exec:
            command:
            - pgrep
            - "ndm"
          initialDelaySeconds: 30
          periodSeconds: 60
        volumeMounts:
        - name: config
          mountPath: /host/node-disk-manager.config
          subPath: node-disk-manager.config
          readOnly: true
        - name: udev
          mountPath: /run/udev
        - name: procmount
          mountPath: /host/proc
          readOnly: true
        - name: devmount
          mountPath: /dev
        - name: basepath
          mountPath: /var/openebs/ndm
        - name: sparsepath
          mountPath: /var/openebs/sparse
      volumes:
      - name: config
        configMap:
          name: release-name-openebs-ndm-config
      - name: udev
        hostPath:
          path: /run/udev
          type: Directory
      # mount /proc (to access mount file of process 1 of host) inside container
      # to read mount-point of disks and partitions
      - name: procmount
        hostPath:
          path: /proc
          type: Directory
      - name: devmount
      # the /dev directory is mounted so that we have access to the devices that
      # are connected at runtime of the pod.
        hostPath:
          path: /dev
          type: Directory
      - name: basepath
        hostPath:
          path: "/var/openebs/ndm"
          type: DirectoryOrCreate
      - name: sparsepath
        hostPath:
          path: /var/openebs/sparse
      # By default the node-disk-manager will be run on all kubernetes nodes
      # If you would like to limit this to only some nodes, say the nodes
      # that have storage attached, you could label those node and use
      # nodeSelector.
      #
      # e.g. label the storage nodes with - "openebs.io/nodegroup"="storage-node"
      # kubectl label node <node-name> "openebs.io/nodegroup"="storage-node"
      #nodeSelector:
      #  "openebs.io/nodegroup": "storage-node"
      hostNetwork: true
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/filebeat/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-filebeat
  labels:
    app: "release-name-filebeat"
    chart: "filebeat-7.17.1"
    heritage: "Helm"
    release: "release-name"
spec:
  selector:
    matchLabels:
      app: "release-name-filebeat"
      release: "release-name"
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      annotations:
        
        configChecksum: f20a4ba769b5a1025363ef0770dda260bbc417de1a7009f1b7f89918bffd5e3
      name: "release-name-filebeat"
      labels:
        app: "release-name-filebeat"
        chart: "filebeat-7.17.1"
        heritage: "Helm"
        release: "release-name"
    spec:
      tolerations: 
        []
      nodeSelector: 
        {}
      affinity: 
        {}
      serviceAccountName: release-name-filebeat
      terminationGracePeriodSeconds: 30
      volumes:
      - name: filebeat-config
        configMap:
          defaultMode: 0600
          name: release-name-filebeat-config
      - name: data
        hostPath:
          path: /var/lib/release-name-filebeat-prod-data
          type: DirectoryOrCreate
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: varlog
        hostPath:
          path: /var/log
      - name: varrundockersock
        hostPath:
          path: /var/run/docker.sock
      containers:
      - name: "filebeat"
        image: "docker.elastic.co/beats/filebeat:7.17.1"
        imagePullPolicy: "IfNotPresent"
        args:
        - "-e"
        - "-E"
        - "http.enabled=true"
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - |
              #!/usr/bin/env bash -e
              curl --fail 127.0.0.1:5066
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - |
              #!/usr/bin/env bash -e
              filebeat test output
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
        resources:
          limits:
            cpu: 1000m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        envFrom: 
          []
        securityContext: 
          privileged: false
          runAsUser: 0
        volumeMounts:
        - name: filebeat-config
          mountPath: /usr/share/filebeat/filebeat.yml
          readOnly: true
          subPath: filebeat.yml
        
        - name: data
          mountPath: /usr/share/filebeat/data
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: varlog
          mountPath: /var/log
          readOnly: true
        # Necessary when using autodiscovery; avoid mounting it otherwise
        # See: https://www.elastic.co/guide/en/beats/filebeat/7.17/configuration-autodiscover.html
        - name: varrundockersock
          mountPath: /var/run/docker.sock
          readOnly: true
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/fluent-bit/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-fluent-bit-loki
  namespace: prod
  labels:
    app: fluent-bit-loki
    chart: fluent-bit-2.3.1
    release: release-name
    heritage: Helm
  annotations:
    {}
spec:
  selector:
    matchLabels:
      app: fluent-bit-loki
      release: release-name
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: fluent-bit-loki
        release: release-name
      annotations:
        checksum/config: 9c25fee09a4d020ea03966748dbe33a9cebe564d69de10a062292d74a069b762
        prometheus.io/path: /api/v1/metrics/prometheus
        prometheus.io/port: "2020"
        prometheus.io/scrape: "true"
    spec:
      serviceAccountName: release-name-fluent-bit-loki
      containers:
        - name: fluent-bit-loki
          image: "grafana/fluent-bit-plugin-loki:2.1.0-amd64"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: config
              mountPath: /fluent-bit/etc
            - name: run
              mountPath: /run/fluent-bit
            - mountPath: /var/log
              name: varlog
            - mountPath: /var/lib/docker/containers
              name: varlibdockercontainers
              readOnly: true
          ports:
            - containerPort: 2020
              name: http-metrics
          resources:
            limits:
              memory: 100Mi
            requests:
              cpu: 100m
              memory: 100Mi
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
      terminationGracePeriodSeconds: 10
      volumes:
        - name: config
          configMap:
            name: release-name-fluent-bit-loki
        - name: run
          hostPath:
            path: /run/fluent-bit
        - hostPath:
            path: /var/log
          name: varlog
        - hostPath:
            path: /var/lib/docker/containers
          name: varlibdockercontainers
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/node-exporter/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    component: "node-exporter"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-node-exporter
  namespace: prod
spec:
  selector:
    matchLabels:
      component: "node-exporter"
      app: prometheus
      release: release-name
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        component: "node-exporter"
        app: prometheus
        release: release-name
        chart: prometheus-15.5.4
        heritage: Helm
    spec:
      serviceAccountName: release-name-prometheus-node-exporter
      containers:
        - name: prometheus-node-exporter
          image: "quay.io/prometheus/node-exporter:v1.3.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --path.procfs=/host/proc
            - --path.sysfs=/host/sys
            - --path.rootfs=/host/root
            - --web.listen-address=:9100
          ports:
            - name: metrics
              containerPort: 9100
              hostPort: 9100
          resources:
            {}
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly:  true
            - name: sys
              mountPath: /host/sys
              readOnly: true
            - name: root
              mountPath: /host/root
              mountPropagation: HostToContainer
              readOnly: true
      hostNetwork: true
      hostPID: true
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
        - name: root
          hostPath:
            path: /
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/promtail/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-promtail
  namespace: prod
  labels:
    helm.sh/chart: promtail-3.11.0
    app.kubernetes.io/name: promtail
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.4.2"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: promtail
      app.kubernetes.io/instance: release-name
  updateStrategy:
    {}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: promtail
        app.kubernetes.io/instance: release-name
      annotations:
        checksum/config: 43f356b96c0311bfd0782e99927b91a48ee0efa0201607507d1402eea4884491
    spec:
      serviceAccountName: release-name-promtail
      securityContext:
        runAsGroup: 0
        runAsUser: 0
      containers:
        - name: promtail
          image: "docker.io/grafana/promtail:2.4.2"
          imagePullPolicy: IfNotPresent
          args:
            - "-config.file=/etc/promtail/promtail.yaml"
          volumeMounts:
            - name: config
              mountPath: /etc/promtail
            - name: run
              mountPath: /run/promtail
            - mountPath: /var/lib/docker/containers
              name: containers
              readOnly: true
            - mountPath: /var/log/pods
              name: pods
              readOnly: true
          env:
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - name: http-metrics
              containerPort: 3101
              protocol: TCP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          readinessProbe:
            failureThreshold: 5
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
      volumes:
        - name: config
          secret:
            secretName: release-name-promtail
        - name: run
          hostPath:
            path: /run/promtail
        - hostPath:
            path: /var/lib/docker/containers
          name: containers
        - hostPath:
            path: /var/log/pods
          name: pods
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/agents/ha/ha-node-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-agent-ha-node
  labels:
    app: agent-ha-node
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
spec:
  selector:
    matchLabels:
      app: agent-ha-node
      openebs.io/release: release-name
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  minReadySeconds: 10
  template:
    metadata:
      labels:
        app: agent-ha-node
        openebs.io/release: release-name
        openebs.io/version: 2.4.0
        openebs.io/logging: "true"
    spec:
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      initContainers:
        - command:
          - sh
          - -c
          - trap "exit 1" TERM; until nc -vzw 5 release-name-agent-core 50052; do date;
            echo "Waiting for agent-cluster-grpc services..."; sleep 1; done;
          image: busybox:latest
          name: agent-cluster-grpc-probe
      imagePullSecrets:
      nodeSelector:
        kubernetes.io/arch: amd64
      containers:
      - name: agent-ha-node
        image: "docker.io/openebs/mayastor-agent-ha-node:v2.4.0"
        imagePullPolicy: IfNotPresent
        securityContext:
          privileged: true
        env:
        - name: RUST_LOG
          value: info
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: RUST_BACKTRACE
          value: "1"
        args:
        - "--node-name=$(MY_NODE_NAME)"
        - "--csi-socket=/csi/csi.sock"
        - "--grpc-endpoint=$(MY_POD_IP):50053"
        - "--cluster-agent=https://release-name-agent-core:50052"
        volumeMounts:
        - name: device
          mountPath: /dev
        - name: sys
          mountPath: /sys
        - name: run-udev
          mountPath: /run/udev
        - name: plugin-dir
          mountPath: /csi
        resources:
          limits:
            cpu: "100m"
            memory: "64Mi"
          requests:
            cpu: "100m"
            memory: "64Mi"
        ports:
          - containerPort: 50053
            protocol: TCP
            name: ha-node
      volumes:
      - name: device
        hostPath:
          path: /dev
          type: Directory
      - name: sys
        hostPath:
          path: /sys
          type: Directory
      - name: run-udev
        hostPath:
          path: /run/udev
          type: Directory
      - name: plugin-dir
        hostPath:
          path: /var/lib/kubelet/plugins/io.openebs.mayastor/
          type: DirectoryOrCreate
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/csi/csi-node-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-csi-node
  labels:
    app: csi-node
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
    openebs.io/csi-node: mayastor
spec:
  selector:
    matchLabels:
      app: csi-node
      openebs.io/release: release-name
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  minReadySeconds: 10
  template:
    metadata:
      labels:
        app: csi-node
        openebs.io/release: release-name
        openebs.io/version: 2.4.0
        openebs.io/logging: "true"
    spec:
      serviceAccount: release-name-service-account
      hostNetwork: true
      imagePullSecrets:
      nodeSelector:
        kubernetes.io/arch: amd64
      # NOTE: Each container must have mem/cpu limits defined in order to
      # belong to Guaranteed QoS class, hence can never get evicted in case of
      # pressure unless they exceed those limits. limits and requests must be
      # the same.
      containers:
      - name: csi-node
        image: "docker.io/openebs/mayastor-csi-node:v2.4.0"
        imagePullPolicy: IfNotPresent
        # we need privileged because we mount filesystems and use mknod
        securityContext:
          privileged: true
        env:
        - name: RUST_LOG
          value: info
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: RUST_BACKTRACE
          value: "1"
        args:
        - "--csi-socket=/csi/csi.sock"
        - "--node-name=$(MY_NODE_NAME)"
        - "--grpc-endpoint=$(MY_POD_IP):10199"
        - "--nvme-core-io-timeout=30"
        - "--nvme-ctrl-loss-tmo=1980"
        - "--nvme-nr-io-queues=2"
        - "--node-selector=openebs.io/csi-node=mayastor"
        command:
        - csi-node
        volumeMounts:
        - name: device
          mountPath: /dev
        - name: sys
          mountPath: /sys
        - name: run-udev
          mountPath: /run/udev
        - name: plugin-dir
          mountPath: /csi
        - name: kubelet-dir
          mountPath: /var/lib/kubelet
          mountPropagation: "Bidirectional"
        resources:
          limits:
            cpu: "100m"
            memory: "128Mi"
          requests:
            cpu: "100m"
            memory: "64Mi"
      - name: csi-driver-registrar
        image: "registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0"
        imagePullPolicy: IfNotPresent
        args:
        - "--csi-address=/csi/csi.sock"
        - "--kubelet-registration-path=/var/lib/kubelet/plugins/io.openebs.mayastor/csi.sock"
        volumeMounts:
        - name: plugin-dir
          mountPath: /csi
        - name: registration-dir
          mountPath: /registration
        resources:
          limits:
            cpu: "100m"
            memory: "50Mi"
          requests:
            cpu: "100m"
            memory: "50Mi"
        # Mayastor node plugin gRPC server
        ports:
        - containerPort: 10199
          protocol: TCP
          name: mayastor-node
      volumes:
      - name: device
        hostPath:
          path: /dev
          type: Directory
      - name: sys
        hostPath:
          path: /sys
          type: Directory
      - name: run-udev
        hostPath:
          path: /run/udev
          type: Directory
      - name: registration-dir
        hostPath:
          path: /var/lib/kubelet/plugins_registry/
          type: Directory
      - name: plugin-dir
        hostPath:
          path: /var/lib/kubelet/plugins/io.openebs.mayastor/
          type: DirectoryOrCreate
      - name: kubelet-dir
        hostPath:
          path: /var/lib/kubelet
          type: Directory
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/io/io-engine-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-io-engine
  labels:
    app: io-engine
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
spec:
  selector:
    matchLabels:
      app: io-engine
      openebs.io/release: release-name
  updateStrategy:
    type: OnDelete
  minReadySeconds: 10
  template:
    metadata:
      labels:
        app: io-engine
        openebs.io/release: release-name
        openebs.io/version: 2.4.0
        openebs.io/logging: "true"
    spec:
      imagePullSecrets:
      hostNetwork: true
      # To resolve services in the namespace
      dnsPolicy: ClusterFirstWithHostNet
      nodeSelector:
        kubernetes.io/arch: amd64
        openebs.io/engine: mayastor
      initContainers:
        - command:
          - sh
          - -c
          - trap "exit 1" TERM; until nc -vzw 5 release-name-agent-core 50051; do date;
            echo "Waiting for agent-core-grpc services..."; sleep 1; done;
          image: busybox:latest
          name: agent-core-grpc-probe
        - command:
          - sh
          - -c
          - trap "exit 1" TERM; until nc -vzw 5 release-name-etcd 2379;
            do date; echo "Waiting for etcd..."; sleep 1; done;
          image: busybox:latest
          name: etcd-probe
      containers:
      - name: metrics-exporter-pool
        image: "docker.io/openebs/mayastor-metrics-exporter-pool:v2.4.0"
        imagePullPolicy: IfNotPresent
        env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        args:
        - "-p5m"
        - "--api-versions=v1"
        command:
        - metrics-exporter-pool
        ports:
          - containerPort: 9502
            protocol: TCP
            name: metrics
      - name: io-engine
        image: "docker.io/openebs/mayastor-io-engine:v2.4.0"
        imagePullPolicy: IfNotPresent
        env:
        - name: RUST_LOG
          value: info
        - name: NVME_QPAIR_CONNECT_ASYNC
          value: "true"
        - name: NVMF_TCP_MAX_QUEUE_DEPTH
          value: "32"
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: NEXUS_NVMF_ANA_ENABLE
          value: "1"
        - name: NEXUS_NVMF_RESV_ENABLE
          value: "1"
        args:
        # The -l argument accepts cpu-list. Indexing starts at zero.
        # For example -l 1,2,10-20 means use core 1, 2, 10 to 20.
        # Note: Ensure that the CPU resources are updated accordingly.
        #       If you use 2 CPUs, the CPU: field should also read 2.
        - "-g$(MY_POD_IP)"
        - "-N$(MY_NODE_NAME)"
        - "-Rhttps://release-name-agent-core:50051"
        - "-y/var/local/io-engine/config.yaml"
        - "-l1,2"
        - "-p=release-name-etcd:2379"
        - "--ptpl-dir=/var/local/io-engine/ptpl/"
        - "--api-versions=v1"
        - "--tgt-crdt=30"
        command:
        - io-engine
        securityContext:
          privileged: true
        volumeMounts:
        - name: device
          mountPath: /dev
        - name: udev
          mountPath: /run/udev
        - name: dshm
          mountPath: /dev/shm
        - name: configlocation
          mountPath: /var/local/io-engine/
        - name: hugepage
          mountPath: /dev/hugepages
        resources:
          limits:
            cpu: "2"
            memory: "1Gi"
            hugepages-2Mi: "2Gi"
          requests:
            cpu: "2"
            memory: "1Gi"
            hugepages-2Mi: "2Gi"
        ports:
        - containerPort: 10124
          protocol: TCP
          name: io-engine
      volumes:
      - name: device
        hostPath:
          path: /dev
          type: Directory
      - name: udev
        hostPath:
          path: /run/udev
          type: Directory
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: "1Gi"
      - name: hugepage
        emptyDir:
          medium: HugePages
      - name: configlocation
        hostPath:
          path: /var/local/io-engine/
          type: DirectoryOrCreate
---
# Source: my-openebs/charts/openebs/charts/openebs-ndm/templates/daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-openebs-ndm
  labels:
    chart: openebs-ndm-2.1.0
    heritage: Helm
    openebs.io/version: "2.1.0"
    app: openebs-ndm
    release: release-name
    component: "ndm"
    openebs.io/component-name: "ndm"
spec:
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: openebs-ndm
      release: release-name
      component: "ndm"
  template:
    metadata:
      labels:
        chart: openebs-ndm-2.1.0
        heritage: Helm
        openebs.io/version: "2.1.0"
        app: openebs-ndm
        release: release-name
        component: "ndm"
        openebs.io/component-name: "ndm"
        name: openebs-ndm
    spec:
      serviceAccountName: openebs-ndm
      containers:
      - name: openebs-ndm
        image: "openebs/node-disk-manager:2.1.0"
        args:
          - -v=4
          - --feature-gates=GPTBasedUUID
        imagePullPolicy: IfNotPresent
        resources:
            {}
        securityContext:
          privileged: true
        env:
        # namespace in which NDM is installed will be passed to NDM Daemonset
        # as environment variable
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        # pass hostname as env variable using downward API to the NDM container
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # specify the directory where the sparse files need to be created.
        # if not specified, then sparse files will not be created.
        - name: SPARSE_FILE_DIR
          value: "/var/openebs/sparse"
        # Size(bytes) of the sparse file to be created.
        - name: SPARSE_FILE_SIZE
          value: "10737418240"
        # Specify the number of sparse files to be created
        - name: SPARSE_FILE_COUNT
          value: "0"
        # Process name used for matching is limited to the 15 characters
        # present in the pgrep output.
        # So fullname can be used here with pgrep (cmd is < 15 chars).
        livenessProbe:
          exec:
            command:
            - pgrep
            - "ndm"
          initialDelaySeconds: 30
          periodSeconds: 60
        volumeMounts:
        - name: config
          mountPath: /host/node-disk-manager.config
          subPath: node-disk-manager.config
          readOnly: true
        - name: udev
          mountPath: /run/udev
        - name: procmount
          mountPath: /host/proc
          readOnly: true
        - name: devmount
          mountPath: /dev
        - name: basepath
          mountPath: /var/openebs/ndm
        - name: sparsepath
          mountPath: /var/openebs/sparse
      volumes:
      - name: config
        configMap:
          name: release-name-openebs-ndm-config
      - name: udev
        hostPath:
          path: /run/udev
          type: Directory
      # mount /proc (to access mount file of process 1 of host) inside container
      # to read mount-point of disks and partitions
      - name: procmount
        hostPath:
          path: /proc
          type: Directory
      - name: devmount
      # the /dev directory is mounted so that we have access to the devices that
      # are connected at runtime of the pod.
        hostPath:
          path: /dev
          type: Directory
      - name: basepath
        hostPath:
          path: "/var/openebs/ndm"
          type: DirectoryOrCreate
      - name: sparsepath
        hostPath:
          path: /var/openebs/sparse
      # By default the node-disk-manager will be run on all kubernetes nodes
      # If you would like to limit this to only some nodes, say the nodes
      # that have storage attached, you could label those node and use
      # nodeSelector.
      #
      # e.g. label the storage nodes with - "openebs.io/nodegroup"="storage-node"
      # kubectl label node <node-name> "openebs.io/nodegroup"="storage-node"
      #nodeSelector:
      #  "openebs.io/nodegroup": "storage-node"
      hostNetwork: true
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/zfs-node.yaml
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: release-name-zfs-localpv-node
  namespace: prod
  labels:
    chart: zfs-localpv-2.3.1
    heritage: Helm
    openebs.io/version: "2.3.1"
    role: "openebs-zfs"
    name: "openebs-zfs-node"
    release: release-name
    openebs.io/component-name: "openebs-zfs-node"
spec:
  selector:
    matchLabels:
      name: "openebs-zfs-node"
      release: release-name
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 100%
    type: RollingUpdate
  template:
    metadata:
      labels:
        chart: zfs-localpv-2.3.1
        heritage: Helm
        openebs.io/version: "2.3.1"
        role: "openebs-zfs"
        name: "openebs-zfs-node"
        release: release-name
        openebs.io/component-name: "openebs-zfs-node"
    spec:
      priorityClassName: release-name-zfs-csi-node-critical
      serviceAccountName: openebs-zfs-node-sa
      hostNetwork: true
      containers:
        - name: csi-node-driver-registrar
          image: "registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.8.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
            - "--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)"
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sh", "-c", "rm -rf /registration/zfs-localpv /registration/zfs-localpv-reg.sock"]
          env:
            - name: ADDRESS
              value: /plugin/csi.sock
            - name: DRIVER_REG_SOCK_PATH
              value: /var/lib/kubelet/plugins/zfs-localpv/csi.sock
            - name: KUBE_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: NODE_DRIVER
              value: openebs-zfs
          volumeMounts:
            - name: plugin-dir
              mountPath: /plugin
            - name: registration-dir
              mountPath: /registration
        - name: openebs-zfs-plugin
          securityContext:
            privileged: true
            allowPrivilegeEscalation: true
          image: "openebs/zfs-driver:2.3.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--nodename=$(OPENEBS_NODE_NAME)"
            - "--endpoint=$(OPENEBS_CSI_ENDPOINT)"
            - "--plugin=$(OPENEBS_NODE_DRIVER)"
          env:
            - name: OPENEBS_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: OPENEBS_CSI_ENDPOINT
              value: unix:///plugin/csi.sock
            - name: OPENEBS_NODE_DRIVER
              value: agent
            - name: OPENEBS_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: ALLOWED_TOPOLOGIES
              value: "All"
          volumeMounts:
            - name: plugin-dir
              mountPath: /plugin
            - name: device-dir
              mountPath: /dev
            - name: encr-keys
              mountPath: /home/keys
            - name: chroot-zfs
              mountPath: /sbin/zfs
              subPath: zfs
            - name: host-root
              mountPath: /host
              mountPropagation: "HostToContainer"
              readOnly: true
            - name: pods-mount-dir
              mountPath: /var/lib/kubelet/
              # needed so that any mounts setup inside this container are
              # propagated back to the host machine.
              mountPropagation: "Bidirectional"
      volumes:
        - name: device-dir
          hostPath:
            path: /dev
            type: Directory
        - name: encr-keys
          hostPath:
            path: /home/keys
            type: DirectoryOrCreate
        - name: chroot-zfs
          configMap:
            defaultMode: 0555
            name: openebs-zfspv-bin
        - name: host-root
          hostPath:
            path: /
            type: Directory
        - name: registration-dir
          hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: DirectoryOrCreate
        - name: plugin-dir
          hostPath:
            path: /var/lib/kubelet/plugins/zfs-localpv/
            type: DirectoryOrCreate
        - name: pods-mount-dir
          hostPath:
            path: /var/lib/kubelet/
            type: Directory
---
# Source: my-openebs/charts/openebs/templates/ndm/daemonset-ndm.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-openebs-ndm
  labels:
    app: openebs
    chart: openebs-3.9.0
    release: release-name
    heritage: Helm
    component: ndm
    openebs.io/component-name: ndm
    openebs.io/version: 3.9.0
spec:
  updateStrategy:
    type: "RollingUpdate"
  selector:
    matchLabels:
      app: openebs
      release: release-name
      component: ndm
  template:
    metadata:
      labels:
        app: openebs
        release: release-name
        component: ndm
        openebs.io/component-name: ndm
        name: openebs-ndm
        openebs.io/version: 3.9.0
    spec:
      serviceAccountName: release-name-openebs
      hostNetwork: true
      # host PID is used to check status of iSCSI Service when the NDM
      # API service is enabled
      containers:
      - name: openebs-ndm
        image: "openebs/node-disk-manager:2.1.0"
        args:
          - -v=4
          - --feature-gates=GPTBasedUUID
        imagePullPolicy: IfNotPresent
        securityContext:
          privileged: true
        env:
        # namespace in which NDM is installed will be passed to NDM Daemonset
        # as environment variable
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        # pass hostname as env variable using downward API to the NDM container
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # specify the directory where the sparse files need to be created.
        # if not specified, then sparse files will not be created.
        - name: SPARSE_FILE_DIR
          value: "/var/openebs/sparse"
        # Size(bytes) of the sparse file to be created.
        - name: SPARSE_FILE_SIZE
          value: "10737418240"
        # Specify the number of sparse files to be created
        - name: SPARSE_FILE_COUNT
          value: "0"
        # Process name used for matching is limited to the 15 characters
        # present in the pgrep output.
        # So fullname can be used here with pgrep (cmd is < 15 chars).
        livenessProbe:
          exec:
            command:
            - pgrep
            - "ndm"
          initialDelaySeconds: 30
          periodSeconds: 60
        volumeMounts:
        - name: config
          mountPath: /host/node-disk-manager.config
          subPath: node-disk-manager.config
          readOnly: true
        - name: udev
          mountPath: /run/udev
        - name: procmount
          mountPath: /host/proc
          readOnly: true
        - name: devmount
          mountPath: /dev
        - name: basepath
          mountPath: /var/openebs/ndm
        - name: sparsepath
          mountPath: /var/openebs/sparse
      volumes:
      - name: config
        configMap:
          name: release-name-openebs-ndm-config
      - name: udev
        hostPath:
          path: /run/udev
          type: Directory
      # mount /proc (to access mount file of process 1 of host) inside container
      # to read mount-point of disks and partitions
      - name: procmount
        hostPath:
          path: /proc
          type: Directory
      # the /dev directory is mounted so that we have access to the devices that
      # are connected at runtime of the pod.
      - name: devmount
        hostPath:
          path: /dev
          type: Directory
      - name: basepath
        hostPath:
          path: "/var/openebs/ndm"
          type: DirectoryOrCreate
      - name: sparsepath
        hostPath:
          path: /var/openebs/sparse
      # By default the node-disk-manager will be run on all kubernetes nodes
      # If you would like to limit this to only some nodes, say the nodes
      # that have storage attached, you could label those node and use
      # nodeSelector.
      #
      # e.g. label the storage nodes with - "openebs.io/nodegroup"="storage-node"
      # kubectl label node <node-name> "openebs.io/nodegroup"="storage-node"
      #nodeSelector:
      #  "openebs.io/nodegroup": "storage-node"
---
# Source: my-openebs/charts/openebs/charts/cstor/charts/openebs-ndm/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-openebs-ndm-operator
  labels:
    chart: openebs-ndm-2.1.0
    heritage: Helm
    openebs.io/version: "2.1.0"
    app: openebs-ndm-operator
    release: release-name
    component: openebs-ndm-operator
    openebs.io/component-name: openebs-ndm-operator
spec:
  replicas: 1
  strategy:
    type: "Recreate"
    rollingUpdate: null
  selector:
    matchLabels:
      app: openebs-ndm-operator
      release: release-name
      component: openebs-ndm-operator
  template:
    metadata:
      labels:
        chart: openebs-ndm-2.1.0
        heritage: Helm
        openebs.io/version: "2.1.0"
        app: openebs-ndm-operator
        release: release-name
        component: openebs-ndm-operator
        openebs.io/component-name: openebs-ndm-operator
        name: openebs-ndm-operator
    spec:
      serviceAccountName: openebs-ndm
      containers:
      - name: release-name-openebs-ndm-operator
        image: "openebs/node-disk-operator:2.1.0"
        imagePullPolicy: IfNotPresent
        resources:
            {}
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8585
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8585
          initialDelaySeconds: 5
          periodSeconds: 10
        env:
        - name: WATCH_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        - name: OPERATOR_NAME
          value: "node-disk-operator"
        - name: CLEANUP_JOB_IMAGE
          value: "openebs/linux-utils:3.4.0"
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/admission-server.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-cstor-admission-server
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    app: "cstor-admission-webhook"
    release: release-name
    component: "cstor-admission-webhook"
    openebs.io/component-name: "cstor-admission-webhook"
spec:
  replicas: 
  strategy:
    type: Recreate
    rollingUpdate: null
  selector:
    matchLabels:
      app: "cstor-admission-webhook"
      release: release-name
      component: "cstor-admission-webhook"
  template:
    metadata:
      labels:
        chart: cstor-3.5.0
        heritage: Helm
        openebs.io/version: "3.5.0"
        app: "cstor-admission-webhook"
        release: release-name
        component: "cstor-admission-webhook"
        openebs.io/component-name: "cstor-admission-webhook"
    spec:
      serviceAccountName: openebs-cstor-operator
      containers:
        - name: release-name-cstor-admission-webhook
          image: "openebs/cstor-webhook:3.5.0"
          imagePullPolicy: IfNotPresent
          resources:
            {}
          args:
            - -alsologtostderr
            - -v=2
            - 2>&1
          env:
            - name: OPENEBS_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: ADMISSION_WEBHOOK_FAILURE_POLICY
              value: Fail
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/cspc-operator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-cstor-cspc-operator
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "cspc-operator"
    release: release-name
    component: "cspc-operator"
    openebs.io/component-name: "cspc-operator"
spec:
  selector:
    matchLabels:
      name: "cspc-operator"
      release: release-name
      component: "cspc-operator"
  replicas: 
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        chart: cstor-3.5.0
        heritage: Helm
        openebs.io/version: "3.5.0"
        name: "cspc-operator"
        release: release-name
        component: "cspc-operator"
        openebs.io/component-name: "cspc-operator"
    spec:
      serviceAccountName: openebs-cstor-operator
      containers:
        - name: release-name-cstor-cspc-operator
          imagePullPolicy: IfNotPresent
          image: "openebs/cspc-operator:3.5.0"
          resources:
            {}
          env:
            - name: OPENEBS_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OPENEBS_SERVICEACCOUNT_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: CSPC_OPERATOR_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            # OPENEBS_IO_BASE_DIR is used to configure base directory for openebs on host path.
            # Where OpenEBS can store required files. Default base path will be /var/openebs
            - name: OPENEBS_IO_BASE_DIR
              value: "/var/openebs"
            # OPENEBS_IO_CSTOR_POOL_SPARSE_DIR can be used to specify the hostpath
            # to be used for saving the shared content between the side cars
            # of cstor pool pod. This ENV is also used to indicate the location
            # of the sparse devices.
            # The default path used is /var/openebs/sparse
            - name: OPENEBS_IO_CSTOR_POOL_SPARSE_DIR
              value: "/var/openebs/sparse"
            - name: OPENEBS_IO_CSPI_MGMT_IMAGE
              value: "openebs/cstor-pool-manager:3.5.0"
            - name: OPENEBS_IO_CSTOR_POOL_IMAGE
              value: "openebs/cstor-pool:3.5.0"
            - name:  OPENEBS_IO_CSTOR_POOL_EXPORTER_IMAGE
              value: "openebs/m-exporter:3.5.0"
            - name: RESYNC_INTERVAL
              value: "30"
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/cvc-operator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-cstor-cvc-operator
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "cvc-operator"
    release: release-name
    component: "cvc-operator"
    openebs.io/component-name: "cvc-operator"
spec:
  selector:
    matchLabels:
      name: "cvc-operator"
      release: release-name
      component: "cvc-operator"
  replicas: 
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        chart: cstor-3.5.0
        heritage: Helm
        openebs.io/version: "3.5.0"
        name: "cvc-operator"
        release: release-name
        component: "cvc-operator"
        openebs.io/component-name: "cvc-operator"
    spec:
      serviceAccountName: openebs-cstor-operator
      containers:
        - name: release-name-cstor-cvc-operator
          imagePullPolicy: IfNotPresent
          image: "openebs/cvc-operator:3.5.0"
          args:
            - "--v=2"
            - "--leader-election=false"
            - "--bind=$(OPENEBS_CVC_POD_IP)"
          resources:
            {}
          env:
            # OPENEBS_IO_BASE_DIR is used to configure base directory for openebs on host path.
            # Where OpenEBS can store required files. Default base path will be /var/openebs
            - name: OPENEBS_IO_BASE_DIR
              value: "/var/openebs"
            - name: OPENEBS_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OPENEBS_SERVICEACCOUNT_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: OPENEBS_CVC_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: OPENEBS_IO_CSTOR_TARGET_IMAGE
              value: "openebs/cstor-istgt:3.5.0"
            - name:  OPENEBS_IO_CSTOR_VOLUME_MGMT_IMAGE
              value: "openebs/cstor-volume-manager:3.5.0"
            - name:  OPENEBS_IO_VOLUME_MONITOR_IMAGE
              value: "openebs/m-exporter:3.5.0"
---
# Source: my-openebs/charts/openebs/charts/jiva/charts/localpv-provisioner/charts/openebs-ndm/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-openebs-ndm-operator
  labels:
    chart: openebs-ndm-2.1.0
    heritage: Helm
    openebs.io/version: "2.1.0"
    app: openebs-ndm-operator
    release: release-name
    component: openebs-ndm-operator
    openebs.io/component-name: openebs-ndm-operator
spec:
  replicas: 1
  strategy:
    type: "Recreate"
    rollingUpdate: null
  selector:
    matchLabels:
      app: openebs-ndm-operator
      release: release-name
      component: openebs-ndm-operator
  template:
    metadata:
      labels:
        chart: openebs-ndm-2.1.0
        heritage: Helm
        openebs.io/version: "2.1.0"
        app: openebs-ndm-operator
        release: release-name
        component: openebs-ndm-operator
        openebs.io/component-name: openebs-ndm-operator
        name: openebs-ndm-operator
    spec:
      serviceAccountName: openebs-ndm
      containers:
      - name: release-name-openebs-ndm-operator
        image: "openebs/node-disk-operator:2.1.0"
        imagePullPolicy: IfNotPresent
        resources:
            {}
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8585
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8585
          initialDelaySeconds: 5
          periodSeconds: 10
        env:
        - name: WATCH_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        - name: OPERATOR_NAME
          value: "node-disk-operator"
        - name: CLEANUP_JOB_IMAGE
          value: "openebs/linux-utils:3.4.0"
---
# Source: my-openebs/charts/openebs/charts/jiva/charts/localpv-provisioner/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-localpv-provisioner
  labels:
    chart: localpv-provisioner-3.4.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    app: localpv-provisioner
    release: release-name
    component: "localpv-provisioner"
    openebs.io/component-name: openebs-localpv-provisioner
spec:
  replicas: 1
  strategy:
    type: "Recreate"
    rollingUpdate: null
  selector:
    matchLabels:
      app: localpv-provisioner
      release: release-name
      component: "localpv-provisioner"
  template:
    metadata:
      labels:
        chart: localpv-provisioner-3.4.1
        heritage: Helm
        openebs.io/version: "3.4.0"
        app: localpv-provisioner
        release: release-name
        component: "localpv-provisioner"
        openebs.io/component-name: openebs-localpv-provisioner
        
        name: openebs-localpv-provisioner
    spec:
      serviceAccountName: release-name-localpv-provisioner
      securityContext:
        {}
      containers:
      - name: release-name-localpv-provisioner
        image: "openebs/provisioner-localpv:3.4.0"
        imagePullPolicy: IfNotPresent
        resources:
          null
        args:
          - "--bd-time-out=$(BDC_BD_BIND_RETRIES)"
        env:
        # OPENEBS_IO_K8S_MASTER enables openebs provisioner to connect to K8s
        # based on this address. This is ignored if empty.
        # This is supported for openebs provisioner version 0.5.2 onwards
        #- name: OPENEBS_IO_K8S_MASTER
        #  value: "http://10.128.0.12:8080"
        # OPENEBS_IO_KUBE_CONFIG enables openebs provisioner to connect to K8s
        # based on this config. This is ignored if empty.
        # This is supported for openebs provisioner version 0.5.2 onwards
        #- name: OPENEBS_IO_KUBE_CONFIG
        #  value: "/home/ubuntu/.kube/config"
        # This sets the number of times the provisioner should try
        # with a polling interval of 5 seconds, to get the Blockdevice
        # Name from a BlockDeviceClaim, before the BlockDeviceClaim
        # is deleted. E.g. 12 * 5 seconds = 60 seconds timeout
        - name: BDC_BD_BIND_RETRIES
          value: "12"
        - name: OPENEBS_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # OPENEBS_SERVICE_ACCOUNT provides the service account of this pod as
        # environment variable
        - name: OPENEBS_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        # OPENEBS_IO_BASE_PATH is the environment variable that provides the
        # default base path on the node where host-path PVs will be provisioned.
        - name: OPENEBS_IO_ENABLE_ANALYTICS
          value: "true"
        - name: OPENEBS_IO_BASE_PATH
          value: "/var/openebs/local"
        - name: OPENEBS_IO_HELPER_IMAGE
          value: "openebs/linux-utils:3.4.0"
        - name: OPENEBS_IO_INSTALLER_TYPE
          value: "localpv-charts-helm"
        # LEADER_ELECTION_ENABLED is used to enable/disable leader election. By default
        # leader election is enabled.
        - name: LEADER_ELECTION_ENABLED
          value: "true"
        # Process name used for matching is limited to the 15 characters
        # present in the pgrep output.
        # So fullname can't be used here with pgrep (>15 chars).A regular expression
        # that matches the entire command name has to specified.
        # Anchor `^` : matches any string that starts with `provisioner-loc`
        # `.*`: matches any string that has `provisioner-loc` followed by zero or more char
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - test `pgrep -c "^provisioner-loc.*"` = 1
          initialDelaySeconds: 30
          periodSeconds: 60
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/jiva-operator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-jiva-operator
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "jiva-operator"
    release: release-name
    component: "jiva-operator"
    openebs.io/component-name: "jiva-operator"
spec:
  selector:
    matchLabels:
      name: "jiva-operator"
      release: release-name
      component: "jiva-operator"
  replicas: 
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        chart: jiva-3.5.1
        heritage: Helm
        openebs.io/version: "3.4.0"
        name: "jiva-operator"
        release: release-name
        component: "jiva-operator"
        openebs.io/component-name: "jiva-operator"
    spec:
      serviceAccountName: openebs-jiva-operator
      containers:
        - name: release-name-jiva-operator
          imagePullPolicy: IfNotPresent
          image: "openebs/jiva-operator:3.4.0"
          command:
          - jiva-operator
          resources:
            {}
          env:
            - name: OPENEBS_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: OPERATOR_NAME
              value: "jiva-operator"
            - name: OPENEBS_SERVICEACCOUNT_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: OPENEBS_IO_JIVA_CONTROLLER_IMAGE
              value: "openebs/jiva:3.4.0"
            - name: OPENEBS_IO_JIVA_REPLICA_IMAGE
              value: "openebs/jiva:3.4.0"
            - name: OPENEBS_IO_MAYA_EXPORTER_IMAGE
              value: "openebs/m-exporter:3.4.0"
---
# Source: my-openebs/charts/openebs/charts/localpv-provisioner/charts/openebs-ndm/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-openebs-ndm-operator
  labels:
    chart: openebs-ndm-2.1.0
    heritage: Helm
    openebs.io/version: "2.1.0"
    app: openebs-ndm-operator
    release: release-name
    component: openebs-ndm-operator
    openebs.io/component-name: openebs-ndm-operator
spec:
  replicas: 1
  strategy:
    type: "Recreate"
    rollingUpdate: null
  selector:
    matchLabels:
      app: openebs-ndm-operator
      release: release-name
      component: openebs-ndm-operator
  template:
    metadata:
      labels:
        chart: openebs-ndm-2.1.0
        heritage: Helm
        openebs.io/version: "2.1.0"
        app: openebs-ndm-operator
        release: release-name
        component: openebs-ndm-operator
        openebs.io/component-name: openebs-ndm-operator
        name: openebs-ndm-operator
    spec:
      serviceAccountName: openebs-ndm
      containers:
      - name: release-name-openebs-ndm-operator
        image: "openebs/node-disk-operator:2.1.0"
        imagePullPolicy: IfNotPresent
        resources:
            {}
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8585
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8585
          initialDelaySeconds: 5
          periodSeconds: 10
        env:
        - name: WATCH_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        - name: OPERATOR_NAME
          value: "node-disk-operator"
        - name: CLEANUP_JOB_IMAGE
          value: "openebs/linux-utils:3.4.0"
---
# Source: my-openebs/charts/openebs/charts/localpv-provisioner/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-localpv-provisioner
  labels:
    chart: localpv-provisioner-3.4.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    app: localpv-provisioner
    release: release-name
    component: "localpv-provisioner"
    openebs.io/component-name: openebs-localpv-provisioner
spec:
  replicas: 1
  strategy:
    type: "Recreate"
    rollingUpdate: null
  selector:
    matchLabels:
      app: localpv-provisioner
      release: release-name
      component: "localpv-provisioner"
  template:
    metadata:
      labels:
        chart: localpv-provisioner-3.4.1
        heritage: Helm
        openebs.io/version: "3.4.0"
        app: localpv-provisioner
        release: release-name
        component: "localpv-provisioner"
        openebs.io/component-name: openebs-localpv-provisioner
        
        name: openebs-localpv-provisioner
    spec:
      serviceAccountName: release-name-localpv-provisioner
      securityContext:
        {}
      containers:
      - name: release-name-localpv-provisioner
        image: "openebs/provisioner-localpv:3.4.0"
        imagePullPolicy: IfNotPresent
        resources:
          null
        args:
          - "--bd-time-out=$(BDC_BD_BIND_RETRIES)"
        env:
        # OPENEBS_IO_K8S_MASTER enables openebs provisioner to connect to K8s
        # based on this address. This is ignored if empty.
        # This is supported for openebs provisioner version 0.5.2 onwards
        #- name: OPENEBS_IO_K8S_MASTER
        #  value: "http://10.128.0.12:8080"
        # OPENEBS_IO_KUBE_CONFIG enables openebs provisioner to connect to K8s
        # based on this config. This is ignored if empty.
        # This is supported for openebs provisioner version 0.5.2 onwards
        #- name: OPENEBS_IO_KUBE_CONFIG
        #  value: "/home/ubuntu/.kube/config"
        # This sets the number of times the provisioner should try
        # with a polling interval of 5 seconds, to get the Blockdevice
        # Name from a BlockDeviceClaim, before the BlockDeviceClaim
        # is deleted. E.g. 12 * 5 seconds = 60 seconds timeout
        - name: BDC_BD_BIND_RETRIES
          value: "12"
        - name: OPENEBS_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # OPENEBS_SERVICE_ACCOUNT provides the service account of this pod as
        # environment variable
        - name: OPENEBS_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        # OPENEBS_IO_BASE_PATH is the environment variable that provides the
        # default base path on the node where host-path PVs will be provisioned.
        - name: OPENEBS_IO_ENABLE_ANALYTICS
          value: "true"
        - name: OPENEBS_IO_BASE_PATH
          value: "/var/openebs/local"
        - name: OPENEBS_IO_HELPER_IMAGE
          value: "openebs/linux-utils:3.4.0"
        - name: OPENEBS_IO_INSTALLER_TYPE
          value: "localpv-charts-helm"
        # LEADER_ELECTION_ENABLED is used to enable/disable leader election. By default
        # leader election is enabled.
        - name: LEADER_ELECTION_ENABLED
          value: "true"
        # Process name used for matching is limited to the 15 characters
        # present in the pgrep output.
        # So fullname can't be used here with pgrep (>15 chars).A regular expression
        # that matches the entire command name has to specified.
        # Anchor `^` : matches any string that starts with `provisioner-loc`
        # `.*`: matches any string that has `provisioner-loc` followed by zero or more char
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - test `pgrep -c "^provisioner-loc.*"` = 1
          initialDelaySeconds: 30
          periodSeconds: 60
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/jaeger-operator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-jaeger-operator
  namespace: prod
  labels:
    
    app.kubernetes.io/name: jaeger-operator
    app.kubernetes.io/instance: release-name
spec:
  replicas: 1
  selector:
    matchLabels:
      
      app.kubernetes.io/name: jaeger-operator
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      name: release-name-jaeger-operator
      labels:
        
        app.kubernetes.io/name: jaeger-operator
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: release-name-jaeger-operator
      containers:
        - name: release-name-jaeger-operator
          image: "jaegertracing/jaeger-operator:1.24.0"
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 8383
            name: metrics
          args: ["start"]
          env:
            - name: WATCH_NAMESPACE
              value: ""
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OPERATOR_NAME
              value: "release-name-jaeger-operator"
          resources:
            {}
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/localpv-provisioner/charts/openebs-ndm/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-openebs-ndm-operator
  labels:
    chart: openebs-ndm-2.1.0
    heritage: Helm
    openebs.io/version: "2.1.0"
    app: openebs-ndm-operator
    release: release-name
    component: openebs-ndm-operator
    openebs.io/component-name: openebs-ndm-operator
spec:
  replicas: 1
  strategy:
    type: "Recreate"
    rollingUpdate: null
  selector:
    matchLabels:
      app: openebs-ndm-operator
      release: release-name
      component: openebs-ndm-operator
  template:
    metadata:
      labels:
        chart: openebs-ndm-2.1.0
        heritage: Helm
        openebs.io/version: "2.1.0"
        app: openebs-ndm-operator
        release: release-name
        component: openebs-ndm-operator
        openebs.io/component-name: openebs-ndm-operator
        name: openebs-ndm-operator
    spec:
      serviceAccountName: openebs-ndm
      containers:
      - name: release-name-openebs-ndm-operator
        image: "openebs/node-disk-operator:2.1.0"
        imagePullPolicy: IfNotPresent
        resources:
            {}
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8585
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8585
          initialDelaySeconds: 5
          periodSeconds: 10
        env:
        - name: WATCH_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        - name: OPERATOR_NAME
          value: "node-disk-operator"
        - name: CLEANUP_JOB_IMAGE
          value: "openebs/linux-utils:3.4.0"
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/localpv-provisioner/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-localpv-provisioner
  labels:
    chart: localpv-provisioner-3.4.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    app: localpv-provisioner
    release: release-name
    component: "localpv-provisioner"
    openebs.io/component-name: openebs-localpv-provisioner
spec:
  replicas: 1
  strategy:
    type: "Recreate"
    rollingUpdate: null
  selector:
    matchLabels:
      app: localpv-provisioner
      release: release-name
      component: "localpv-provisioner"
  template:
    metadata:
      labels:
        chart: localpv-provisioner-3.4.1
        heritage: Helm
        openebs.io/version: "3.4.0"
        app: localpv-provisioner
        release: release-name
        component: "localpv-provisioner"
        openebs.io/component-name: openebs-localpv-provisioner
        
        name: openebs-localpv-provisioner
    spec:
      serviceAccountName: release-name-localpv-provisioner
      securityContext:
        {}
      containers:
      - name: release-name-localpv-provisioner
        image: "openebs/provisioner-localpv:3.4.0"
        imagePullPolicy: IfNotPresent
        resources:
          null
        args:
          - "--bd-time-out=$(BDC_BD_BIND_RETRIES)"
        env:
        # OPENEBS_IO_K8S_MASTER enables openebs provisioner to connect to K8s
        # based on this address. This is ignored if empty.
        # This is supported for openebs provisioner version 0.5.2 onwards
        #- name: OPENEBS_IO_K8S_MASTER
        #  value: "http://10.128.0.12:8080"
        # OPENEBS_IO_KUBE_CONFIG enables openebs provisioner to connect to K8s
        # based on this config. This is ignored if empty.
        # This is supported for openebs provisioner version 0.5.2 onwards
        #- name: OPENEBS_IO_KUBE_CONFIG
        #  value: "/home/ubuntu/.kube/config"
        # This sets the number of times the provisioner should try
        # with a polling interval of 5 seconds, to get the Blockdevice
        # Name from a BlockDeviceClaim, before the BlockDeviceClaim
        # is deleted. E.g. 12 * 5 seconds = 60 seconds timeout
        - name: BDC_BD_BIND_RETRIES
          value: "12"
        - name: OPENEBS_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # OPENEBS_SERVICE_ACCOUNT provides the service account of this pod as
        # environment variable
        - name: OPENEBS_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        # OPENEBS_IO_BASE_PATH is the environment variable that provides the
        # default base path on the node where host-path PVs will be provisioned.
        - name: OPENEBS_IO_ENABLE_ANALYTICS
          value: "true"
        - name: OPENEBS_IO_BASE_PATH
          value: "/var/openebs/local"
        - name: OPENEBS_IO_HELPER_IMAGE
          value: "openebs/linux-utils:3.4.0"
        - name: OPENEBS_IO_INSTALLER_TYPE
          value: "localpv-charts-helm"
        # LEADER_ELECTION_ENABLED is used to enable/disable leader election. By default
        # leader election is enabled.
        - name: LEADER_ELECTION_ENABLED
          value: "true"
        # Process name used for matching is limited to the 15 characters
        # present in the pgrep output.
        # So fullname can't be used here with pgrep (>15 chars).A regular expression
        # that matches the entire command name has to specified.
        # Anchor `^` : matches any string that starts with `provisioner-loc`
        # `.*`: matches any string that has `provisioner-loc` followed by zero or more char
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - test `pgrep -c "^provisioner-loc.*"` = 1
          initialDelaySeconds: 30
          periodSeconds: 60
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-grafana
  namespace: prod
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: release-name
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: release-name
      annotations:
        checksum/config: 8fdef32c969ccb65b36f4eea56361c649d449e1b5d398cfacc18c7f924de752c
        checksum/dashboards-json-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/sc-dashboard-provider-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/secret: 08bc4df365c6c3d49337e0c42d5e5e545b4ca2c64e4057ea89cd07ac52b236c0
    spec:
      
      serviceAccountName: release-name-grafana
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsUser: 472
      enableServiceLinks: true
      containers:
        - name: grafana-sc-datasources
          image: "quay.io/kiwigrid/k8s-sidecar:1.15.6"
          imagePullPolicy: IfNotPresent
          env:
            - name: METHOD
              value: WATCH
            - name: LABEL
              value: "grafana_datasource"
            - name: FOLDER
              value: "/etc/grafana/provisioning/datasources"
            - name: RESOURCE
              value: "both"
            - name: REQ_USERNAME
              valueFrom:
                secretKeyRef:
                  name: release-name-grafana
                  key: admin-user
            - name: REQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-grafana
                  key: admin-password
            - name: REQ_URL
              value: http://localhost:3000/api/admin/provisioning/datasources/reload
            - name: REQ_METHOD
              value: POST
          resources:
            {}
          volumeMounts:
            - name: sc-datasources-volume
              mountPath: "/etc/grafana/provisioning/datasources"
        - name: grafana
          image: "grafana/grafana:8.3.5"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
            - name: sc-datasources-volume
              mountPath: "/etc/grafana/provisioning/datasources"
          ports:
            - name: service
              containerPort: 80
              protocol: TCP
            - name: grafana
              containerPort: 3000
              protocol: TCP
          env:
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: release-name-grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: release-name-grafana
                  key: admin-password
            - name: GF_PATHS_DATA
              value: /var/lib/grafana/
            - name: GF_PATHS_LOGS
              value: /var/log/grafana
            - name: GF_PATHS_PLUGINS
              value: /var/lib/grafana/plugins
            - name: GF_PATHS_PROVISIONING
              value: /etc/grafana/provisioning
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: release-name-grafana
        - name: storage
          emptyDir: {}
        - name: sc-datasources-volume
          emptyDir: {}
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/charts/kube-state-metrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-kube-state-metrics
  namespace: prod
  labels:    
    helm.sh/chart: kube-state-metrics-4.4.3
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kube-state-metrics
    app.kubernetes.io/name: kube-state-metrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.3.0"
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/instance: release-name
  replicas: 1
  template:
    metadata:
      labels:        
        helm.sh/chart: kube-state-metrics-4.4.3
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kube-state-metrics
        app.kubernetes.io/name: kube-state-metrics
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "2.3.0"
    spec:
      hostNetwork: false
      serviceAccountName: release-name-kube-state-metrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsUser: 65534
      containers:
      - name: kube-state-metrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        - --telemetry-port=8081
        imagePullPolicy: IfNotPresent
        image: "k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.3.0"
        ports:
        - containerPort: 8080
          name: "http"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          timeoutSeconds: 5
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/alertmanager/deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: "alertmanager"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-alertmanager
  namespace: prod
spec:
  selector:
    matchLabels:
      component: "alertmanager"
      app: prometheus
      release: release-name
  replicas: 1
  template:
    metadata:
      labels:
        component: "alertmanager"
        app: prometheus
        release: release-name
        chart: prometheus-15.5.4
        heritage: Helm
    spec:
      serviceAccountName: release-name-prometheus-alertmanager
      containers:
        - name: prometheus-alertmanager
          image: "quay.io/prometheus/alertmanager:v0.23.0"
          imagePullPolicy: "IfNotPresent"
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
          args:
            - --config.file=/etc/config/alertmanager.yml
            - --storage.path=/data
            - --cluster.listen-address=
            - --web.external-url=http://localhost:9093

          ports:
            - containerPort: 9093
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9093
            initialDelaySeconds: 30
            timeoutSeconds: 30
          resources:
            {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: "/data"
              subPath: ""
        - name: prometheus-alertmanager-configmap-reload
          image: "jimmidyson/configmap-reload:v0.5.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --volume-dir=/etc/config
            - --webhook-url=http://127.0.0.1:9093/-/reload
          resources:
            {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      volumes:
        - name: config-volume
          configMap:
            name: release-name-prometheus-alertmanager
        - name: storage-volume
          persistentVolumeClaim:
            claimName: release-name-prometheus-alertmanager
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/pushgateway/deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: "pushgateway"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-pushgateway
  namespace: prod
spec:
  selector:
    matchLabels:
      component: "pushgateway"
      app: prometheus
      release: release-name
  replicas: 1
  template:
    metadata:
      labels:
        component: "pushgateway"
        app: prometheus
        release: release-name
        chart: prometheus-15.5.4
        heritage: Helm
    spec:
      serviceAccountName: release-name-prometheus-pushgateway
      containers:
        - name: prometheus-pushgateway
          image: "prom/pushgateway:v1.4.2"
          imagePullPolicy: "IfNotPresent"
          args:
          ports:
            - containerPort: 9091
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9091
            initialDelaySeconds: 10
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9091
            initialDelaySeconds: 10
            timeoutSeconds: 10
          resources:
            {}
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/prometheus/templates/server/deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: "server"
    app: prometheus
    release: release-name
    chart: prometheus-15.5.4
    heritage: Helm
  name: release-name-prometheus-server
  namespace: prod
spec:
  selector:
    matchLabels:
      component: "server"
      app: prometheus
      release: release-name
  replicas: 1
  template:
    metadata:
      labels:
        component: "server"
        app: prometheus
        release: release-name
        chart: prometheus-15.5.4
        heritage: Helm
    spec:
      enableServiceLinks: true
      serviceAccountName: release-name-prometheus-server
      containers:
        - name: prometheus-server-configmap-reload
          image: "jimmidyson/configmap-reload:v0.5.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --volume-dir=/etc/config
            - --webhook-url=http://127.0.0.1:9090/-/reload
          resources:
            {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true

        - name: prometheus-server
          image: "quay.io/prometheus/prometheus:v2.34.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --storage.tsdb.retention.time=15d
            - --config.file=/etc/config/prometheus.yml
            - --storage.tsdb.path=/data
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-lifecycle
          ports:
            - containerPort: 9090
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 5
            timeoutSeconds: 4
            failureThreshold: 3
            successThreshold: 1
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 3
            successThreshold: 1
          resources:
            {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: /data
              subPath: ""
      hostNetwork: false
      dnsPolicy: ClusterFirst
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: release-name-prometheus-server
        - name: storage-volume
          persistentVolumeClaim:
            claimName: release-name-prometheus-server
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/agents/core/agent-core-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-agent-core
  labels:
    app: agent-core
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agent-core
      openebs.io/release: release-name
  template:
    metadata:
      labels:
        app: agent-core
        openebs.io/release: release-name
        openebs.io/version: 2.4.0
        openebs.io/logging: "true"
    spec:
      serviceAccount: release-name-service-account
      imagePullSecrets:
      initContainers:
        - command:
          - sh
          - -c
          - trap "exit 1" TERM; until nc -vzw 5 release-name-etcd 2379;
            do date; echo "Waiting for etcd..."; sleep 1; done;
          image: busybox:latest
          name: etcd-probe
      priorityClassName: release-name-cluster-critical
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations: 
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 5
      containers:
        - name: agent-core
          resources:
            limits:
              cpu: "1000m"
              memory: "128Mi"
            requests:
              cpu: "500m"
              memory: "32Mi"
          image: "docker.io/openebs/mayastor-agent-core:v2.4.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-srelease-name-etcd:2379"
            - "--request-timeout=5s"
            - "--cache-period=30s"
            - "--grpc-server-addr=0.0.0.0:50051"
            - "--pool-commitment=250%"
            - "--snapshot-commitment=40%"
            - "--volume-commitment-initial=40%"
            - "--volume-commitment=40%"
            - "--events-url=nats://release-name-nats:4222"
          ports:
            - containerPort: 50051
          env:
            - name: RUST_LOG
              value: info
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
        - name: agent-ha-cluster
          resources:
            limits:
              cpu: "100m"
              memory: "64Mi"
            requests:
              cpu: "100m"
              memory: "16Mi"
          image: "docker.io/openebs/mayastor-agent-ha-cluster:v2.4.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-g=0.0.0.0:50052"
            - "--store=http://release-name-etcd:2379"
            - "--core-grpc=https://release-name-agent-core:50051"
          ports:
            - containerPort: 50052
          env:
            - name: RUST_LOG
              value: info
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/apis/api-rest-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-api-rest
  labels:
    app: api-rest
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: api-rest
      openebs.io/release: release-name
  template:
    metadata:
      labels:
        app: api-rest
        openebs.io/release: release-name
        openebs.io/version: 2.4.0
        openebs.io/logging: "true"
    spec:
      imagePullSecrets:
      initContainers:
        - command:
          - sh
          - -c
          - trap "exit 1" TERM; until nc -vzw 5 release-name-agent-core 50051; do date;
            echo "Waiting for agent-core-grpc services..."; sleep 1; done;
          image: busybox:latest
          name: agent-core-grpc-probe
        - command:
          - sh
          - -c
          - trap "exit 1" TERM; until nc -vzw 5 release-name-etcd 2379;
            do date; echo "Waiting for etcd..."; sleep 1; done;
          image: busybox:latest
          name: etcd-probe
      priorityClassName: release-name-cluster-critical
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations: 
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 5
      containers:
        - name: api-rest
          resources:
            limits:
              cpu: "100m"
              memory: "64Mi"
            requests:
              cpu: "50m"
              memory: "32Mi"
          image: "docker.io/openebs/mayastor-api-rest:v2.4.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--dummy-certificates"
            - "--no-auth"
            - "--http=0.0.0.0:8081"
            - "--request-timeout=5s"
            - "--core-grpc=https://release-name-agent-core:50051"
          ports:
            - containerPort: 8080
            - containerPort: 8081
          env:
            - name: RUST_LOG
              value: info
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/csi/csi-controller-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-csi-controller
  labels:
    app: csi-controller
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: csi-controller
      openebs.io/release: release-name
  template:
    metadata:
      labels:
        app: csi-controller
        openebs.io/release: release-name
        openebs.io/version: 2.4.0
        openebs.io/logging: "true"
    spec:
      hostNetwork: true
      serviceAccount: release-name-service-account
      dnsPolicy: ClusterFirstWithHostNet
      imagePullSecrets:
      initContainers:
        - command:
          - sh
          - -c
          - trap "exit 1" TERM; until nc -vzw 5 release-name-api-rest 8081; do date;
            echo "Waiting for REST API endpoint to become available"; sleep 1; done;
          image: busybox:latest
          name: api-rest-probe
      nodeSelector:
        kubernetes.io/arch: amd64
      containers:
        - name: csi-provisioner
          image: "registry.k8s.io/sig-storage/csi-provisioner:v3.5.0"
          args:
            - "--v=2"
            - "--csi-address=$(ADDRESS)"
            - "--feature-gates=Topology=true"
            - "--strict-topology=false"
            - "--default-fstype=ext4"
            - "--extra-create-metadata" # This is needed for volume group feature to work
            - "--timeout=36s"
            - "--worker-threads=10" # 10 for create and 10 for delete
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: csi-attacher
          image: "registry.k8s.io/sig-storage/csi-attacher:v4.3.0"
          args:
            - "--v=2"
            - "--csi-address=$(ADDRESS)"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: csi-snapshotter
          image: "registry.k8s.io/sig-storage/csi-snapshotter:v6.2.1"
          args:
            - "--v=2"
            - "--csi-address=$(ADDRESS)"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: csi-snapshot-controller
          args:
            - "--v=2"
            - "--leader-election=false" # since we are running single container
          image: "registry.k8s.io/sig-storage/snapshot-controller:v6.2.1"
          imagePullPolicy: IfNotPresent
        - name: csi-controller
          resources:
            limits:
              cpu: "32m"
              memory: "128Mi"
            requests:
              cpu: "16m"
              memory: "64Mi"
          image: "docker.io/openebs/mayastor-csi-controller:v2.4.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--csi-socket=/var/lib/csi/sockets/pluginproxy/csi.sock"
            - "--rest-endpoint=http://release-name-api-rest:8081"
            - "--node-selector=openebs.io/csi-node=mayastor"
          env:
            - name: RUST_LOG
              value: info
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
      volumes:
        - name: socket-dir
          emptyDir:
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/obs/obs-callhome-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-obs-callhome
  labels:
    app: obs-callhome
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: obs-callhome
      openebs.io/release: release-name
  template:
    metadata:
      labels:
        app: obs-callhome
        openebs.io/release: release-name
        openebs.io/version: 2.4.0
        openebs.io/logging: "true"
    spec:
      serviceAccountName: release-name-service-account
      imagePullSecrets:
      nodeSelector:
        kubernetes.io/arch: amd64
      containers:
        - name: obs-callhome
          image: "docker.io/openebs/mayastor-obs-callhome:v2.4.0"
          args:
            - "-e http://release-name-api-rest:8081"
            - "-n prod"
            - "--aggregator-url=http://release-name-obs-callhome-stats:9090/stats"
            
            - "--send-report"
            
          env:
            - name: RUST_LOG
              value: info
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: "100m"
              memory: "32Mi"
            requests:
              cpu: "50m"
              memory: "16Mi"
        - name: obs-callhome-stats
          image: "docker.io/openebs/mayastor-obs-callhome-stats:v2.4.0"
          args:
            - "--namespace=prod"
            - "--release-name=release-name"
            - "--mbus-url=nats://release-name-nats:4222"
          ports:
            - containerPort: 9090
              protocol: TCP
              name: stats
          env:
            - name: RUST_LOG
              value: info
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              cpu: "100m"
              memory: "32Mi"
            requests:
              cpu: "50m"
              memory: "16Mi"
---
# Source: my-openebs/charts/openebs/charts/mayastor/templates/mayastor/operators/operator-diskpool-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-operator-diskpool
  labels:
    app: operator-diskpool
    openebs.io/release: release-name
    openebs.io/version: 2.4.0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: operator-diskpool
      openebs.io/release: release-name
  template:
    metadata:
      labels:
        app: operator-diskpool
        openebs.io/release: release-name
        openebs.io/version: 2.4.0
        openebs.io/logging: "true"
    spec:
      serviceAccount: release-name-service-account
      imagePullSecrets:
      initContainers:
        - command:
          - sh
          - -c
          - trap "exit 1" TERM; until nc -vzw 5 release-name-agent-core 50051; do date;
            echo "Waiting for agent-core-grpc services..."; sleep 1; done;
          image: busybox:latest
          name: agent-core-grpc-probe
        - command:
          - sh
          - -c
          - trap "exit 1" TERM; until nc -vzw 5 release-name-etcd 2379;
            do date; echo "Waiting for etcd..."; sleep 1; done;
          image: busybox:latest
          name: etcd-probe
      nodeSelector:
        kubernetes.io/arch: amd64
      containers:
        - name: operator-diskpool
          resources:
            limits:
              cpu: "100m"
              memory: "32Mi"
            requests:
              cpu: "50m"
              memory: "16Mi"
          image: "docker.io/openebs/mayastor-operator-diskpool:v2.4.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-e http://release-name-api-rest:8081"
            - "-nprod"
            - "--request-timeout=5s"
            - "--interval=30s"
          env:
            - name: RUST_LOG
              value: info
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
---
# Source: my-openebs/charts/openebs/charts/nfs-provisioner/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-nfs-provisioner
  namespace: prod
  labels:
    chart: nfs-provisioner-0.10.0
    heritage: Helm
    openebs.io/version: "0.10.0"
    app: nfs-provisioner
    release: release-name
    component: nfs-provisioner
    openebs.io/component-name: openebs-nfs-provisioner
spec:
  selector:
    matchLabels:
        app: nfs-provisioner
        release: release-name
        component: nfs-provisioner
  replicas: 1
  strategy:
    type: Recreate
    rollingUpdate: null
  template:
    metadata:
      labels:
        chart: nfs-provisioner-0.10.0
        heritage: Helm
        openebs.io/version: "0.10.0"
        app: nfs-provisioner
        release: release-name
        component: nfs-provisioner
        openebs.io/component-name: openebs-nfs-provisioner
        name: openebs-nfs-provisioner
    spec:
      serviceAccountName: release-name-nfs-provisioner
      containers:
        - name: release-name-nfs-provisioner
          imagePullPolicy: IfNotPresent
          image: "openebs/provisioner-nfs:0.10.0"
          env:
            # OPENEBS_IO_K8S_MASTER enables openebs provisioner to connect to K8s
            # based on this address. This is ignored if empty.
            # This is supported for openebs provisioner version 0.5.2 onwards
            #- name: OPENEBS_IO_K8S_MASTER
            #  value: "http://10.128.0.12:8080"
            # OPENEBS_IO_KUBE_CONFIG enables openebs provisioner to connect to K8s
            # based on this config. This is ignored if empty.
            # This is supported for openebs provisioner version 0.5.2 onwards
            #- name: OPENEBS_IO_KUBE_CONFIG
            #  value: "/home/ubuntu/.kube/config"
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: OPENEBS_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            # OPENEBS_SERVICE_ACCOUNT provides the service account of this pod as
            # environment variable
            - name: OPENEBS_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            - name: OPENEBS_IO_ENABLE_ANALYTICS
              value: "true"
            - name: OPENEBS_IO_NFS_SERVER_USE_CLUSTERIP
              value: "true"
            - name: OPENEBS_IO_INSTALLER_TYPE
              value: "nfs-helm"
            # OPENEBS_IO_NFS_SERVER_IMG defines the nfs-server-alpine image name to be used
            # while creating nfs volume
            - name: OPENEBS_IO_NFS_SERVER_IMG
              value: "openebs/nfs-server-alpine:0.10.0"
            # LEADER_ELECTION_ENABLED is used to enable/disable leader election. By default
            # leader election is enabled.
            - name: LEADER_ELECTION_ENABLED
              value: "true"
            # OPENEBS_IO_NFS_SERVER_NODE_AFFINITY defines the node affinity rules to place NFS Server
            # instance. It accepts affinity rules in multiple ways:
            # - If NFS Server needs to be placed on storage nodes as well as only in
            #   zone-1 & zone-2 then value can be:
            #   value:  "kubernetes.io/zone:[zone-1,zone-2],kubernetes.io/storage-node".
            # - If NFS Server needs to be placed only on storage nodes & nfs nodes then
            #   value can be:
            #   value:  "kubernetes.io/storage-node,kubernetes.io/nfs-node"
          # Process name used for matching is limited to the 15 characters
          # present in the pgrep output.
          # So fullname can't be used here with pgrep (>15 chars).A regular expression
          # that matches the entire command name has to specified.
          # Anchor `^` : matches any string that starts with `provisioner-nfs`
          # `.*`: matches any string that has `provisioner-loc` followed by zero or more char
          livenessProbe:
            exec:
              command:
                - sh
                - -c
                - test `pgrep "^provisioner-nfs.*"` = 1
            initialDelaySeconds: 30
            periodSeconds: 60
          volumeMounts:
            # Mounting hook-config volume into nfs-provisioner config directory
      volumes:
        # hook-config volume uses ConfigMap 'hook-config' to load hook configuration
---
# Source: my-openebs/charts/openebs/charts/openebs-ndm/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-openebs-ndm-operator
  labels:
    chart: openebs-ndm-2.1.0
    heritage: Helm
    openebs.io/version: "2.1.0"
    app: openebs-ndm-operator
    release: release-name
    component: openebs-ndm-operator
    openebs.io/component-name: openebs-ndm-operator
spec:
  replicas: 1
  strategy:
    type: "Recreate"
    rollingUpdate: null
  selector:
    matchLabels:
      app: openebs-ndm-operator
      release: release-name
      component: openebs-ndm-operator
  template:
    metadata:
      labels:
        chart: openebs-ndm-2.1.0
        heritage: Helm
        openebs.io/version: "2.1.0"
        app: openebs-ndm-operator
        release: release-name
        component: openebs-ndm-operator
        openebs.io/component-name: openebs-ndm-operator
        name: openebs-ndm-operator
    spec:
      serviceAccountName: openebs-ndm
      containers:
      - name: release-name-openebs-ndm-operator
        image: "openebs/node-disk-operator:2.1.0"
        imagePullPolicy: IfNotPresent
        resources:
            {}
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8585
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8585
          initialDelaySeconds: 5
          periodSeconds: 10
        env:
        - name: WATCH_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        - name: OPERATOR_NAME
          value: "node-disk-operator"
        - name: CLEANUP_JOB_IMAGE
          value: "openebs/linux-utils:3.4.0"
---
# Source: my-openebs/charts/openebs/templates/localprovisioner/deployment-local-provisioner.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-openebs-localpv-provisioner
  labels:
    app: openebs
    chart: openebs-3.9.0
    release: release-name
    heritage: Helm
    component: localpv-provisioner
    openebs.io/component-name: openebs-localpv-provisioner
    openebs.io/version: 3.9.0
spec:
  replicas: 1
  strategy:
    type: "Recreate"
    rollingUpdate: null
  selector:
    matchLabels:
      app: openebs
      release: release-name
  template:
    metadata:
      labels:
        app: openebs
        release: release-name
        component: localpv-provisioner
        name: openebs-localpv-provisioner
        openebs.io/component-name: openebs-localpv-provisioner
        openebs.io/version: 3.9.0
    spec:
      serviceAccountName: release-name-openebs
      containers:
      - name: openebs-localpv-provisioner
        image: "openebs/provisioner-localpv:3.4.0"
        imagePullPolicy: IfNotPresent
        args:
          - "--bd-time-out=$(BDC_BD_BIND_RETRIES)"
        env:
        # OPENEBS_IO_K8S_MASTER enables openebs provisioner to connect to K8s
        # based on this address. This is ignored if empty.
        # This is supported for openebs provisioner version 0.5.2 onwards
        #- name: OPENEBS_IO_K8S_MASTER
        #  value: "http://10.128.0.12:8080"
        # OPENEBS_IO_KUBE_CONFIG enables openebs provisioner to connect to K8s
        # based on this config. This is ignored if empty.
        # This is supported for openebs provisioner version 0.5.2 onwards
        #- name: OPENEBS_IO_KUBE_CONFIG
        #  value: "/home/ubuntu/.kube/config"
        # This sets the number of times the provisioner should try
        # with a polling interval of 5 seconds, to get the Blockdevice
        # Name from a BlockDeviceClaim, before the BlockDeviceClaim
        # is deleted. E.g. 12 * 5 seconds = 60 seconds timeout
        - name: BDC_BD_BIND_RETRIES
          value: "12"
        # OPENEBS_NAMESPACE is the namespace that this provisioner will
        # lookup to find maya api service
        - name: OPENEBS_NAMESPACE
          value: "prod"
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        # OPENEBS_SERVICE_ACCOUNT provides the service account of this pod as
        # environment variable
        - name: OPENEBS_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        # OPENEBS_IO_BASE_PATH is the environment variable that provides the
        # default base path on the node where host-path PVs will be provisioned.
        - name: OPENEBS_IO_ENABLE_ANALYTICS
          value: "true"
        - name: OPENEBS_IO_BASE_PATH
          value: "/var/openebs/local"
        - name: OPENEBS_IO_HELPER_IMAGE
          value: "openebs/linux-utils:3.4.0"
        - name: OPENEBS_IO_INSTALLER_TYPE
          value: "charts-helm"
        # LEADER_ELECTION_ENABLED is used to enable/disable leader election. By default
        # leader election is enabled.
        - name: LEADER_ELECTION_ENABLED
          value: "true"
        # Process name used for matching is limited to the 15 characters
        # present in the pgrep output.
        # So fullname can't be used here with pgrep (>15 chars).A regular expression
        # that matches the entire command name has to specified.
        # Anchor `^` : matches any string that starts with `provisioner-loc`
        # `.*`: matches any string that has `provisioner-loc` followed by zero or more char
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - test `pgrep -c "^provisioner-loc.*"` = 1
          initialDelaySeconds: 30
          periodSeconds: 60
---
# Source: my-openebs/charts/openebs/templates/ndm/deployment-ndm-operator.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-openebs-ndm-operator
  labels:
    app: openebs
    chart: openebs-3.9.0
    release: release-name
    heritage: Helm
    component: ndm-operator
    openebs.io/component-name: ndm-operator
    openebs.io/version: 3.9.0
    name: ndm-operator
spec:
  replicas: 1
  strategy:
    type: "Recreate"
    rollingUpdate: null
  selector:
    matchLabels:
      app: openebs
      release: release-name
  template:
    metadata:
      labels:
        app: openebs
        release: release-name
        component: ndm-operator
        name: ndm-operator
        openebs.io/component-name: ndm-operator
        openebs.io/version: 3.9.0
    spec:
      serviceAccountName: release-name-openebs
      containers:
      - name: release-name-openebs-ndm-operator
        image: "openebs/node-disk-operator:2.1.0"
        imagePullPolicy: IfNotPresent
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8585
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8585
          initialDelaySeconds: 5
          periodSeconds: 10
        env:
        - name: WATCH_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        - name: OPERATOR_NAME
          value: "node-disk-operator"
        - name: CLEANUP_JOB_IMAGE
          value: "openebs/linux-utils:3.4.0"
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-controller.yaml
kind: StatefulSet
apiVersion: apps/v1
metadata:
  name: release-name-cstor-csi-controller
  labels:
    chart: cstor-3.5.0
    heritage: Helm
    openebs.io/version: "3.5.0"
    name: "openebs-cstor-csi-controller"
    release: release-name
    component: "openebs-cstor-csi-controller"
    openebs.io/component-name: "openebs-cstor-csi-controller"
spec:
  selector:
    matchLabels:
      name: "openebs-cstor-csi-controller"
      release: release-name
      component: "openebs-cstor-csi-controller"
  serviceName: "openebs-csi"
  replicas: 
  template:
    metadata:
      labels:
        chart: cstor-3.5.0
        heritage: Helm
        openebs.io/version: "3.5.0"
        name: "openebs-cstor-csi-controller"
        release: release-name
        component: "openebs-cstor-csi-controller"
        openebs.io/component-name: "openebs-cstor-csi-controller"
    spec:
      priorityClassName: release-name-cstor-csi-controller-critical
      serviceAccountName: openebs-cstor-csi-controller-sa
      containers:
        - name: csi-resizer
          image: "registry.k8s.io/sig-storage/csi-resizer:v1.8.0"
          resources:
            {}
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
            - "--leader-election"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: csi-snapshotter
          image: "registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2"
          args:
            - "--csi-address=$(ADDRESS)"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: snapshot-controller
          image: "registry.k8s.io/sig-storage/snapshot-controller:v6.2.2"
          args:
            - "--v=5"
            - "--leader-election=false"
          imagePullPolicy: IfNotPresent
        - name: csi-provisioner
          image: "registry.k8s.io/sig-storage/csi-provisioner:v3.5.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--csi-address=$(ADDRESS)"
            - "--v=5"
            - "--feature-gates=Topology=true"
            - "--extra-create-metadata=true"
            - "--metrics-address=:22011"
            - "--timeout=250s"
            - "--default-fstype=ext4"
          env:
            - name: MY_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: csi-attacher
          image: "registry.k8s.io/sig-storage/csi-attacher:v4.3.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: cstor-csi-plugin
          image: "openebs/cstor-csi-driver:3.5.0"
          imagePullPolicy: IfNotPresent
          env:
            - name: OPENEBS_CONTROLLER_DRIVER
              value: controller
            - name: OPENEBS_CSI_ENDPOINT
              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock
            - name: OPENEBS_CSI_API_URL
              value: https://openebs.io
              # OpenEBS namespace where the openebs cstor operator components
              # has been installed
            - name: OPENEBS_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OPENEBS_IO_INSTALLER_TYPE
              value: "cstor-helm"
            - name: OPENEBS_IO_ENABLE_ANALYTICS
              value: "true"
          args :
            - "--endpoint=$(OPENEBS_CSI_ENDPOINT)"
            - "--url=$(OPENEBS_CSI_API_URL)"
            - "--plugin=$(OPENEBS_CONTROLLER_DRIVER)"
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
      volumes:
        - name: socket-dir
          emptyDir: {}
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-controller.yaml
kind: StatefulSet
apiVersion: apps/v1
metadata:
  name: release-name-jiva-csi-controller
  labels:
    chart: jiva-3.5.1
    heritage: Helm
    openebs.io/version: "3.4.0"
    name: "openebs-jiva-csi-controller"
    release: release-name
    component: "openebs-jiva-csi-controller"
    openebs.io/component-name: "openebs-jiva-csi-controller"
spec:
  selector:
    matchLabels:
      name: "openebs-jiva-csi-controller"
      release: release-name
      component: "openebs-jiva-csi-controller"
  serviceName: "openebs-csi"
  replicas: 
  template:
    metadata:
      labels:
        chart: jiva-3.5.1
        heritage: Helm
        openebs.io/version: "3.4.0"
        name: "openebs-jiva-csi-controller"
        release: release-name
        component: "openebs-jiva-csi-controller"
        openebs.io/component-name: "openebs-jiva-csi-controller"
    spec:
      priorityClassName: release-name-jiva-csi-controller-critical
      serviceAccountName: openebs-jiva-csi-controller-sa
      containers:
        - name: csi-resizer
          image: "registry.k8s.io/sig-storage/csi-resizer:v1.8.0"
          resources:
            {}
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
            - "--leader-election"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: csi-provisioner
          image: "registry.k8s.io/sig-storage/csi-provisioner:v3.5.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--csi-address=$(ADDRESS)"
            - "--v=5"
            - "--feature-gates=Topology=true"
            - "--extra-create-metadata=true"
            - "--metrics-address=:22011"
            - "--timeout=250s"
            - "--default-fstype=ext4"
          env:
            - name: MY_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: csi-attacher
          image: "registry.k8s.io/sig-storage/csi-attacher:v4.3.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: jiva-csi-plugin
          image: "openebs/jiva-csi:3.4.0"
          imagePullPolicy: IfNotPresent
          env:
            - name: OPENEBS_JIVA_CSI_CONTROLLER
              value: controller
            - name: OPENEBS_JIVA_CSI_ENDPOINT
              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock
            - name: OPENEBS_CSI_API_URL
              value: https://openebs.io
            - name: OPENEBS_NODEID
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
              # OpenEBS namespace where the openebs jiva operator components
              # has been installed
            - name: OPENEBS_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OPENEBS_IO_INSTALLER_TYPE
              value: "jiva-helm"
            - name: OPENEBS_IO_ENABLE_ANALYTICS
              value: "true"
          args :
            - "--endpoint=$(OPENEBS_JIVA_CSI_ENDPOINT)"
            - "--plugin=$(OPENEBS_JIVA_CSI_CONTROLLER)"
            - "--name=jiva.csi.openebs.io"
            - "--nodeid=$(OPENEBS_NODEID)"
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: liveness-probe
          image: "registry.k8s.io/sig-storage/livenessprobe:v2.10.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--csi-address=/csi/csi.sock"
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
      volumes:
        - name: socket-dir
          emptyDir: {}
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/lvm-controller.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-lvm-localpv-controller
  labels:
    chart: lvm-localpv-1.3.0
    heritage: Helm
    openebs.io/version: "1.3.0"
    role: "openebs-lvm"
    app: "openebs-lvm-controller"
    release: release-name
    component: "openebs-lvm-controller"
    openebs.io/component-name: "openebs-lvm-controller"
spec:
  selector:
    matchLabels:
      app: "openebs-lvm-controller"
      release: release-name
      component: "openebs-lvm-controller"
  serviceName: "openebs-lvm"
  replicas: 1
  template:
    metadata:
      labels:
        chart: lvm-localpv-1.3.0
        heritage: Helm
        openebs.io/version: "1.3.0"
        role: "openebs-lvm"
        app: "openebs-lvm-controller"
        release: release-name
        component: "openebs-lvm-controller"
        openebs.io/component-name: "openebs-lvm-controller"
        
        name: openebs-lvm-controller
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - openebs-lvm-controller
            topologyKey: "kubernetes.io/hostname"
      priorityClassName: release-name-lvm-localpv-csi-controller-critical
      serviceAccountName: openebs-lvm-controller-sa
      containers:
        - name: csi-resizer
          image: "registry.k8s.io/sig-storage/csi-resizer:v1.8.0"
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
          resources:
            {}
        - name: csi-snapshotter
          image: "registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2"
          imagePullPolicy: IfNotPresent
          args:
            - "--csi-address=$(ADDRESS)"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
          resources:
            {}
        - name: snapshot-controller
          image: "registry.k8s.io/sig-storage/snapshot-controller:v6.2.2"
          args:
            - "--v=5"
          imagePullPolicy: IfNotPresent
          resources:
            {}
        - name: csi-provisioner
          image: "registry.k8s.io/sig-storage/csi-provisioner:v3.5.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--csi-address=$(ADDRESS)"
            - "--v=5"
            - "--feature-gates=Topology=true"
            - "--strict-topology"
            - "--enable-capacity=true"
            - "--extra-create-metadata=true"
            - "--default-fstype=ext4"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
          resources:
            {}
        - name: openebs-lvm-plugin
          image: "openebs/lvm-driver:1.3.0"
          imagePullPolicy: IfNotPresent
          env:
            - name: OPENEBS_CONTROLLER_DRIVER
              value: controller
            - name: OPENEBS_CSI_ENDPOINT
              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock
            - name: LVM_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OPENEBS_IO_INSTALLER_TYPE
              value: "lvm-localpv-helm"
            - name: OPENEBS_IO_ENABLE_ANALYTICS
              value: "true"
          args :
            - "--endpoint=$(OPENEBS_CSI_ENDPOINT)"
            - "--plugin=$(OPENEBS_CONTROLLER_DRIVER)"
            - "--kube-api-qps=0"
            - "--kube-api-burst=0"
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
          resources:
            {}
      volumes:
        - name: socket-dir
          emptyDir: {}
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/etcd/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-etcd
  namespace: "prod"
  labels:
    app.kubernetes.io/name: etcd
    helm.sh/chart: etcd-8.6.0
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: etcd
      app.kubernetes.io/instance: release-name
  serviceName: release-name-etcd-headless
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: etcd
        helm.sh/chart: etcd-8.6.0
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app: etcd
        openebs.io/logging: "true"
      annotations:
        checksum/token-secret: 1fd7c04069b66241e44c5cc67892bb5b865d3705606f9ef6cbf1eb813f4a7589
    spec:
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: etcd
                  app.kubernetes.io/instance: release-name
              topologyKey: kubernetes.io/hostname
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      serviceAccountName: "default"
      initContainers:
        - name: volume-permissions
          image: docker.io/bitnami/bitnami-shell:11-debian-11-r63
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/bash
            - -ec
            - |
              chown -R 1001:1001 /bitnami/etcd
          securityContext:
            runAsUser: 0
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/etcd
      containers:
        - name: etcd
          image: docker.io/bitnami/etcd:3.5.6-debian-11-r10
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_STS_NAME
              value: "release-name-etcd"
            - name: ETCDCTL_API
              value: "3"
            - name: ETCD_ON_K8S
              value: "yes"
            - name: ETCD_START_FROM_SNAPSHOT
              value: "no"
            - name: ETCD_DISASTER_RECOVERY
              value: "no"
            - name: ETCD_NAME
              value: "$(MY_POD_NAME)"
            - name: ETCD_DATA_DIR
              value: "/bitnami/etcd/data"
            - name: ETCD_LOG_LEVEL
              value: "info"
            - name: ALLOW_NONE_AUTHENTICATION
              value: "yes"
            - name: ETCD_AUTH_TOKEN
              value: "jwt,priv-key=/opt/bitnami/etcd/certs/token/jwt-token.pem,sign-method=RS256,ttl=10m"
            - name: ETCD_ADVERTISE_CLIENT_URLS
              value: "http://$(MY_POD_NAME).release-name-etcd-headless.prod.svc.cluster.local:2379,http://release-name-etcd.prod.svc.cluster.local:2379"
            - name: ETCD_LISTEN_CLIENT_URLS
              value: "http://0.0.0.0:2379"
            - name: ETCD_INITIAL_ADVERTISE_PEER_URLS
              value: "http://$(MY_POD_NAME).release-name-etcd-headless.prod.svc.cluster.local:2380"
            - name: ETCD_LISTEN_PEER_URLS
              value: "http://0.0.0.0:2380"
            - name: ETCD_AUTO_COMPACTION_MODE
              value: "revision"
            - name: ETCD_AUTO_COMPACTION_RETENTION
              value: "100"
            - name: ETCD_INITIAL_CLUSTER_TOKEN
              value: "etcd-cluster-k8s"
            - name: ETCD_INITIAL_CLUSTER_STATE
              value: "new"
            - name: ETCD_INITIAL_CLUSTER
              value: "release-name-etcd-0=http://release-name-etcd-0.release-name-etcd-headless.prod.svc.cluster.local:2380,release-name-etcd-1=http://release-name-etcd-1.release-name-etcd-headless.prod.svc.cluster.local:2380,release-name-etcd-2=http://release-name-etcd-2.release-name-etcd-headless.prod.svc.cluster.local:2380"
            - name: ETCD_CLUSTER_DOMAIN
              value: "release-name-etcd-headless.prod.svc.cluster.local"
            - name: ETCD_QUOTA_BACKEND_BYTES
              value: "8589934592"
          envFrom:
          ports:
            - name: client
              containerPort: 2379
              protocol: TCP
            - name: peer
              containerPort: 2380
              protocol: TCP
          livenessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            exec:
              command:
                - /opt/bitnami/scripts/etcd/healthcheck.sh
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: data
              mountPath: /bitnami/etcd
            - name: etcd-jwt-token
              mountPath: /opt/bitnami/etcd/certs/token/
              readOnly: true
      volumes:
        - name: etcd-jwt-token
          secret:
            secretName: release-name-etcd-jwt-token
            defaultMode: 256
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "2Gi"
        storageClassName: mayastor-etcd-localpv
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/logstash/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-logstash
  labels:
    app: "release-name-logstash"
    chart: "logstash"
    heritage: "Helm"
    release: "release-name"
spec:
  serviceName: release-name-logstash-headless
  selector:
    matchLabels:
      app: "release-name-logstash"
      release: "release-name"
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      name: "release-name-logstash"
      labels:
        app: "release-name-logstash"
        chart: "logstash"
        heritage: "Helm"
        release: "release-name"
      annotations:
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "release-name-logstash"
            topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 120
      volumes:
      containers:
      - name: "logstash"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "grafana/logstash-output-loki:1.0.1"
        imagePullPolicy: "IfNotPresent"
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: http
          initialDelaySeconds: 300
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: http
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9600
        resources:
          limits:
            cpu: 1000m
            memory: 1536Mi
          requests:
            cpu: 100m
            memory: 1536Mi
        env:
          - name: LS_JAVA_OPTS
            value: "-Xmx1g -Xms1g"
        volumeMounts:
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/loki/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-loki
  namespace: prod
  labels:
    app: loki
    chart: loki-2.11.0
    release: release-name
    heritage: Helm
  annotations:
    {}
spec:
  podManagementPolicy: OrderedReady
  replicas: 1
  selector:
    matchLabels:
      app: loki
      release: release-name
  serviceName: release-name-loki-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: loki
        name: release-name-loki
        release: release-name
      annotations:
        checksum/config: d2a4115b36809a64afcffe014b0e6fd4211c8c41dbf243088648622847fa9cd3
        prometheus.io/port: http-metrics
        prometheus.io/scrape: "true"
    spec:
      serviceAccountName: release-name-loki
      securityContext:
        fsGroup: 1001
        runAsGroup: 1001
        runAsNonRoot: false
        runAsUser: 1001
      initContainers:
        - command:
          - /bin/bash
          - -ec
          - chown -R 1001:1001 /data
          image: docker.io/bitnami/bitnami-shell:10
          imagePullPolicy: IfNotPresent
          name: volume-permissions
          securityContext:
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /data
            name: storage
      containers:
        - name: loki
          image: "grafana/loki:2.5.0"
          imagePullPolicy: IfNotPresent
          args:
            - "-config.file=/etc/loki/loki.yaml"
          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: config
              mountPath: /etc/loki
            - name: storage
              mountPath: "/data"
              subPath: 
          ports:
            - name: http-metrics
              containerPort: 3100
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            {}
          securityContext:
            readOnlyRootFilesystem: true
          env:
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      terminationGracePeriodSeconds: 4800
      volumes:
        - name: tmp
          emptyDir: {}
        - name: config
          secret:
            secretName: release-name-loki
  volumeClaimTemplates:
  - metadata:
      name: storage
      annotations:
        {}
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: "10Gi"
      storageClassName: mayastor-loki-localpv
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/nats/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-nats
  namespace: prod
  labels:
    helm.sh/chart: nats-0.19.14
    app.kubernetes.io/name: nats
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.9.17"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: nats
      app.kubernetes.io/instance: release-name
  replicas: 3
  serviceName: release-name-nats

  podManagementPolicy: Parallel

  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "7777"
        prometheus.io/scrape: "true"
        checksum/config: fb359864f3673c8c7dcf31c238e172f1fce973796344f16c34d384e08852eb2c
      labels:
        app.kubernetes.io/name: nats
        app.kubernetes.io/instance: release-name
    spec:
      dnsPolicy: ClusterFirst
      # Common volumes for the containers.
      volumes:
      - name: config-volume
        configMap:
          name: release-name-nats-config

      # Local volume shared with the reloader.
      - name: pid
        emptyDir: {}

      #################
      #               #
      #  TLS Volumes  #
      #               #
      #################

      serviceAccountName: release-name-nats

      # Required to be able to HUP signal and apply config
      # reload to the server without restarting the pod.
      shareProcessNamespace: true

      #################
      #               #
      #  NATS Server  #
      #               #
      #################
      terminationGracePeriodSeconds: 60
      containers:
      - name: nats
        image: nats:2.9.17-alpine
        imagePullPolicy: IfNotPresent
        resources:
          {}
        ports:
        - containerPort: 4222
          name: client
        - containerPort: 6222
          name: cluster
        - containerPort: 8222
          name: monitor

        command:
        - "nats-server"
        - "--config"
        - "/etc/nats-config/nats.conf"

        # Required to be able to define an environment variable
        # that refers to other environment variables.  This env var
        # is later used as part of the configuration file.
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SERVER_NAME
          value: $(POD_NAME)
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CLUSTER_ADVERTISE
          value: $(POD_NAME).release-name-nats.$(POD_NAMESPACE).svc.cluster.local
        volumeMounts:
        - name: config-volume
          mountPath: /etc/nats-config
        - name: pid
          mountPath: /var/run/nats
        

        #######################
        #                     #
        # Healthcheck Probes  #
        #                     #
        #######################
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 8222
          initialDelaySeconds: 10
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /
            port: 8222
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        startupProbe:
          # for NATS server versions >=2.7.1, /healthz will be enabled
          # startup probe checks that the JS server is enabled, is current with the meta leader,
          # and that all streams and consumers assigned to this JS server are current
          failureThreshold: 90
          httpGet:
            path: /healthz
            port: 8222
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5

        # Gracefully stop NATS Server on pod deletion or image upgrade.
        #
        lifecycle:
          preStop:
            exec:
              # send the lame duck shutdown signal to trigger a graceful shutdown
              # nats-server will ignore the TERM signal it receives after this
              #
              command:
              - "nats-server"
              - "-sl=ldm=/var/run/nats/nats.pid"

      #################################
      #                               #
      #  NATS Configuration Reloader  #
      #                               #
      #################################
      - name: reloader
        image: natsio/nats-server-config-reloader:0.10.1
        imagePullPolicy: IfNotPresent
        resources:
          {}
        command:
        - "nats-server-config-reloader"
        - "-pid"
        - "/var/run/nats/nats.pid"
        - "-config"
        - "/etc/nats-config/nats.conf"
        volumeMounts:
        - name: config-volume
          mountPath: /etc/nats-config
        - name: pid
          mountPath: /var/run/nats
        

      ##############################
      #                            #
      #  NATS Prometheus Exporter  #
      #                            #
      ##############################
      - name: metrics
        image: natsio/prometheus-nats-exporter:0.11.0
        imagePullPolicy: IfNotPresent
        resources:
          {}
        args:
        - -connz
        - -routez
        - -subz
        - -varz
        - -prefix=nats
        - -use_internal_server_id
        - -jsz=all
        - http://localhost:8222/
        ports:
        - containerPort: 7777
          name: metrics

  volumeClaimTemplates:
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/zfs-contoller.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-zfs-localpv-controller
  namespace: prod
  labels:
    chart: zfs-localpv-2.3.1
    heritage: Helm
    openebs.io/version: "2.3.1"
    role: "openebs-zfs"
    app: "openebs-zfs-controller"
    release: release-name
    component: "openebs-zfs-controller"
    openebs.io/component-name: "openebs-zfs-controller"
spec:
  selector:
    matchLabels:
      app: "openebs-zfs-controller"
      release: release-name
      component: "openebs-zfs-controller"
  serviceName: "openebs-zfs"
  replicas: 1
  template:
    metadata:
      labels:
        chart: zfs-localpv-2.3.1
        heritage: Helm
        openebs.io/version: "2.3.1"
        role: "openebs-zfs"
        app: "openebs-zfs-controller"
        release: release-name
        component: "openebs-zfs-controller"
        openebs.io/component-name: "openebs-zfs-controller"
        
        name: openebs-zfs-controller
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - openebs-zfs-controller
            topologyKey: "kubernetes.io/hostname"
      priorityClassName: release-name-zfs-csi-controller-critical
      serviceAccountName: openebs-zfs-controller-sa
      containers:
        - name: csi-resizer
          image: "registry.k8s.io/sig-storage/csi-resizer:v1.8.0"
          args:
            - "--v=5"
            - "--csi-address=$(ADDRESS)"
            - "--leader-election"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: csi-snapshotter
          image: "registry.k8s.io/sig-storage/csi-snapshotter:v6.2.2"
          imagePullPolicy: IfNotPresent
          args:
            - "--csi-address=$(ADDRESS)"
            - "--leader-election"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: snapshot-controller
          image: "registry.k8s.io/sig-storage/snapshot-controller:v6.2.2"
          args:
            - "--v=5"
            - "--leader-election=true"
          imagePullPolicy: IfNotPresent
        - name: csi-provisioner
          image: "registry.k8s.io/sig-storage/csi-provisioner:v3.5.0"
          imagePullPolicy: IfNotPresent
          args:
            - "--csi-address=$(ADDRESS)"
            - "--v=5"
            - "--feature-gates=Topology=true"
            - "--strict-topology"
            - "--leader-election"
            - "--enable-capacity=true"
            - "--extra-create-metadata=true"
            - "--default-fstype=ext4"
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
        - name: openebs-zfs-plugin
          image: "openebs/zfs-driver:2.3.0"
          imagePullPolicy: IfNotPresent
          env:
            - name: OPENEBS_CONTROLLER_DRIVER
              value: controller
            - name: OPENEBS_CSI_ENDPOINT
              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock
            - name: OPENEBS_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OPENEBS_IO_INSTALLER_TYPE
              value: "zfs-localpv-helm"
            - name: OPENEBS_IO_ENABLE_ANALYTICS
              value: "true"
          args :
            - "--endpoint=$(OPENEBS_CSI_ENDPOINT)"
            - "--plugin=$(OPENEBS_CONTROLLER_DRIVER)"
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
      volumes:
        - name: socket-dir
          emptyDir: {}
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/filebeat/templates/deployment.yaml
# Deploy singleton instance in the whole cluster for some unique data sources, like aws input
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/csi-driver.yaml
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: cstor.csi.openebs.io
spec:
  # Supports persistent inline volumes.
  volumeLifecycleModes:
    - Persistent
    # Not yet supported but added just to support upgrade control plane seamlessly
    - Ephemeral
  # To determine at runtime which mode a volume uses, pod info and its
  # "csi.storage.k8s.io/ephemeral" entry are needed.
  podInfoOnMount: true
  attachRequired: false
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/csi-driver.yaml
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: jiva.csi.openebs.io
spec:
  podInfoOnMount: true
  attachRequired: false
---
# Source: my-openebs/charts/openebs/charts/lvm-localpv/templates/csidriver.yaml
# Create the CSI Driver object
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: local.csi.openebs.io
spec:
  # do not require volumeattachment
  attachRequired: false
  podInfoOnMount: true
  storageCapacity: true
---
# Source: my-openebs/charts/openebs/charts/zfs-localpv/templates/csidriver.yaml
# Create the CSI Driver object
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: zfs.csi.openebs.io
spec:
  # do not require volumeattachment
  attachRequired: false
  podInfoOnMount: false
  storageCapacity: true
---
# Source: my-openebs/charts/openebs/charts/jiva/templates/default-policy.yaml
apiVersion: openebs.io/v1alpha1
kind: JivaVolumePolicy
metadata:
  name: openebs-jiva-default-policy
spec:
  replicaSC: openebs-hostpath
  target:
    replicationFactor: 3
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/charts/grafana/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: release-name-grafana-test
  labels:
    helm.sh/chart: grafana-6.24.1
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "8.3.5"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
  namespace: prod
spec:
  serviceAccountName: release-name-grafana-test
  containers:
    - name: release-name-test
      image: "bats/bats:v1.4.1"
      imagePullPolicy: "IfNotPresent"
      command: ["/opt/bats/bin/bats", "-t", "/tests/run.sh"]
      volumeMounts:
        - mountPath: /tests
          name: tests
          readOnly: true
  volumes:
  - name: tests
    configMap:
      name: release-name-grafana-test
  restartPolicy: Never
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/loki-stack/templates/tests/loki-test-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    "helm.sh/hook": test-success
  labels:
    app: loki-stack
    chart: loki-stack-2.6.4
    release: release-name
    heritage: Helm
  name: release-name-loki-stack-test
spec:
  containers:
  - name: test
    image: bats/bats:v1.1.0
    args:
    - /var/lib/loki/test.sh
    env:
    - name: LOKI_SERVICE
      value: release-name-loki
    - name: LOKI_PORT
      value: "3100"
    volumeMounts:
    - name: tests
      mountPath: /var/lib/loki
  restartPolicy: Never
  volumes:
  - name: tests
    configMap:
      name: release-name-loki-stack-test
---
# Source: my-openebs/charts/openebs/charts/mayastor/charts/nats/templates/tests/test-request-reply.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-name-nats-test-request-reply"
  labels:
    chart: nats-0.19.14
    app: release-name-nats-test-request-reply
  annotations:
    "helm.sh/hook": test
spec:
  containers:
  - name: nats-box
    image: natsio/nats-box:0.13.8
    env:
    - name: NATS_HOST
      value: release-name-nats
    command:
    - /bin/sh
    - -ec
    - |
      nats reply -s nats://$NATS_HOST:4222 'name.>' --command "echo 1" &
    - |
      "&&"
    - |
      name=$(nats request -s nats://$NATS_HOST:4222 name.test '' 2>/dev/null)
    - |
      "&&"
    - |
      [ $name = test ]

  restartPolicy: Never
---
# Source: my-openebs/charts/openebs/charts/cstor/templates/cleanup-webhook.yaml
# HELM first deletes RBAC, then it tries to delete other resources like CSPC and PVC. 
# We've got validating webhook on CSPC and PVC.
# But even that the policy of this webhook is Ignore, it fails because the ServiceAccount 
# does not have permission to access resources like BDC anymore which are used for validation.
# Therefore we first need to delete webhook so we can delete the rest of the deployments.
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-cstor-webhook-cleanup
  namespace: prod
  annotations:
    "helm.sh/hook": pre-delete
    "helm.sh/hook-delete-policy": hook-succeeded
  labels:
    app: cstor
spec:
  template:
    metadata:
      name: release-name-cstor-webhook-cleanup
      labels:
        app: cstor
    spec:
      serviceAccountName: openebs-cstor-operator
      containers:
        - name: kubectl
          image: "bitnami/kubectl:1.28"
          command:
          - /bin/sh
          - -c
          - >
              kubectl delete validatingWebhookConfiguration openebs-cstor-validation-webhook || true;
      restartPolicy: OnFailure
